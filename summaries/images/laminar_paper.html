<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Laminar: A Scalable Asynchronous RL Post-Training Framework</title>
<!--Generated on Tue Oct 14 15:20:53 2025 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="/static/browse/0.3.4/css/arxiv-html-papers-20250916.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2510.12633v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#S1" title="In Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#S2" title="In Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Background and Motivation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#S2.SS1" title="In 2. Background and Motivation ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>RL for LLM Post-Training</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#S2.SS2" title="In 2. Background and Motivation ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Recent Trends in RL Post-Training</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#S2.SS3" title="In 2. Background and Motivation ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Limitations of existing asynchronous RL systems</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#S2.SS4" title="In 2. Background and Motivation ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4 </span>Opportunity and Challenges</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#S3" title="In Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Fully Decoupled Architecture Design</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#S3.SS1" title="In 3. Fully Decoupled Architecture Design ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>System Components</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#S3.SS2" title="In 3. Fully Decoupled Architecture Design ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Training Workflow</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#S3.SS3" title="In 3. Fully Decoupled Architecture Design ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Fault Tolerance and Recovery</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#S4" title="In Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Asynchronous Weight Synchronization Using Relay Workers</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#S4.SS1" title="In 4. Asynchronous Weight Synchronization Using Relay Workers ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Design Considerations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#S4.SS2" title="In 4. Asynchronous Weight Synchronization Using Relay Workers ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Hierarchical Relays and Their Workflow</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#S4.SS3" title="In 4. Asynchronous Weight Synchronization Using Relay Workers ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Fault-Tolerant Relay Broadcast</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#S5" title="In Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Bubble Elimination in Long-tail Trajectory Generation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#S5.SS1" title="In 5. Bubble Elimination in Long-tail Trajectory Generation ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Workflow of the Repack Mechanism</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#S5.SS2" title="In 5. Bubble Elimination in Long-tail Trajectory Generation ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Online Trajecgtory Repacking</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#S6" title="In Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Trajectory-level Asynchrony Analysis</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#S7" title="In Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Implementation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#S8" title="In Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span>Evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#S8.SS1" title="In 8. Evaluation ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text" style="font-size:90%;">8.1</span> </span><span class="ltx_text" style="font-size:90%;">End-to-End Training Throughput</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#S8.SS2" title="In 8. Evaluation ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text" style="font-size:90%;">8.2</span> </span><span class="ltx_text" style="font-size:90%;">Training Convergence</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#S8.SS3" title="In 13(c) ‣ 8.2. Training Convergence ‣ 8. Evaluation ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text" style="font-size:90%;">8.3</span> </span><span class="ltx_text" style="font-size:90%;">Weight Synchronization Overhead</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#S8.SS4" title="In 8.3. Weight Synchronization Overhead ‣ 13(c) ‣ 8.2. Training Convergence ‣ 8. Evaluation ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text" style="font-size:90%;">8.4</span> </span><span class="ltx_text" style="font-size:90%;">Repack Efficiency</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#S8.SS5" title="In 8.4. Repack Efficiency ‣ 8.3. Weight Synchronization Overhead ‣ 13(c) ‣ 8.2. Training Convergence ‣ 8. Evaluation ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text" style="font-size:90%;">8.5</span> </span><span class="ltx_text" style="font-size:90%;">Fault Tolerance Analysis</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#S9" title="In 8.5. Fault Tolerance Analysis ‣ 8.4. Repack Efficiency ‣ 8.3. Weight Synchronization Overhead ‣ 13(c) ‣ 8.2. Training Convergence ‣ 8. Evaluation ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text" style="font-size:90%;">9</span> </span><span class="ltx_text" style="font-size:90%;">Related Work</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#S10" title="In 9. Related Work ‣ 8.5. Fault Tolerance Analysis ‣ 8.4. Repack Efficiency ‣ 8.3. Weight Synchronization Overhead ‣ 13(c) ‣ 8.2. Training Convergence ‣ 8. Evaluation ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text" style="font-size:90%;">10</span> </span><span class="ltx_text" style="font-size:90%;">Conclusion</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#A1" title="In 10. Conclusion ‣ 9. Related Work ‣ 8.5. Fault Tolerance Analysis ‣ 8.4. Repack Efficiency ‣ 8.3. Weight Synchronization Overhead ‣ 13(c) ‣ 8.2. Training Convergence ‣ 8. Evaluation ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text" style="font-size:90%;">A</span> </span><span class="ltx_text" style="font-size:90%;">Experiment Details</span></span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#A1.SS1" title="In Appendix A Experiment Details ‣ 10. Conclusion ‣ 9. Related Work ‣ 8.5. Fault Tolerance Analysis ‣ 8.4. Repack Efficiency ‣ 8.3. Weight Synchronization Overhead ‣ 13(c) ‣ 8.2. Training Convergence ‣ 8. Evaluation ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text" style="font-size:90%;">A.1</span> </span><span class="ltx_text" style="font-size:90%;">Response Length Distribution of Each Model</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#A1.SS2" title="In A.1. Response Length Distribution of Each Model ‣ Appendix A Experiment Details ‣ 10. Conclusion ‣ 9. Related Work ‣ 8.5. Fault Tolerance Analysis ‣ 8.4. Repack Efficiency ‣ 8.3. Weight Synchronization Overhead ‣ 13(c) ‣ 8.2. Training Convergence ‣ 8. Evaluation ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text" style="font-size:90%;">A.2</span> </span><span class="ltx_text" style="font-size:90%;">Hyperparameter Details</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#A2" title="In A.2. Hyperparameter Details ‣ A.1. Response Length Distribution of Each Model ‣ Appendix A Experiment Details ‣ 10. Conclusion ‣ 9. Related Work ‣ 8.5. Fault Tolerance Analysis ‣ 8.4. Repack Efficiency ‣ 8.3. Weight Synchronization Overhead ‣ 13(c) ‣ 8.2. Training Convergence ‣ 8. Evaluation ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text" style="font-size:90%;">B</span> </span><span class="ltx_text" style="font-size:90%;">Detailed Experiment Analysis</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#A3" title="In A.2. Hyperparameter Details ‣ A.1. Response Length Distribution of Each Model ‣ Appendix A Experiment Details ‣ 10. Conclusion ‣ 9. Related Work ‣ 8.5. Fault Tolerance Analysis ‣ 8.4. Repack Efficiency ‣ 8.3. Weight Synchronization Overhead ‣ 13(c) ‣ 8.2. Training Convergence ‣ 8. Evaluation ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text" style="font-size:90%;">C</span> </span><span class="ltx_text" style="font-size:90%;">Discussion</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#A4" title="In A.2. Hyperparameter Details ‣ A.1. Response Length Distribution of Each Model ‣ Appendix A Experiment Details ‣ 10. Conclusion ‣ 9. Related Work ‣ 8.5. Fault Tolerance Analysis ‣ 8.4. Repack Efficiency ‣ 8.3. Weight Synchronization Overhead ‣ 13(c) ‣ 8.2. Training Convergence ‣ 8. Evaluation ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text" style="font-size:90%;">D</span> </span><span class="ltx_text" style="font-size:90%;">Theoretical Analysis of Chain-Based Pipelined Broadcast</span></span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#A4.SS1" title="In Appendix D Theoretical Analysis of Chain-Based Pipelined Broadcast ‣ A.2. Hyperparameter Details ‣ A.1. Response Length Distribution of Each Model ‣ Appendix A Experiment Details ‣ 10. Conclusion ‣ 9. Related Work ‣ 8.5. Fault Tolerance Analysis ‣ 8.4. Repack Efficiency ‣ 8.3. Weight Synchronization Overhead ‣ 13(c) ‣ 8.2. Training Convergence ‣ 8. Evaluation ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text" style="font-size:90%;">D.1</span> </span><span class="ltx_text" style="font-size:90%;">Communication Model</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#A4.SS2" title="In Appendix D Theoretical Analysis of Chain-Based Pipelined Broadcast ‣ A.2. Hyperparameter Details ‣ A.1. Response Length Distribution of Each Model ‣ Appendix A Experiment Details ‣ 10. Conclusion ‣ 9. Related Work ‣ 8.5. Fault Tolerance Analysis ‣ 8.4. Repack Efficiency ‣ 8.3. Weight Synchronization Overhead ‣ 13(c) ‣ 8.2. Training Convergence ‣ 8. Evaluation ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text" style="font-size:90%;">D.2</span> </span><span class="ltx_text" style="font-size:90%;">Latency Derivation</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#A4.SS3" title="In Appendix D Theoretical Analysis of Chain-Based Pipelined Broadcast ‣ A.2. Hyperparameter Details ‣ A.1. Response Length Distribution of Each Model ‣ Appendix A Experiment Details ‣ 10. Conclusion ‣ 9. Related Work ‣ 8.5. Fault Tolerance Analysis ‣ 8.4. Repack Efficiency ‣ 8.3. Weight Synchronization Overhead ‣ 13(c) ‣ 8.2. Training Convergence ‣ 8. Evaluation ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text" style="font-size:90%;">D.3</span> </span><span class="ltx_text" style="font-size:90%;">Scalability Analysis</span></span></a></li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno" lang="en">
<h1 class="ltx_title ltx_title_document">Laminar: A Scalable Asynchronous RL Post-Training Framework</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">

Guangming Sheng<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">1∗</span></sup>, Yuxuan Tong<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">2∗</span></sup>, Borui Wan<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">1</span></sup>, Wang Zhang<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">2</span></sup>, Chaobo Jia<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">2</span></sup>, Xibin Wu<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">2</span></sup>, Yuqi Wu<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">2</span></sup>, Xiang Li<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">2</span></sup>, Chi Zhang<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">2</span></sup>, Yanghua Peng<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">2</span></sup>, Haibin Lin<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">2</span></sup>, Xin Liu<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">2</span></sup>, Chuan Wu<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">1</span></sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><sup class="ltx_sup"><span class="ltx_text" style="font-size:144%;">1</span></sup><span class="ltx_text ltx_font_italic" style="font-size:144%;">The University of Hong Kong</span>   <sup class="ltx_sup"><span class="ltx_text" style="font-size:144%;">2</span></sup><span class="ltx_text ltx_font_italic" style="font-size:144%;">ByteDance Seed</span><span class="ltx_text ltx_affiliation_country"></span>
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p class="ltx_p">Reinforcement learning (RL) post-training for Large Language Models (LLMs) is now scaling to large clusters and running for extended durations
to enhance model reasoning performance.
However, the scalability of existing RL frameworks is limited,
as extreme long-tail skewness in RL trajectory generation causes severe GPU underutilization.
Current asynchronous RL systems attempt to mitigate this, but they rely on global weight synchronization between the actor and all rollouts, which creates a rigid model update schedule.
This global synchronization is ill-suited for the highly skewed and evolving distribution of trajectory generation latency in RL training,
crippling training efficiency.
Our key insight is that efficient scaling requires breaking this lockstep through <span class="ltx_text ltx_font_italic">trajectory-level asynchrony, which generates and consumes each trajectory independently</span>.
We propose Laminar, a scalable and robust RL post-training system built on a fully decoupled architecture.
First, we replace global updates with a tier of relay workers acting as a distributed parameter service.
This enables asynchronous and fine-grained weight synchronization, allowing rollouts to pull the latest weight anytime without stalling the actor’s training loop.
Second, a dynamic repack mechanism consolidates long-tail trajectories onto a few dedicated rollouts, maximizing generation throughput.
The fully decoupled design also isolates failures, ensuring robustness for
long-running jobs.
Our evaluation on a 1024-GPU cluster shows that Laminar achieves up to 5.48<math alttext="\times" class="ltx_Math" display="inline" id="m16" intent=":literal"><semantics><mo>×</mo><annotation encoding="application/x-tex">\times</annotation></semantics></math> training throughput speedup over state-of-the-art systems, while reducing model convergence time.</p>
</div>
<span class="ltx_note ltx_note_frontmatter ltx_role_submissionid"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">submissionid: </span>91</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_copyright"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>none</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_conference"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">conference: </span>Make sure to enter the correct
conference title from your rights confirmation emai; June 03–05,
2018; Woodstock, NY</span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p">Reinforcement learning (RL) has emerged as a transformative paradigm for post-training large language models (LLMs), fundamentally enhancing their reasoning capabilities through iterative policy optimization <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Guo et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib2" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>; <span class="ltx_text" style="font-size:90%;">Yu et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib3" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>; <span class="ltx_text" style="font-size:90%;">Kumar et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib4" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>)</cite>. Contemporary state-of-the-art models, including OpenAI’s o1 series <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">OpenAI</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib5" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>)</cite>, DeepSeek-R1 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Guo et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib2" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>)</cite>, and xAI’s Grok 4 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">xAI</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib6" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>)</cite>, leverage RL techniques to achieve unprecedented performance on complex reasoning tasks spanning mathematics, coding,
and agentic tasks.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="382" id="S1.F1.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 1</span>. </span><span class="ltx_text" style="font-size:90%;">RL post-training workflow and time breakdown of single-turn (math) <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Yu et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib3" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>)</cite> and multi-turn (code) tasks <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Jimenez et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib7" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite>.</span></figcaption>
</figure>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p">Current RL post-training workflows operate through two main serial phases: generation and training, as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#S1.F1" title="Figure 1 ‣ 1. Introduction ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_tag">1</span></a>(a).
In the generation phase, rollout models produce trajectories by responding to prompts or interacting with environments.
After LLM generates some tokens, the system may trigger an interaction with the environment, which in turn produces feedback.
A recent trend is to scale this phase across many rollout replicas <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Guo et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib2" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>; <span class="ltx_text" style="font-size:90%;">Yu et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib3" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>; <span class="ltx_text" style="font-size:90%;">Seed et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib8" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>; <span class="ltx_text" style="font-size:90%;">Team et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib9" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>; <span class="ltx_text" style="font-size:90%;">xAI</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib6" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>)</cite>.
xAI scales an RL training job to
200k GPUs scale
with massive rollout generation <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">xAI</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib6" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>)</cite>.
The goal is to produce vast and diverse trajectories.
These trajectories are then evaluated using verifiers, reward models, or environmental signals <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Kumar et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib4" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>; <span class="ltx_text" style="font-size:90%;">Li et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib10" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>)</cite> and stored in the experience buffer.
During the training phase, a subset of these trajectories is sampled from the buffer <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Wang et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib11" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>; <span class="ltx_text" style="font-size:90%;">Yu et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib3" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>)</cite>.
The sampled batch is then used to compute policy gradients and update model parameters.
This extensive generation
is critical for effective policy optimization,
but relies
on the system’s ability to manage the scaled-out rollout process efficiently.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p">Existing synchronous RL systems <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Yao et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib12" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>; <span class="ltx_text" style="font-size:90%;">Sheng et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib13" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>; <span class="ltx_text" style="font-size:90%;">Lei et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib14" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>; <span class="ltx_text" style="font-size:90%;">Mei et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib15" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>; <span class="ltx_text" style="font-size:90%;">Zhong et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib16" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>; <span class="ltx_text" style="font-size:90%;">Hu et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib17" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite> face significant efficiency bottlenecks that limit their scalability. The generation stage dominates overall training time, accounting for up to 83.1% of total execution time in reasoning tasks <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Yu et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib3" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>)</cite> (Figure <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#S1.F1" title="Figure 1 ‣ 1. Introduction ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_tag">1</span></a>(b)).
This bottleneck stems primarily from long-tail trajectory generation, where
the distributions of
output length and environment latency are highly skewed.
For instance, in mathematics and coding tasks, the 99th percentile output length can exceed the 50th percentile by an order of magnitude (Figure <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#S2.F2" title="Figure 2 ‣ 2.2. Recent Trends in RL Post-Training ‣ 2. Background and Motivation ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_tag">2</span></a>).
This issue is particularly acute for tasks requiring complex reasoning or multi-turn environment interaction (detailed in §<a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#S2.SS2" title="2.2. Recent Trends in RL Post-Training ‣ 2. Background and Motivation ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_tag">2.2</span></a>).
As generation progresses, only the generation of a few long-tail trajectories remains active, leading to severe GPU underutilization across the cluster.
Simply adding more
GPUs cannot resolve this load imbalance issue.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p">Recent asynchronous RL frameworks attempt to address this long-tail problem by decoupling generation and training across different devices and overlapping their execution <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Zhong et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib18" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>; <span class="ltx_text" style="font-size:90%;">Fu et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib19" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>; <span class="ltx_text" style="font-size:90%;">Wu et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib20" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>; <span class="ltx_text" style="font-size:90%;">Tan et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib21" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>; <span class="ltx_text" style="font-size:90%;">Team et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib22" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>; <span class="ltx_text" style="font-size:90%;">Han et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib23" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>)</cite>. They typically operate under <math alttext="k" class="ltx_Math" display="inline" id="S1.p4.m1" intent=":literal"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>-step bounded staleness, where rollouts use actor weights that are up to <math alttext="k" class="ltx_Math" display="inline" id="S1.p4.m2" intent=":literal"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math> training iterations old.
A global synchronization then distributes the new actor weights to all rollouts following a predefined schedule.
However, this approach still struggles with fundamental inefficiencies on the long-tail generation problem.
A simple and widely adopted one-step staleness (<math alttext="k" class="ltx_Math" display="inline" id="S1.p4.m3" intent=":literal"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>=1) pipeline <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Wu et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib20" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>; <span class="ltx_text" style="font-size:90%;">Zhong et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib18" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>; <span class="ltx_text" style="font-size:90%;">Tan et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib21" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>; <span class="ltx_text" style="font-size:90%;">Han et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib23" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>)</cite> cannot effectively hide the generation latency of long-tail trajectories.
While increasing the staleness bound (<math alttext="k" class="ltx_Math" display="inline" id="S1.p4.m4" intent=":literal"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math> ¿ 1) can
absorb this skewness, it
can negatively affect model convergence <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Noukhovitch et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib24" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>; <span class="ltx_text" style="font-size:90%;">Berner et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib25" title=""><span class="ltx_text" style="font-size:90%;">2019</span></a>; <span class="ltx_text" style="font-size:90%;">Wang et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib11" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>; <span class="ltx_text" style="font-size:90%;">Fu et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib19" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>)</cite>.
Consequently, determining an optimal staleness bound that balances overlap efficiency and training convergence remains a difficult tuning problem <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Li et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib26" title=""><span class="ltx_text" style="font-size:90%;">2018</span></a>; <span class="ltx_text" style="font-size:90%;">Barkai et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib27" title=""><span class="ltx_text" style="font-size:90%;">2019</span></a>; <span class="ltx_text" style="font-size:90%;">Chen et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib28" title=""><span class="ltx_text" style="font-size:90%;">2022</span></a>; <span class="ltx_text" style="font-size:90%;">Sheng et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib29" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p">A fundamental limitation of existing RL systems is their reliance on global model weight synchronization, which is not suitable for the
highly variant and
dynamic nature of RL workloads.
Recent findings show that trajectory lengths change dynamically as models learn <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Guo et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib2" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>; <span class="ltx_text" style="font-size:90%;">Yu et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib3" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>; <span class="ltx_text" style="font-size:90%;">Team et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib9" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>)</cite>, exhibiting increasing, decreasing, or fluctuating patterns.
Environment interaction latency varies dramatically due to unpredictable API call time or task complexity <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Jimenez et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib30" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>; <span class="ltx_text" style="font-size:90%;">Cheng et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib31" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>; <span class="ltx_text" style="font-size:90%;">OpenAI</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib32" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>)</cite>.
Current approaches force all trajectories to conform to the same rigid update schedule, regardless of
their diverse characteristics, completion status, or inherent execution variability.
This inflexibility enforces rigid data and parameter dependencies across actor and all rollouts,
preventing effective system scaling.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p">To address these limitations, we propose Laminar, a scalable and robust asynchronous RL post-training system that
eliminates the long-tail trajectory generation bottleneck at production scale
while ensuring stable
RL training.
Our key idea is enabling <span class="ltx_text ltx_font_italic">trajectory-level asynchrony</span>, with each trajectory generated and consumed independently at its own optimal pace, to accommodate the vast variability in output length and environment latency.
It removes the primary bottleneck to scaling out to thousands of GPUs and enhances trajectory diversity, which is crucial for efficient RL training.</p>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p">To realize trajectory-level asynchrony, we introduce a fully decoupled architecture that breaks data and parameter dependencies between the actor model and all rollout replicas, and among rollout replicas themselves.
This architectural decoupling is further fundamental to achieving robustness at scale. By isolating components, the failure of a single rollout machine does not halt the entire training job, enabling swift recovery
that is critical for long-running jobs.</p>
</div>
<div class="ltx_para" id="S1.p8">
<p class="ltx_p">We implement this architecture with two key designs.
<span class="ltx_text ltx_font_italic">First</span>, we decouple the actor and each rollout replica using a tier of <span class="ltx_text ltx_font_italic">relay workers</span>.
Functioning as a distributed parameter service, the relays provide asynchronous and fine-grained weight synchronization.
The actor can be trained
uninterruptedly,
while rollouts can retrieve new model parameters from the relays at any time.
Nonetheless, individual rollouts may still face long-tail trajectory
generation issues.
<span class="ltx_text ltx_font_italic">Further</span>, we propose a dynamic repack mechanism to consolidate long-tail trajectories from underutilized rollouts
into a few dedicated rollout replicas, while liberating other rollouts to update using the latest weight version.
This ensures high generation throughput while introducing minimal staleness
across the system.
Our contributions are summarized as follows:</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p9">
<p class="ltx_p"><math alttext="\bullet" class="ltx_Math" display="inline" id="S1.p9.m1" intent=":literal"><semantics><mo>∙</mo><annotation encoding="application/x-tex">\bullet</annotation></semantics></math> We identify trajectory-level asynchrony as the key to scaling-out RL and realize it through a fully decoupled architecture. Our design systematically breaks data and parameter dependencies between all system components and isolates component failures to ensure swift recovery for long-running training (§<a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#S3" title="3. Fully Decoupled Architecture Design ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_tag">3</span></a>).</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p10">
<p class="ltx_p"><math alttext="\bullet" class="ltx_Math" display="inline" id="S1.p10.m1" intent=":literal"><semantics><mo>∙</mo><annotation encoding="application/x-tex">\bullet</annotation></semantics></math> We design a tier of relay workers functioning as distributed parameter service, providing fine-grained, robust, and anytime weight synchronization without stalling training (§<a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#S4" title="4. Asynchronous Weight Synchronization Using Relay Workers ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_tag">4</span></a>).</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p11">
<p class="ltx_p"><math alttext="\bullet" class="ltx_Math" display="inline" id="S1.p11.m1" intent=":literal"><semantics><mo>∙</mo><annotation encoding="application/x-tex">\bullet</annotation></semantics></math> We propose a dynamic repack mechanism that boosts generation throughput, by actively monitoring rollout idleness via KVCache-based metrics and concentrating long-tail trajectories generation onto fewer rollout replicas
(§<a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#S5" title="5. Bubble Elimination in Long-tail Trajectory Generation ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_tag">5</span></a>).</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p12">
<p class="ltx_p"><math alttext="\bullet" class="ltx_Math" display="inline" id="S1.p12.m1" intent=":literal"><semantics><mo>∙</mo><annotation encoding="application/x-tex">\bullet</annotation></semantics></math> We conduct extensive experiments comparing Laminar with state-of-the-art RL systems <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Sheng et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib13" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>; <span class="ltx_text" style="font-size:90%;">Tan et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib21" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>; <span class="ltx_text" style="font-size:90%;">Zhong et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib18" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>; <span class="ltx_text" style="font-size:90%;">Fu et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib19" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>)</cite> at scales up to 1024 GPUs.
Our evaluation demonstrates up to 5.48<math alttext="\times" class="ltx_Math" display="inline" id="S1.p12.m2" intent=":literal"><semantics><mo>×</mo><annotation encoding="application/x-tex">\times</annotation></semantics></math> throughput speedup while ensuring model convergence and robustness
(§<a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#S8" title="8. Evaluation ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_tag">8</span></a>).</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Background and Motivation</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1. </span>RL for LLM Post-Training</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p">Reinforcement Learning (RL) for LLM post-training was first used to
align models with human preferences, known as Reinforcement Learning from Human Feedback <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Ouyang et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib33" title=""><span class="ltx_text" style="font-size:90%;">2022</span></a>; <span class="ltx_text" style="font-size:90%;">Bai et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib34" title=""><span class="ltx_text" style="font-size:90%;">2022</span></a>; <span class="ltx_text" style="font-size:90%;">Dai et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib35" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>; <span class="ltx_text" style="font-size:90%;">Zheng et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib36" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>; <span class="ltx_text" style="font-size:90%;">Lee et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib37" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite>.
More recently, RL has been extended to enhance complex reasoning in domains like mathematics <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Yu et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib3" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>; <span class="ltx_text" style="font-size:90%;">Shao et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib38" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite> and to develop multi-turn agentic capabilities <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Jimenez et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib7" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>; <span class="ltx_text" style="font-size:90%;">Feng et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib39" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>; <span class="ltx_text" style="font-size:90%;">OpenAI</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib32" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>)</cite>.
As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#S1.F1" title="Figure 1 ‣ 1. Introduction ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_tag">1</span></a>(a), RL post-training workflow can be primarily decomposed into two stages: Generation and Training.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.p2">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Generation stage</span>:
The rollout model produces trajectories by responding to prompts
through auto-regressive generation or by interacting with environments.
For single-turn reasoning tasks, such as solving math problems <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">of America</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib40" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>, a trajectory for a simple question may be a short chain-of-thought, while difficult ones can require exploring vast reasoning trees.
For multi-turn agentic tasks like SWE-Bench <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Jimenez et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib7" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite>, a trajectory consists of interactions with a code sandbox; fixing a simple bug requires only a few interactions, whereas diagnosing a complex issue can lead to more debugging steps.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.p3">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Training stage</span>:
Actor training begins by evaluating each trajectory and assigning it a reward score.
This score can come from a separate reward model <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Ouyang et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib33" title=""><span class="ltx_text" style="font-size:90%;">2022</span></a>; <span class="ltx_text" style="font-size:90%;">Bai et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib34" title=""><span class="ltx_text" style="font-size:90%;">2022</span></a>)</cite>, rule-based functions that define explicit criteria <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Guo et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib2" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>; <span class="ltx_text" style="font-size:90%;">Yu et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib3" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>)</cite>,
or signals derived from the environment <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Jimenez et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib7" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>; <span class="ltx_text" style="font-size:90%;">Cheng et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib31" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>)</cite>. Each scored trajectory is combined with other metrics, such as advantage estimation
derived from a reference model and a critic model, to form a training experience.
These experiences are then added to a buffer, where batches are sampled to update the actor model’s parameters via backpropagation. While foundational RL methods like PPO <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Schulman et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib41" title=""><span class="ltx_text" style="font-size:90%;">2017</span></a>)</cite> rely on a critic model, many recent algorithms
(e.g., GRPO <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Shao et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib38" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>, RLOO <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Ahmadian et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib42" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite> and DAPO <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Yu et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib3" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>)</cite>)
simplify this process by approximating advantage computation through generating multiple trajectories to the same prompt, removing the need for a critic model entirely.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2. </span>Recent Trends in RL Post-Training</h3>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="243" id="S2.F2.g1" src="x2.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 2</span>. </span><span class="ltx_text" style="font-size:90%;">Trajectory length distribution on AIME dataset and execution latency distribution of code sandbox <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">of America</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib40" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>; <span class="ltx_text" style="font-size:90%;">Feng et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib39" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>)</cite>.</span></figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S2.SS2.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Exacerbated data skewness.</span>
Data skewness in modern RL post-training has become increasingly severe due to the shift toward reasoning-focused models and agentic tasks.
The primary source of this skew is trajectory length.
As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#S2.F2" title="Figure 2 ‣ 2.2. Recent Trends in RL Post-Training ‣ 2. Background and Motivation ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_tag">2</span></a>, trajectory lengths are highly heterogeneous with the 99th-percentile being ten times the median in some tasks <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Yu et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib3" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>; <span class="ltx_text" style="font-size:90%;">Jimenez et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib7" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite>. As shorter trajectories in a batch are generated, only a few long-running trajectory generation tasks remain, leading to severe GPU underutilization. The problem is acute because the memory-bound nature of LLM decoding requires large batch sizes to maintain high throughput.
Consequently, the generation stage now dominates the post-training workflow, accounting for up to 83.1% of total execution time in reasoning tasks, as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#S1.F1" title="Figure 1 ‣ 1. Introduction ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_tag">1</span></a>(b).</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p">This long-tail problem extends to multi-turn agentic tasks, which involve highly unpredictable environment execution time.
Here, the LLM interacts with environments like code sandboxes <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Jimenez et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib7" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>; <span class="ltx_text" style="font-size:90%;">Feng et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib39" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>)</cite>,
computer environments <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Mialon et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib43" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>; <span class="ltx_text" style="font-size:90%;">Xie et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib44" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>
and gaming platforms <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Berner et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib25" title=""><span class="ltx_text" style="font-size:90%;">2019</span></a>; <span class="ltx_text" style="font-size:90%;">Vinyals et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib45" title=""><span class="ltx_text" style="font-size:90%;">2019</span></a>)</cite>.
These environments typically run as external programs or shared services.
As shown in Figure 2, the latency of these interactions can vary significantly due to request queuing and varying computational load.
This environmental unpredictability, combined with the inherent variance in trajectory length, introduces major efficiency bottlenecks that current systems cannot resolve.</p>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p">Existing synchronous RL post-training frameworks <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Yao et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib12" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>; <span class="ltx_text" style="font-size:90%;">Lei et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib14" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>; <span class="ltx_text" style="font-size:90%;">Sheng et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib13" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>; <span class="ltx_text" style="font-size:90%;">Mei et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib15" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>; <span class="ltx_text" style="font-size:90%;">Hu et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib17" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>; <span class="ltx_text" style="font-size:90%;">Shen et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib46" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>
commonly optimize the generation stage and the training stage independently.
They adopt sequential and synchronous execution in stages as they are typically built for on-policy RL algorithms.
While some systems adopt hybrid execution parallelism (e.g., HybridEngine <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Sheng et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib13" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>; <span class="ltx_text" style="font-size:90%;">Yao et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib12" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite> and Context Switching <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Lei et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib14" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>; <span class="ltx_text" style="font-size:90%;">Mei et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib15" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>) to improve throughput, they struggle to mitigate the prolonged GPU idle time caused by skewed trajectory generation, as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#S2.F3" title="Figure 3 ‣ 2.2. Recent Trends in RL Post-Training ‣ 2. Background and Motivation ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_tag">3</span></a>(a).
Other systems <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Zhong et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib16" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>; <span class="ltx_text" style="font-size:90%;">Xiao et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib47" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite> introduce an inter-stage fusion strategy to overlap the generation stage and experience preparation in the training stage.
However, since experience preparation computation only comprises 7.3% of total RL iteration time,
the benefits remain limited.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS2.p4">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Applying asynchronous RL.</span>
Recent advances in RL post-training have increasingly explored asynchronous (i.e., off-policy) RL algorithms to enable parallel execution of generation and training stages.
They place actor and rollout models on different devices, allowing trajectories used for training to be generated from previous model versions.
Asynchronous RL algorithms have demonstrated remarkable success in domains such as games <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Berner et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib25" title=""><span class="ltx_text" style="font-size:90%;">2019</span></a>; <span class="ltx_text" style="font-size:90%;">Vinyals et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib45" title=""><span class="ltx_text" style="font-size:90%;">2019</span></a>)</cite> and robotics <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Gu et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib48" title=""><span class="ltx_text" style="font-size:90%;">2017</span></a>; <span class="ltx_text" style="font-size:90%;">Kahn et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib49" title=""><span class="ltx_text" style="font-size:90%;">2018</span></a>)</cite>.
While they improve resource efficiency, their benefits in recent asynchronous RL for LLM post-training systems are severely diminished.
The fundamental limitation lies in their batch-oriented design, which fails to effectively mask the generation latency of long-tail trajectories.</p>
</div>
<figure class="ltx_figure" id="S2.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="913" id="S2.F3.g1" src="x3.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 3</span>. </span><span class="ltx_text" style="font-size:90%;">Comparison of RL systems. (a)-(d) adopt GPU-direct communication to perform global weight synchronization, while (c) needs to copy the previous weight version to dedicate memory buffers for later synchronization. (e) performs asynchronous weight synchronization through RDMA on CPU. <math alttext="\text{W}_{\text{n}}" class="ltx_Math" display="inline" id="S2.F3.m2" intent=":literal"><semantics><msub><mtext>W</mtext><mtext>n</mtext></msub><annotation encoding="application/x-tex">\text{W}_{\text{n}}</annotation></semantics></math> is the actor weight version at iteration n.
</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3. </span>Limitations of existing asynchronous RL systems</h3>
<div class="ltx_para ltx_noindent" id="S2.SS3.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Limited scalability due to global weight synchronization.</span>
Recent asynchronous RL training systems for LLMs pipeline generation and training across different training iterations, ensuring <math alttext="k" class="ltx_Math" display="inline" id="S2.SS3.p1.m1" intent=":literal"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>-step staleness bounds <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Fu et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib19" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>; <span class="ltx_text" style="font-size:90%;">Wu et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib20" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>; <span class="ltx_text" style="font-size:90%;">Zhong et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib18" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>; <span class="ltx_text" style="font-size:90%;">Tan et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib21" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>; <span class="ltx_text" style="font-size:90%;">Team et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib22" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>; <span class="ltx_text" style="font-size:90%;">Han et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib23" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>)</cite>.
Within an RL iteration, the actor processes a full batch of trajectories by dividing it into smaller mini-batches and conducting a model update on each mini-batch <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Schulman et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib41" title=""><span class="ltx_text" style="font-size:90%;">2017</span></a>; <span class="ltx_text" style="font-size:90%;">Shao et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib38" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>. The actor’s final model weights for that iteration are only ready after the last mini-batch is processed.
In a widely used <span class="ltx_text ltx_font_italic">one-step staleness pipeline</span> <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Tan et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib21" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>; <span class="ltx_text" style="font-size:90%;">Zhong et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib18" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>)</cite> shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#S2.F3" title="Figure 3 ‣ 2.2. Recent Trends in RL Post-Training ‣ 2. Background and Motivation ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_tag">3</span></a>(b),
rollouts generate a full batch of trajectories using model weights from the past iteration, while concurrently the actor is trained on an older, completely generated batch.
Some concurrent research <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Zhong et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib18" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>; <span class="ltx_text" style="font-size:90%;">Han et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib23" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>)</cite> introduces a <span class="ltx_text ltx_font_italic">streaming generation</span> approach.
As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#S2.F3" title="Figure 3 ‣ 2.2. Recent Trends in RL Post-Training ‣ 2. Background and Motivation ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_tag">3</span></a>(c),
the actor is trained first on short trajectories in early
mini-batches
while consuming long trajectories in later
mini-batches.
Both systems create a strong data dependency: trainer’s progress is tied to the completion of a full batch, forcing it to wait for the slowest trajectory and coupling the execution of all rollouts.</p>
</div>
<div class="ltx_para" id="S2.SS3.p2">
<p class="ltx_p">These systems also suffer from a rigid model weight dependency, as all rollouts rely on a global synchronization point to receive new weights after <math alttext="k" class="ltx_Math" display="inline" id="S2.SS3.p2.m1" intent=":literal"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math> training iterations.
This global synchronization becomes a major bottleneck in tasks with highly skewed generation times
(e.g., math competition <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">of America</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib40" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>),
failing to effectively hide the long generation time of a few tail trajectories and creating substantial pipeline bubbles.
It limits
scalability, as adding more GPUs to increase rollouts cannot mitigate the wait for long-tail generation,
and allocating additional GPUs per rollout
provides only marginal latency reductions,
as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#S2.F4" title="Figure 4 ‣ 2.4. Opportunity and Challenges ‣ 2. Background and Motivation ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<div class="ltx_para" id="S2.SS3.p3">
<p class="ltx_p">Other asynchronous RL systems <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Seed et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib8" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>; <span class="ltx_text" style="font-size:90%;">Fu et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib19" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>; <span class="ltx_text" style="font-size:90%;">Team et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib9" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>; <span class="ltx_text" style="font-size:90%;">Wu et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib20" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>)</cite> improve the global synchronization design with <span class="ltx_text ltx_font_italic">partial rollout</span>.
It interrupts all ongoing trajectories generation to apply the latest actor model in rollouts, and then continues trajectory generation using the latest model weights (Figure <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#S2.F3" title="Figure 3 ‣ 2.2. Recent Trends in RL Post-Training ‣ 2. Background and Motivation ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_tag">3</span></a>(d)).
A single long trajectory is then composed of several segments, each generated by a different policy version.
While this reduces long-tail bubbles, it introduces two main issues. (1) The pause-and-sync cycle incurs significant overhead by forcing rollouts to rebuild the KVCache (i.e., re-prefill) for every interrupted trajectory, repeatedly in each RL iteration,
wasting GPU resources without advancing generation.
(2) Generating a single response with inconsistent policy versions
can harm model convergence,
resulting in slower convergence as we show experimentally in §<a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#S8.SS2" title="8.2. Training Convergence ‣ 8. Evaluation ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_tag">8.2</span></a>.
A comprehensive discussion with more related works is provided in §<a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#S9" title="9. Related Work ‣ 8.5. Fault Tolerance Analysis ‣ 8.4. Repack Efficiency ‣ 8.3. Weight Synchronization OverheadIn 8.2. Training Convergence ‣ 8. Evaluation ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_tag">9</span></a>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS3.p4">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Undesirable coupling between system throughput and training stability.</span>
A core challenge in existing asynchronous RL systems is selecting an appropriate staleness bound <math alttext="k" class="ltx_Math" display="inline" id="S2.SS3.p4.m1" intent=":literal"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>.
A larger <math alttext="k" class="ltx_Math" display="inline" id="S2.SS3.p4.m2" intent=":literal"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math> can better hide generation latency and improve system throughput. However, it also increases the divergence between the data-generating policy and the policy being trained, which can harm model convergence.
Conversely, a smaller <math alttext="k" class="ltx_Math" display="inline" id="S2.SS3.p4.m3" intent=":literal"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math> reduces this divergence but is less effective at masking generation latency, leading to poor throughput.
Finding a suitable <math alttext="k" class="ltx_Math" display="inline" id="S2.SS3.p4.m4" intent=":literal"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math> is a difficult, task-specific tuning problem.</p>
</div>
<div class="ltx_para" id="S2.SS3.p5">
<p class="ltx_p">This problem is compounded by the dynamic nature of RL training. As an LLM learns, trajectory length often changes significantly <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Guo et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib2" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>; <span class="ltx_text" style="font-size:90%;">Team et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib9" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>; <span class="ltx_text" style="font-size:90%;">Yu et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib3" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>)</cite>. A staleness bound <math alttext="k" class="ltx_Math" display="inline" id="S2.SS3.p5.m1" intent=":literal"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math> that is optimal early in training can become inefficient or unstable over time. Existing systems treat <math alttext="k" class="ltx_Math" display="inline" id="S2.SS3.p5.m2" intent=":literal"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math> as a static hyperparameter,
creating a rigid parameter dependency
that fails to adapt to evolving training dynamics.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4. </span>Opportunity and Challenges</h3>
<div class="ltx_para ltx_noindent" id="S2.SS4.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Opportunity: Adopting trajectory-level asynchrony to scale up RL.</span>
Existing asynchronous RL frameworks are fundamentally constrained by a global synchronization point to update all rollouts.
The key opportunity lies in decoupling individual trajectories generation from this global lockstep, enabling <span class="ltx_text ltx_font_italic">trajectory-level asynchrony</span>.
It allows each trajectory generation to complete at its own pace on a consistent rollout model version and be stored in an experience buffer.
Rollouts operate independently and do not stall the execution of one another
(Figure <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#S2.F3" title="Figure 3 ‣ 2.2. Recent Trends in RL Post-Training ‣ 2. Background and Motivation ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_tag">3</span></a>(e)).
This effectively masks generation delays of longer trajectories and
allows shorter trajectories to be promptly available for sampling.
The fully decoupled trainer samples a batch from the experience buffer
without interrupting ongoing rollout generation.
Importantly, the staleness of each trajectory (due to the model version it is generated on) emerges naturally from its generation latency, rather than due to a static staleness bound (§<a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#S6" title="6. Trajectory-level Asynchrony Analysis ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_tag">6</span></a>).
The staleness varies according to evolving trajectory lengths and diverse environmental latencies throughout training.</p>
</div>
<div class="ltx_para" id="S2.SS4.p2">
<p class="ltx_p">We identify the following challenges in achieving this trajectory-level asynchrony and scaling up RL.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS4.p3">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Challenge 1: Support asynchronous weight synchronization between actor and rollouts.</span>
Implementing tra- jectory-level asynchrony requires asynchronous and fine-grained weight synchronization,
where a rollout fetches the latest actor weights as soon as it completes a batch’s generation.
As each rollout operates independently and finishes generation at its own pace, rollout updates can occur at any moment during actor training.
Existing asynchronous RL systems leverage high GPU-GPU bandwidth for parameter transfers
at global synchronization points <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Zhong et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib18" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>; <span class="ltx_text" style="font-size:90%;">Wu et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib20" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>; <span class="ltx_text" style="font-size:90%;">Fu et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib19" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>)</cite>.
This approach becomes infeasible for asynchronous rollout updates.</p>
</div>
<div class="ltx_para" id="S2.SS4.p4">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">First</span>, direct GPU transfers for asynchronous rollout updates introduce resource contention.
It requires dedicated memory buffers, yet GPU memory is a scarce and critical resource in RL training <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Sheng et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib13" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>; <span class="ltx_text" style="font-size:90%;">Mei et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib15" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>; <span class="ltx_text" style="font-size:90%;">Zhong et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib16" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>. To maximize throughput, both actor and rollouts operate near peak memory capacity; actor uses large training batches, while rollouts expand their KVCache for larger decode batches <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Qin et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib50" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>)</cite>. However, asynchronous transfers require buffering. As discussed in §<a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#S2.SS3" title="2.3. Limitations of existing asynchronous RL systems ‣ 2. Background and Motivation ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_tag">2.3</span></a>, the final model weights from an RL iteration are not available until the last mini-batch has been trained. Consequently, to support on-demand pulls from rollouts during a training iteration, the actor must hold the previous iteration’s weights in a buffer. Similarly, to handle proactive pushes from the actor, a rollout must buffer incoming weights until its batch generation completes.
For large models, the buffer can exceed available GPU memory. Even if it fits, this would reduce the training batch size or KVCache capacity, degrading system throughput.
Furthermore, the communication kernels (e.g., NCCL)
may compete for the GPU’s streaming multiprocessors with training and decoding kernels, stalling computation on both actor and rollouts.</p>
</div>
<div class="ltx_para" id="S2.SS4.p5">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Second</span>, without these buffers, the actor must stall until rollouts fetch the new weights
before it can proceed to the next mini-batch’s training, in an RL iteration.
This reintroduces a blocking synchronization point, defeating the purpose of trajectory-level asynchrony, especially as the number of rollouts increases.</p>
</div>
<div class="ltx_para" id="S2.SS4.p6">
<p class="ltx_p">We seek to minimize GPU idle time
while
introducing zero extra GPU memory consumption
on both the actor and rollouts during asynchronous weight synchronization, by introducing a tier of relay workers
(§<a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#S4" title="4. Asynchronous Weight Synchronization Using Relay Workers ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_tag">4</span></a>).</p>
</div>
<figure class="ltx_figure" id="S2.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="324" id="S2.F4.g1" src="x4.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 4</span>. </span><span class="ltx_text" style="font-size:90%;">
One-step decode latency of Qwen2.5-7B/32B on H800 GPUs, under various tensor parallel (TP) sizes with decode batch sizes up to the KVCache limit.
</span></figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S2.SS4.p7">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Challenge 2: Resolve the long-tail generation problem at each rollout.</span>
While the relay worker design allows rollouts to operate independently, individual rollouts can still get stuck in long-tail generation
(Figure <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#S2.F3" title="Figure 3 ‣ 2.2. Recent Trends in RL Post-Training ‣ 2. Background and Motivation ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_tag">3</span></a>(e)).
Some rollouts may eventually run only a few trajectories’ generation,
resulting in low utilization of their GPUs.
More critically, these rollouts are prevented from fetching the latest policy (model weights) from the actor, forcing them to continue generating trajectories with increasingly stale weights.
Maximizing the number of rollouts that generate near-on-policy trajectories is crucial for training stability and model performance.</p>
</div>
<div class="ltx_para" id="S2.SS4.p8">
<p class="ltx_p">We leverage that LLM decoding is a memory-bound operation <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Kwon et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib51" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>; <span class="ltx_text" style="font-size:90%;">Zhu et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib52" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>; <span class="ltx_text" style="font-size:90%;">Zheng et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib53" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite>.
Rollout replicas stuck on long-tail generations are left with a very small decode batch (e.g., ¡8) of incomplete trajectories. For a memory-bound operation,
decoding such a small batch has nearly the same latency as a much larger one (e.g., 64) <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Agrawal et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib54" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>; <span class="ltx_text" style="font-size:90%;">Sun et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib55" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>; <span class="ltx_text" style="font-size:90%;">Zhu et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib56" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>)</cite>,
as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#S2.F4" title="Figure 4 ‣ 2.4. Opportunity and Challenges ‣ 2. Background and Motivation ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_tag">4</span></a>.
This motivates us to consolidate incomplete trajectories from multiple rollouts using the same weight version onto a few destination rollouts, forming a larger decode batch with negligible impact on latency.
The released replicas can then be updated with the latest actor weights to produce fresh on-policy trajectories.</p>
</div>
<div class="ltx_para" id="S2.SS4.p9">
<p class="ltx_p">However, the dynamic nature of RL training complicates this consolidation.
The central challenge is determining when to trigger consolidation and which rollout replicas are involved,
as identifying genuinely underutilized rollouts is non-trivial.
Prior research has relied on task- and model-specific metrics that require extensive offline profiling to define static thresholds to identify underutilization <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Zhong et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib16" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>, which are impractical for dynamic RL workloads
due to the prohibitive overhead of repeated profiling.
We design an efficient solution that uses a lightweight indicator based on KVCache utilization to dynamically make consolidation decisions
(§<a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#S5" title="5. Bubble Elimination in Long-tail Trajectory Generation ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_tag">5</span></a>).</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS4.p10">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Challenge 3: Ensure robust RL training at scale.</span>
Production-level RL post-training jobs are scaling to thousands of GPUs and can run for weeks or months <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">xAI</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib6" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>; <span class="ltx_text" style="font-size:90%;">Team et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib57" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>)</cite>.
At this scale, hardware faults are inevitable.
As the rollout generation stage often dominates RL iteration time,
resilience of rollouts is critical to overall training progress.
While the trainer can leverage checkpointing for recovery, existing RL systems provide no fault tolerance for the rollout stage.
In addition, GPU-direct communication with NCCL <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">ncc</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib58" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite> is typically used for weight synchronization <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Fu et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib19" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>; <span class="ltx_text" style="font-size:90%;">Zhong et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib18" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>; <span class="ltx_text" style="font-size:90%;">Hu et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib17" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>; <span class="ltx_text" style="font-size:90%;">Sheng et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib13" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>)</cite>, and NCCL lacks native support for fault tolerance and elasticity <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Hu et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib59" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>)</cite>.
Consequently, a single rollout failure can force a full job to restart from a checkpoint, preventing fast recovery needed for long-running workloads.</p>
</div>
<div class="ltx_para" id="S2.SS4.p11">
<p class="ltx_p">In existing systems, each rollout independently manages its active trajectories being generated. A machine failure results in the complete loss of all in-progress work on that node.
This loss is particularly costly for complex tasks <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Jimenez et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib7" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>; <span class="ltx_text" style="font-size:90%;">OpenAI</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib32" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>)</cite>, where a single trajectory can take hours to generate.
Regenerating trajectories during failure recovery wastes valuable GPU cycles. Our fully decoupled architecture enables robust long-running RL training by isolating each system component, ensuring rapid and independent recovery
(§<a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#S3.SS3" title="3.3. Fault Tolerance and Recovery ‣ 3. Fully Decoupled Architecture Design ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_tag">3.3</span></a>, §<a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#S4.SS3" title="4.3. Fault-Tolerant Relay Broadcast ‣ 4. Asynchronous Weight Synchronization Using Relay Workers ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_tag">4.3</span></a>).</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Fully Decoupled Architecture Design</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p">We introduce Laminar, a scalable and robust asynchronous RL post-training framework designed to scale out RL training jobs, while ensuring robust training.
Laminar achieves full decoupling of both data dependencies (i.e., decoupling trainer consumption from trajectory batch generation) and parameter dependencies (i.e., no lock-step weight synchronization)
between the actor
and all rollout replicas, as well as among rollouts themselves.
Such decoupling enables trajectory generation, actor model training, and weight synchronization between actor and rollouts to proceed independently.
It eliminates any global synchronization bottlenecks that limit scalability, and effectively isolates faults to ensure rapid, non-disruptive recovery and long-running training.
Enabling trajectory-level asynchrony, our framework allows each trajectory’s generation to complete at its own optimal pace, while maintaining training stability with diverse trajectories under minimal staleness.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>System Components</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p">Figure <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#S3.F5" title="Figure 5 ‣ 3.2. Training Workflow ‣ 3. Fully Decoupled Architecture Design ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_tag">5</span></a> gives the architecture of Laminar, consisting of four core modules:</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p2">
<p class="ltx_p"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.SS1.p2.m1" intent=":literal"><semantics><mo>∙</mo><annotation encoding="application/x-tex">\bullet</annotation></semantics></math><span class="ltx_text ltx_font_italic">Rollout Module</span> consists of a rollout manager and numerous rollouts.
The manager runs on a CPU machine, isolating it from any failures of GPU machines.
It coordinates the rollouts by monitoring their workload and applying trajectory repacking accordingly.
It also ensures system stability by monitoring rollout health, managing the weight update topology, and handling fault recovery.
Each rollout performs auto-regressive generation to produce trajectories independently and may interact with external environments during generation.
After generating its own batch sampled from the prompt pool,
each rollout fetches the latest actor model weights from its colocated relay worker.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p3">
<p class="ltx_p"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.SS1.p3.m1" intent=":literal"><semantics><mo>∙</mo><annotation encoding="application/x-tex">\bullet</annotation></semantics></math><span class="ltx_text ltx_font_italic">Data Module</span> manages the lifecycle of trajectories through three distinct storage components, each running as a separate process and storing data in a CPU machine:
a prompt pool to supply initial states for generation, such as a math or coding question;
a partial response pool that centrally stores
in-progress trajectories to ensure fault tolerance; and an experience buffer that holds completely generated trajectories.
Interaction with this buffer is managed by a writer and a sampler.
Flexible APIs are provided for both writer and sampler,
allowing users to customize the sampling strategy for training and the eviction strategy for removing old experiences when buffer capacity is not enough.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p4">
<p class="ltx_p"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.SS1.p4.m1" intent=":literal"><semantics><mo>∙</mo><annotation encoding="application/x-tex">\bullet</annotation></semantics></math><span class="ltx_text ltx_font_italic">Relay Workers</span> function as a hierarchical parameter service.
The rollout manager designates one relay as the master, which receives updated weights from the actor and broadcasts them to the other relays.
Each relay is a separate CPU process running on each rollout GPU machine, hosting the latest actor model weights in local CPU memory. This allows any rollout replica to pull a new version on demand without blocking GPU computation on trainer and other rollouts.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p5">
<p class="ltx_p"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.SS1.p5.m1" intent=":literal"><semantics><mo>∙</mo><annotation encoding="application/x-tex">\bullet</annotation></semantics></math><span class="ltx_text ltx_font_italic">Trainer</span> samples batches of experiences from the experience buffer to perform model updates according to the selected RL algorithm, and may involve multiple models, such as actor, critic, and reference models <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Schulman et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib41" title=""><span class="ltx_text" style="font-size:90%;">2017</span></a>; <span class="ltx_text" style="font-size:90%;">Shao et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib38" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>.
The models are colocated on the same set of GPUs, and are executed sequentially in a time-sharing manner <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Sheng et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib13" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>; <span class="ltx_text" style="font-size:90%;">Yao et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib12" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>Training Workflow</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p">The continuous, asynchronous training workflow of Laminar is designed to maintain high training throughput when scaling up.
It begins as rollouts pull prompts from the prompt pool to generate trajectories on their GPUs
(step ①).
For fault tolerance, in-progress trajectories are streamed to the partial response pool (step ②). Upon generation completion, they are moved to the experience buffer (step ③).
In parallel with rollout generation, the trainer samples completed trajectories from the experience buffer
to perform model training (step ④). This fundamental decoupling of data production from consumption is key to the system’s scalability.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p">After a model update, the trainer pushes the new actor weights to the master relay and immediately resumes its next training iteration without waiting for weights to be fully distributed to other relays or rollouts (step ⑤).
The master relay then broadcasts the new weights directly to all other relays using RDMA, which occurs in the background on CPU memory without affecting ongoing GPU-based generation on the same machine (step ⑥).
A rollout can fetch the latest weights from its colocated relay at any time, over high-speed PCIe with minimal latency (step ⑦).
This decouples parameter dependencies across the actor and all rollouts in the system.</p>
</div>
<figure class="ltx_figure" id="S3.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="434" id="S3.F5.g1" src="x5.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 5</span>. </span><span class="ltx_text" style="font-size:90%;">Laminar architecture and training workflow.
</span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p">Meanwhile, Laminar actively manages workload imbalance on rollouts to maintain high generation throughput. Rollout manager monitors rollouts stuck on long-tail trajectories generation and triggers a repack mechanism (step ⑧).
The repack consolidates trajectories from underutilized rollouts onto fewer rollout, freeing the former which can pull the latest model weights and reducing system-wide staleness.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3. </span>Fault Tolerance and Recovery</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p">The decoupled architecture in Laminar
allows dynamic removal and addition of rollouts without halting training and losing data, attaining elasticity. Individual faults in rollouts, relays, and the trainer are isolated,
enabling rapid recovery without a costly global restart.
This resilience is critical for large-scale training and is handled in key stages of the training workflow.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p">When a fault occurs during rollout generation,
our heartbeat-based failover mechanism
quickly detects a faulty rollout and notifies the rollout manager. We first attempt to recover by re-initializing the faulty replica on the same GPUs. If the fault persists after re-initialization, we then evict the entire affected machine. During this, in-progress trajectory states remain safe in the partial response pool.
The rollout manager redirects interrupted trajectories to healthy rollouts with the same weight version, or waits for replacement machines if none are available.
Recovery is swift as new machines initialize rollouts and corresponding relays by synchronizing with the master relay for the latest weights
or loading specific weight versions from actor checkpointing files.</p>
</div>
<div class="ltx_para" id="S3.SS3.p3">
<p class="ltx_p">Relay faults during weight synchronization are managed by our fault-tolerant relay broadcast mechanism (§<a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#S4.SS3" title="4.3. Fault-Tolerant Relay Broadcast ‣ 4. Asynchronous Weight Synchronization Using Relay Workers ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_tag">4.3</span></a>). Faulty relay workers are detected instantly without a timeout, and the communication scheduler rebuilds the broadcast chain using healthy nodes.
This recovery can be completed within seconds without disrupting ongoing rollout generation.</p>
</div>
<div class="ltx_para" id="S3.SS3.p4">
<p class="ltx_p">Trainer faults are handled by standard checkpoint recovery methods <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Wagenländer et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib60" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>; <span class="ltx_text" style="font-size:90%;">Wan et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib61" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>; <span class="ltx_text" style="font-size:90%;">Xingda Wei and Chen</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib62" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>)</cite>, with actor model weights checkpointed periodically. When a trainer worker fails, it is evicted  <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">ama</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib63" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>; <span class="ltx_text" style="font-size:90%;">Hu et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib64" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite> and then recovered from the latest checkpoint. During this period, rollouts continue generation with the latest available weights.
Once recovered, the actor resumes sampling from the experience buffer and resumes training.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Asynchronous Weight Synchronization Using Relay Workers</h2>
<figure class="ltx_figure" id="S4.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="434" id="S4.F6.g1" src="x6.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 6</span>. </span><span class="ltx_text" style="font-size:90%;">Asynchronous weight synchronization workflow.</span></figcaption>
</figure>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>Design Considerations</h3>
<div class="ltx_para ltx_noindent" id="S4.SS1.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Inefficiency of using a storage system.</span>
Traditional RL systems often involve a storage system for weights synchronization between the actor and rollout models.
SRL <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Mei et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib65" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite> relies on a file system (NFS <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Batsakis et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib66" title=""><span class="ltx_text" style="font-size:90%;">2009</span></a>)</cite>) and OpenAI-Five <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">OpenAI et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib67" title=""><span class="ltx_text" style="font-size:90%;">2019</span></a>)</cite> utilizes key-value stores (Redis <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">S</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib68" title=""><span class="ltx_text" style="font-size:90%;">2009</span></a>)</cite>) to transfer and store the published weights.
When model sizes were measured in megabytes, weight synchronization through these storage systems incurs reasonable delay; however, they are ill-suited for LLMs with gigabytes scale of model sizes.
<span class="ltx_text ltx_font_italic">First</span>, substantial serialization and I/O overhead is incurred.
Our profiling of a 32B LLM shows that serializing a single 4GB model shard takes approximately 8 seconds.
Transferring this volume of data over a standard TCP network to/from the storage system
adds another 10 to 20 seconds of latency into the critical path of every rollout.
<span class="ltx_text ltx_font_italic">Next</span>, the storage system renders a contention bottleneck when numerous rollouts simultaneously pull the latest
weights, incurring long and unpredictable latencies for weight updates.
Instead, we exploit local host memory and high-bandwidth networks like RDMA for efficient weight synchronization, avoiding serialization and a single point of contention.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p2">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Hosting relay workers in local host memory.</span>
Direct GPU-to-GPU transfers for asynchronous weight synchronization incur prohibitive memory overhead and computational stalls (§<a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#S2.SS4" title="2.4. Opportunity and Challenges ‣ 2. Background and Motivation ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_tag">2.4</span></a>).
Allocating separate GPUs dedicated as relays would be computationally wasteful and cost-prohibitive.
We introduce a tier of relay workers that run in CPU memory of rollout machines,
exploiting that such host memory remains largely underutilized.
The CPU-based intermediaries decouple the actor model weights from asynchronous rollout demands and leverage RDMA for efficient and robust weight synchronization among machines.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>Hierarchical Relays and Their Workflow</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p">Our relay worker hierarchy is designed to attain two goals: <span class="ltx_text ltx_font_bold">1)</span> to minimize actor stall time during weight publication, and <span class="ltx_text ltx_font_bold">2)</span> to ensure low-latency access for any rollout requesting a new version.
For the first goal, we designate a single master relay: the actor transfers its updated weights to this master relay and can immediately resume training, effectively hiding the broadcast latency (step <math alttext="①" class="ltx_Math" display="inline" id="S4.SS2.p1.m1" intent=":literal"><semantics><mn>①</mn><annotation encoding="application/x-tex">①</annotation></semantics></math> in Figure <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#S4.F6" title="Figure 6 ‣ 4. Asynchronous Weight Synchronization Using Relay Workers ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_tag">6</span></a>).
After receiving the latest weights, the master relay reshards them according to the rollout sharding strategy (e.g., tensor parallelism among GPUs a rollout runs on)
 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Sheng et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib13" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>; <span class="ltx_text" style="font-size:90%;">Yao et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib12" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>; <span class="ltx_text" style="font-size:90%;">Lei et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib14" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>; <span class="ltx_text" style="font-size:90%;">Mei et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib15" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>.
A single master relay may become a bottleneck when serving weight requests from multiple rollouts. We further adopt one relay worker per rollout machine to distribute the workload, and enable rollout’s low-latency access to updated weights anytime.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p">The updated weights must be efficiently propagated from the master relay to all other relays. We implement this with
a chain-based pipelined broadcast
using RDMA (step <math alttext="②" class="ltx_Math" display="inline" id="S4.SS2.p2.m1" intent=":literal"><semantics><mn>②</mn><annotation encoding="application/x-tex">②</annotation></semantics></math>).
The broadcast pipelines model chunk transfers along a chain of relays, overlapping communication among different hops. This makes the broadcast time nearly constant regardless of the length of the chain scale <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Ikkert et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib69" title=""><span class="ltx_text" style="font-size:90%;">2009</span></a>)</cite>, which we formally analyze in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#A4" title="Appendix D Theoretical Analysis of Chain-Based Pipelined Broadcast ‣ A.2. Hyperparameter Details ‣ A.1. Response Length Distribution of Each Model ‣ Appendix A Experiment Details ‣ 10. Conclusion ‣ 9. Related Work ‣ 8.5. Fault Tolerance Analysis ‣ 8.4. Repack Efficiency ‣ 8.3. Weight Synchronization OverheadIn 8.2. Training Convergence ‣ 8. Evaluation ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_tag">D</span></a>.
Broadcast time of less than 1.6 seconds is incurred for a 72B model from the master to 127
other relays (Figure <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#A1.F19" title="Figure 19 ‣ D.1. Communication Model ‣ Appendix D Theoretical Analysis of Chain-Based Pipelined Broadcast ‣ A.2. Hyperparameter Details ‣ A.1. Response Length Distribution of Each Model ‣ Appendix A Experiment Details ‣ 10. Conclusion ‣ 9. Related Work ‣ 8.5. Fault Tolerance Analysis ‣ 8.4. Repack Efficiency ‣ 8.3. Weight Synchronization OverheadIn 8.2. Training Convergence ‣ 8. Evaluation ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_tag">19</span></a> in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#A4" title="Appendix D Theoretical Analysis of Chain-Based Pipelined Broadcast ‣ A.2. Hyperparameter Details ‣ A.1. Response Length Distribution of Each Model ‣ Appendix A Experiment Details ‣ 10. Conclusion ‣ 9. Related Work ‣ 8.5. Fault Tolerance Analysis ‣ 8.4. Repack Efficiency ‣ 8.3. Weight Synchronization OverheadIn 8.2. Training Convergence ‣ 8. Evaluation ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_tag">D</span></a>).
This broadcast time is negligible compared to the lengthy trajectory generation time at hundreds to thousands of seconds <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Yu et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib3" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>; <span class="ltx_text" style="font-size:90%;">Feng et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib39" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>; <span class="ltx_text" style="font-size:90%;">Jimenez et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib7" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite>.
Rollouts fetch the latest weights through PCIe links from their colocated relay at any time, without waiting for the resharding and broadcast to complete (step <math alttext="③" class="ltx_Math" display="inline" id="S4.SS2.p2.m2" intent=":literal"><semantics><mn>③</mn><annotation encoding="application/x-tex">③</annotation></semantics></math>).</p>
</div>
<figure class="ltx_figure" id="S4.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="350" id="S4.F7.g1" src="x7.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 7</span>. </span><span class="ltx_text" style="font-size:90%;">Swift recovery during relay broadcast.</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3. </span>Fault-Tolerant Relay Broadcast</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p">As RL training scales to thousands of GPUs, hardware and software faults become inevitable.
We design a fault-tolerant relay broadcast scheme. The communication scheduler in
the rollout manager
establishes the broadcast pipeline connecting the relay workers
on
rollout machines.
The master relay receives weights from the actor, chunks them, and broadcasts them down the chain in a pipelined manner.
If a rollout machine fails, the scheduler detects the failure via heartbeat monitoring.
The failed machine is evicted and the scheduler immediately reconstructs the broadcast chain
among the remaining rollout machines, as depicted in Figure <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#S4.F7" title="Figure 7 ‣ 4.2. Hierarchical Relays and Their Workflow ‣ 4. Asynchronous Weight Synchronization Using Relay Workers ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_tag">7</span></a>.
This repair is a constant-time (<math alttext="O" class="ltx_Math" display="inline" id="S4.SS3.p1.m1" intent=":literal"><semantics><mi>O</mi><annotation encoding="application/x-tex">O</annotation></semantics></math>(1)) operation that can be completed in less than one second.
If the master relay fails, the rollout manager selects a new master from the available relays and rebuilds the broadcast chain. The trainer is then notified of the new master relay’s IP address, for resuming weight distribution.
During this fast repair, RL training continues seamlessly, and rollout generation on healthy rollout machines is not affected, as rollout generation and relay communication are isolated on different processes in each machine (§<a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#S7" title="7. Implementation ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_tag">7</span></a>).</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Bubble Elimination in Long-tail Trajectory Generation</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p">Each rollout fetches the last model weight upon its completion of a trajectory batch generation. At any time, different versions of the model weights can be used by different rollouts in trajectory generation, while groups of rollouts are performing generation using the same weight versions.
To remove GPU idle time on rollouts generating long-tail trajectories, we introduce a repack mechanism that consolidates in-progress trajectory generation from these straggler rollouts onto a few designated rollouts within the same weight version group.
Our design involves three key aspects: establishing the workflow of repacking, monitoring rollouts trapped in long-tail generation, and determining suitable repack destination rollouts.</p>
</div>
<figure class="ltx_figure" id="S5.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="402" id="S5.F8.g1" src="x8.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 8</span>. </span><span class="ltx_text" style="font-size:90%;">Workflow of the Repack Mechanism. Dashed timeline shows the weight version in the trainer.
</span></figcaption>
</figure>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1. </span>Workflow of the Repack Mechanism</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p">Our repack mechanism is primarily triggered by a periodic check (e.g., 5 seconds),
which detects if any rollout replicas are trapped in long-tail trajectories generation and thus being underutilized. Additionally, a repack is also initiated immediately after the trainer completes weight updating on a global batch,
aiming to more rapidly free up rollouts for on-policy trajectories generation with the latest model version.</p>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p">The repack workflow, given in Figure <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#S5.F8" title="Figure 8 ‣ 5. Bubble Elimination in Long-tail Trajectory Generation ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_tag">8</span></a>, begins with the rollout manager collecting progress metrics from all rollouts
and grouping them by their model weight versions
(step <math alttext="①" class="ltx_Math" display="inline" id="S5.SS1.p2.m1" intent=":literal"><semantics><mn>①</mn><annotation encoding="application/x-tex">①</annotation></semantics></math>).
Within each group, the rollout manager
identifies rollouts undergoing long-tail generation
using an idleness metric (§<a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#S5.SS2" title="5.2. Online Trajecgtory Repacking ‣ 5. Bubble Elimination in Long-tail Trajectory Generation ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_tag">5.2</span></a>). Then a packing algorithm is performed to decide the repacking plan that consolidates in-progress trajectories from multiple rollouts into fewer destination rollouts (step <math alttext="②" class="ltx_Math" display="inline" id="S5.SS1.p2.m2" intent=":literal"><semantics><mn>②</mn><annotation encoding="application/x-tex">②</annotation></semantics></math>). The rollout manager then transfers unfinished trajectories of the long-tail generation rollouts to destination rollouts accordingly (step <math alttext="③" class="ltx_Math" display="inline" id="S5.SS1.p2.m3" intent=":literal"><semantics><mn>③</mn><annotation encoding="application/x-tex">③</annotation></semantics></math>).</p>
</div>
<figure class="ltx_figure" id="S5.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="325" id="S5.F9.g1" src="x9.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 9</span>. </span><span class="ltx_text" style="font-size:90%;">KVCache utilization lifecycle during rollout generation. Usage ramps up to a threshold, then remains steady while remaining trajectories decrease, and finally falls, marking the idle phase that enables trajectory repacking.</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2. </span>Online Trajecgtory Repacking</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p">Upon each triggering, the packing algorithm dynamically partitions a group of rollouts into sources (conducting long-tail generation to be released) and destinations (to consolidate long-tail trajectories generation onto).</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS2.p2">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Idleness metric.</span>
We design the idleness metric to identify GPU-underutilized rollouts.
Prior approaches rely on a predefined threshold of remaining generation requests to detect the long-tail effects <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Zhong et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib16" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>.
Determining an optimal threshold requires extensive, per-job offline profiling, which is impractical for RL deployments where workloads can vary significantly.
Instead, we leverage <span class="ltx_text ltx_font_italic">KVCache utilization</span> as a more direct indicator of resource pressure in memory-bound LLM generation.
Our observation is that KVCache capacity is the primary barrier to scaling the parallel-decode batch size in memory-bound rollout generation <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Kwon et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib51" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>; <span class="ltx_text" style="font-size:90%;">Zhu et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib52" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>; <span class="ltx_text" style="font-size:90%;">Zheng et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib53" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite>.
We have observed that the latency per decode step remains stable even as the batch size increases (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#S2.SS4" title="2.4. Opportunity and Challenges ‣ 2. Background and Motivation ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_tag">2.4</span></a>).</p>
</div>
<div class="ltx_para" id="S5.SS2.p3">
<p class="ltx_p">Figure <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#S5.F9" title="Figure 9 ‣ 5.1. Workflow of the Repack Mechanism ‣ 5. Bubble Elimination in Long-tail Trajectory Generation ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_tag">9</span></a>,
whose results are from a 32B model generating a batch of 512 trajectories on H800 GPUs (TP=4),
shows that a rollout’s KVCache usage follows a distinct lifecycle according to a natural threshold, <math alttext="C_{\text{max}}" class="ltx_Math" display="inline" id="S5.SS2.p3.m1" intent=":literal"><semantics><msub><mi>C</mi><mtext>max</mtext></msub><annotation encoding="application/x-tex">C_{\text{max}}</annotation></semantics></math> (e.g., above 99% of total KVCache),
which indicates full KVCache utilization.
Initially, usage ramps up to this threshold as the rollout is filled with active token generation of trajectories.
KVCache utilization then remains at this peak even as trajectories finish. This is because waiting trajectories in the batch immediately fill the newly available KVCache space. The KVCache usage begins to fall from this peak only when no waiting trajectories are left and the remaining running trajectories complete.
We consider the rollout idle from this time on and its remaining long trajectory generations become candidates for repacking.
This consistent threshold behavior across different RL workloads eliminates the need for workload-specific threshold tuning, facilitating our identification of source rollouts.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS2.p4">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Repacking algorithm.</span>
We consider trajectory repacking as a bin packing problem <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Johnson</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib70" title=""><span class="ltx_text" style="font-size:90%;">1974</span></a>; <span class="ltx_text" style="font-size:90%;">Yao</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib71" title=""><span class="ltx_text" style="font-size:90%;">1980</span></a>)</cite>, where each underutilized rollout represents an item
and each destination rollout acts as a bin.
The destinations are selected from the pool of underutilized rollouts
to achieve
the most densely packed KVCache after the repack.
Our primary goal is to maximize the number of released source rollouts (aka minimize the number of destination rollouts)
with minimal overhead and negligible impact on generation latency (verified in §<a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#S8.SS4" title="8.4. Repack Efficiency ‣ 8.3. Weight Synchronization OverheadIn 8.2. Training Convergence ‣ 8. Evaluation ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_tag">8.4</span></a>).
We design an efficient heuristic algorithm (Algorithm <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#alg1" title="Algorithm 1 ‣ 5.2. Online Trajecgtory Repacking ‣ 5. Bubble Elimination in Long-tail Trajectory Generation ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_tag">1</span></a>), inspired by the Best-Fit approach <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Johnson et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib72" title=""><span class="ltx_text" style="font-size:90%;">1974</span></a>)</cite>.</p>
</div>
<div class="ltx_para" id="S5.SS2.p5">
<p class="ltx_p">We define a rollout’s capacity using two key indicators: its KVCache utilization, <math alttext="C_{\text{used}}" class="ltx_Math" display="inline" id="S5.SS2.p5.m1" intent=":literal"><semantics><msub><mi>C</mi><mtext>used</mtext></msub><annotation encoding="application/x-tex">C_{\text{used}}</annotation></semantics></math>, and its current trajectory count <math alttext="N_{\text{reqs}}" class="ltx_Math" display="inline" id="S5.SS2.p5.m2" intent=":literal"><semantics><msub><mi>N</mi><mtext>reqs</mtext></msub><annotation encoding="application/x-tex">N_{\text{reqs}}</annotation></semantics></math>.
The KVCache threshold, <math alttext="C_{\text{max}}" class="ltx_Math" display="inline" id="S5.SS2.p5.m3" intent=":literal"><semantics><msub><mi>C</mi><mtext>max</mtext></msub><annotation encoding="application/x-tex">C_{\text{max}}</annotation></semantics></math>,
serves as a memory capacity limit that bounds the decode batch size, representing the maximum number of trajectories a single decode step can hold.
We also enforce a batch size upperbound, <math alttext="B" class="ltx_Math" display="inline" id="S5.SS2.p5.m4" intent=":literal"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math>,
representing the maximum number of trajectories that can be decoded in parallel with only a negligible increase in latency.
This limit is determined using the roofline model <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Williams et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib73" title=""><span class="ltx_text" style="font-size:90%;">2009</span></a>; <span class="ltx_text" style="font-size:90%;">Yuan et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib74" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>, which identifies the point where the decode operation transitions from being memory-bound to compute-bound. Exceeding this limit would unacceptably increase generation latency.
A rollout becomes one of the candidates <math alttext="S" class="ltx_Math" display="inline" id="S5.SS2.p5.m5" intent=":literal"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math>
for repacking if it is in its ramp-down phase (its KVCache utilization is non-increasing and below the <math alttext="C_{\text{max}}" class="ltx_Math" display="inline" id="S5.SS2.p5.m6" intent=":literal"><semantics><msub><mi>C</mi><mtext>max</mtext></msub><annotation encoding="application/x-tex">C_{\text{max}}</annotation></semantics></math> threshold)
and its remaining trajectory count is smaller than the upperbound <math alttext="B" class="ltx_Math" display="inline" id="S5.SS2.p5.m7" intent=":literal"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math> (Line 3).
Then we sort the candidate rollouts <math alttext="S" class="ltx_Math" display="inline" id="S5.SS2.p5.m8" intent=":literal"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math> and prioritize releasing those with the smallest KVCache footprint first (Line <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#alg1.l4" title="In Algorithm 1 ‣ 5.2. Online Trajecgtory Repacking ‣ 5. Bubble Elimination in Long-tail Trajectory Generation ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_tag">4</span></a>), as smaller workloads are easier to be replaced.</p>
</div>
<figure class="ltx_float ltx_float_algorithm ltx_framed ltx_framed_top" id="alg1">
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_float"><span class="ltx_text ltx_font_bold">Algorithm 1</span> </span> Best-Fit Trajectory Consolidation</figcaption>
<div class="ltx_listing ltx_listing">
<div class="ltx_listingline" id="alg1.l1">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">1:</span></span><span class="ltx_text" style="font-size:90%;">  </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Input:</span><span class="ltx_text" style="font-size:90%;"> Set of rollout replicas </span><math alttext="R" class="ltx_Math" display="inline" id="alg1.l1.m1" intent=":literal"><semantics><mi mathsize="0.900em">R</mi><annotation encoding="application/x-tex">R</annotation></semantics></math><span class="ltx_text" style="font-size:90%;">, KVCache threshold </span><math alttext="C_{\text{max}}" class="ltx_Math" display="inline" id="alg1.l1.m2" intent=":literal"><semantics><msub><mi mathsize="0.900em">C</mi><mtext mathsize="0.900em">max</mtext></msub><annotation encoding="application/x-tex">C_{\text{max}}</annotation></semantics></math><span class="ltx_text" style="font-size:90%;">, Roofline batch size </span><math alttext="B" class="ltx_Math" display="inline" id="alg1.l1.m3" intent=":literal"><semantics><mi mathsize="0.900em">B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math><span class="ltx_text" style="font-size:90%;">
</span>
</div>
<div class="ltx_listingline" id="alg1.l2">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">2:</span></span><span class="ltx_text" style="font-size:90%;">  </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Output:</span><span class="ltx_text" style="font-size:90%;"> A consolidation plan </span><math alttext="P" class="ltx_Math" display="inline" id="alg1.l2.m1" intent=":literal"><semantics><mi mathsize="0.900em">P</mi><annotation encoding="application/x-tex">P</annotation></semantics></math><span class="ltx_text" style="font-size:90%;"> of (source, destination) pairs
</span>
</div>
<div class="ltx_listingline" id="alg1.l3">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">3:</span></span><span class="ltx_text" style="font-size:90%;">  </span><math alttext="S\leftarrow\{r\in R\mid r.C_{\text{used}}&lt;\text{min}(C_{\text{max}},r.C_{\text{prev}})\land r.N_{\text{reqs}}&lt;B\}" class="ltx_math_unparsed" display="inline" id="alg1.l3.m1" intent=":literal"><semantics><mrow><mi mathsize="0.900em">S</mi><mo mathsize="0.900em" stretchy="false">←</mo><mrow><mo maxsize="0.900em" minsize="0.900em">{</mo><mi mathsize="0.900em">r</mi><mo mathsize="0.900em">∈</mo><mi mathsize="0.900em">R</mi><mo lspace="0em" mathsize="0.900em" rspace="0.167em">∣</mo><mi mathsize="0.900em">r</mi><mo lspace="0em" mathsize="0.900em" rspace="0.167em">.</mo><msub><mi mathsize="0.900em">C</mi><mtext mathsize="0.900em">used</mtext></msub><mo mathsize="0.900em">&lt;</mo><mtext mathsize="0.900em">min</mtext><mrow><mo maxsize="0.900em" minsize="0.900em">(</mo><msub><mi mathsize="0.900em">C</mi><mtext mathsize="0.900em">max</mtext></msub><mo mathsize="0.900em">,</mo><mi mathsize="0.900em">r</mi><mo lspace="0em" mathsize="0.900em" rspace="0.167em">.</mo><msub><mi mathsize="0.900em">C</mi><mtext mathsize="0.900em">prev</mtext></msub><mo maxsize="0.900em" minsize="0.900em">)</mo></mrow><mo mathsize="0.900em">∧</mo><mi mathsize="0.900em">r</mi><mo lspace="0em" mathsize="0.900em" rspace="0.167em">.</mo><msub><mi mathsize="0.900em">N</mi><mtext mathsize="0.900em">reqs</mtext></msub><mo mathsize="0.900em">&lt;</mo><mi mathsize="0.900em">B</mi><mo maxsize="0.900em" minsize="0.900em">}</mo></mrow></mrow><annotation encoding="application/x-tex">S\leftarrow\{r\in R\mid r.C_{\text{used}}&lt;\text{min}(C_{\text{max}},r.C_{\text{prev}})\land r.N_{\text{reqs}}&lt;B\}</annotation></semantics></math><span class="ltx_text" style="font-size:90%;">
</span>
</div>
<div class="ltx_listingline" id="alg1.l4">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">4:</span></span><span class="ltx_text" style="font-size:90%;">  </span><math alttext="S\leftarrow\text{sort}(S,\text{key}\leftarrow r.C_{\text{used}})" class="ltx_math_unparsed" display="inline" id="alg1.l4.m1" intent=":literal"><semantics><mrow><mi mathsize="0.900em">S</mi><mo mathsize="0.900em" stretchy="false">←</mo><mtext mathsize="0.900em">sort</mtext><mrow><mo maxsize="0.900em" minsize="0.900em">(</mo><mi mathsize="0.900em">S</mi><mo mathsize="0.900em">,</mo><mtext mathsize="0.900em">key</mtext><mo mathsize="0.900em" stretchy="false">←</mo><mi mathsize="0.900em">r</mi><mo lspace="0em" mathsize="0.900em" rspace="0.167em">.</mo><msub><mi mathsize="0.900em">C</mi><mtext mathsize="0.900em">used</mtext></msub><mo maxsize="0.900em" minsize="0.900em">)</mo></mrow></mrow><annotation encoding="application/x-tex">S\leftarrow\text{sort}(S,\text{key}\leftarrow r.C_{\text{used}})</annotation></semantics></math><span class="ltx_text" style="font-size:90%;">
</span>
</div>
<div class="ltx_listingline" id="alg1.l5">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">5:</span></span><span class="ltx_text" style="font-size:90%;">  </span><math alttext="P\leftarrow\emptyset" class="ltx_Math" display="inline" id="alg1.l5.m1" intent=":literal"><semantics><mrow><mi mathsize="0.900em">P</mi><mo mathsize="0.900em" stretchy="false">←</mo><mi mathsize="0.900em" mathvariant="normal">∅</mi></mrow><annotation encoding="application/x-tex">P\leftarrow\emptyset</annotation></semantics></math><span class="ltx_text" style="font-size:90%;">; </span><math alttext="\mathcal{E}\leftarrow\emptyset" class="ltx_Math" display="inline" id="alg1.l5.m2" intent=":literal"><semantics><mrow><mi class="ltx_font_mathcaligraphic" mathsize="0.900em">ℰ</mi><mo mathsize="0.900em" stretchy="false">←</mo><mi mathsize="0.900em" mathvariant="normal">∅</mi></mrow><annotation encoding="application/x-tex">\mathcal{E}\leftarrow\emptyset</annotation></semantics></math><span class="ltx_text" style="font-size:90%;"> </span><span class="ltx_text ltx_font_italic" style="font-size:90%;"> // Plan and set of emptied replicas</span><span class="ltx_text" style="font-size:90%;">
</span>
</div>
<div class="ltx_listingline" id="alg1.l6">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">6:</span></span><span class="ltx_text" style="font-size:90%;">  </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">for all</span><span class="ltx_text" style="font-size:90%;"> </span><math alttext="s\in S" class="ltx_Math" display="inline" id="alg1.l6.m1" intent=":literal"><semantics><mrow><mi mathsize="0.900em">s</mi><mo mathsize="0.900em">∈</mo><mi mathsize="0.900em">S</mi></mrow><annotation encoding="application/x-tex">s\in S</annotation></semantics></math><span class="ltx_text" style="font-size:90%;"> </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">do</span><span class="ltx_text" style="font-size:90%;">
</span>
</div>
<div class="ltx_listingline" id="alg1.l7">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">7:</span></span><span class="ltx_text" style="font-size:90%;">   </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">if</span><span class="ltx_text" style="font-size:90%;"> </span><math alttext="s\in\mathcal{E}" class="ltx_Math" display="inline" id="alg1.l7.m1" intent=":literal"><semantics><mrow><mi mathsize="0.900em">s</mi><mo mathsize="0.900em">∈</mo><mi class="ltx_font_mathcaligraphic" mathsize="0.900em">ℰ</mi></mrow><annotation encoding="application/x-tex">s\in\mathcal{E}</annotation></semantics></math><span class="ltx_text" style="font-size:90%;"> </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">then</span><span class="ltx_text" style="font-size:90%;">
</span>
</div>
<div class="ltx_listingline" id="alg1.l8">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">8:</span></span><span class="ltx_text" style="font-size:90%;">    </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">continue</span><span class="ltx_text" style="font-size:90%;">
</span>
</div>
<div class="ltx_listingline" id="alg1.l9">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">9:</span></span><span class="ltx_text" style="font-size:90%;">   </span><math alttext="D_{s}\leftarrow\{d\in S\mid d\notin\mathcal{E}\land d\neq s\land\text{CanFit}(d,s,P)\}" class="ltx_Math" display="inline" id="alg1.l9.m1" intent=":literal"><semantics><mrow><msub><mi mathsize="0.900em">D</mi><mi mathsize="0.900em">s</mi></msub><mo mathsize="0.900em" stretchy="false">←</mo><mrow><mo maxsize="0.900em" minsize="0.900em">{</mo><mrow><mi mathsize="0.900em">d</mi><mo mathsize="0.900em">∈</mo><mi mathsize="0.900em">S</mi></mrow><mo fence="true" lspace="0em" mathsize="0.900em" rspace="0em">∣</mo><mrow><mi mathsize="0.900em">d</mi><mo mathsize="0.900em">∉</mo><mrow><mi class="ltx_font_mathcaligraphic" mathsize="0.900em">ℰ</mi><mo mathsize="0.900em">∧</mo><mi mathsize="0.900em">d</mi></mrow><mo mathsize="0.900em">≠</mo><mrow><mi mathsize="0.900em">s</mi><mo mathsize="0.900em">∧</mo><mrow><mtext mathsize="0.900em">CanFit</mtext><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="0.900em" minsize="0.900em">(</mo><mi mathsize="0.900em">d</mi><mo mathsize="0.900em">,</mo><mi mathsize="0.900em">s</mi><mo mathsize="0.900em">,</mo><mi mathsize="0.900em">P</mi><mo maxsize="0.900em" minsize="0.900em">)</mo></mrow></mrow></mrow></mrow><mo maxsize="0.900em" minsize="0.900em">}</mo></mrow></mrow><annotation encoding="application/x-tex">D_{s}\leftarrow\{d\in S\mid d\notin\mathcal{E}\land d\neq s\land\text{CanFit}(d,s,P)\}</annotation></semantics></math><span class="ltx_text" style="font-size:90%;">
</span>
</div>
<div class="ltx_listingline" id="alg1.l10">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">10:</span></span><span class="ltx_text" style="font-size:90%;">   </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">if</span><span class="ltx_text" style="font-size:90%;"> </span><math alttext="D_{s}\neq\emptyset" class="ltx_Math" display="inline" id="alg1.l10.m1" intent=":literal"><semantics><mrow><msub><mi mathsize="0.900em">D</mi><mi mathsize="0.900em">s</mi></msub><mo mathsize="0.900em">≠</mo><mi mathsize="0.900em" mathvariant="normal">∅</mi></mrow><annotation encoding="application/x-tex">D_{s}\neq\emptyset</annotation></semantics></math><span class="ltx_text" style="font-size:90%;"> </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">then</span><span class="ltx_text" style="font-size:90%;">
</span>
</div>
<div class="ltx_listingline" id="alg1.l11">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">11:</span></span><span class="ltx_text" style="font-size:90%;">    </span><math alttext="d^{*}\leftarrow\text{argmax}_{d\in D_{s}}\left(d.C_{\text{used}}+\sum_{(s^{\prime},d^{\prime})\in P,d^{\prime}=d}s^{\prime}.C_{\text{used}}\right)" class="ltx_math_unparsed" display="inline" id="alg1.l11.m1" intent=":literal"><semantics><mrow><msup><mi mathsize="0.900em">d</mi><mo mathsize="0.900em">∗</mo></msup><mo mathsize="0.900em" stretchy="false">←</mo><msub><mtext mathsize="0.900em">argmax</mtext><mrow><mi mathsize="0.900em">d</mi><mo mathsize="0.900em">∈</mo><msub><mi mathsize="0.900em">D</mi><mi mathsize="0.900em">s</mi></msub></mrow></msub><mrow><mo>(</mo><mi mathsize="0.900em">d</mi><mo lspace="0em" mathsize="0.900em" rspace="0.167em">.</mo><msub><mi mathsize="0.900em">C</mi><mtext mathsize="0.900em">used</mtext></msub><mo mathsize="0.900em" rspace="0.055em">+</mo><msub><mo maxsize="0.900em" minsize="0.900em" stretchy="true">∑</mo><mrow><mrow><mrow><mo maxsize="0.900em" minsize="0.900em">(</mo><msup><mi mathsize="0.900em">s</mi><mo mathsize="0.900em">′</mo></msup><mo mathsize="0.900em">,</mo><msup><mi mathsize="0.900em">d</mi><mo mathsize="0.900em">′</mo></msup><mo maxsize="0.900em" minsize="0.900em">)</mo></mrow><mo mathsize="0.900em">∈</mo><mi mathsize="0.900em">P</mi></mrow><mo mathsize="0.900em">,</mo><mrow><msup><mi mathsize="0.900em">d</mi><mo mathsize="0.900em">′</mo></msup><mo mathsize="0.900em">=</mo><mi mathsize="0.900em">d</mi></mrow></mrow></msub><msup><mi mathsize="0.900em">s</mi><mo mathsize="0.900em">′</mo></msup><mo lspace="0em" mathsize="0.900em" rspace="0.167em">.</mo><msub><mi mathsize="0.900em">C</mi><mtext mathsize="0.900em">used</mtext></msub><mo>)</mo></mrow></mrow><annotation encoding="application/x-tex">d^{*}\leftarrow\text{argmax}_{d\in D_{s}}\left(d.C_{\text{used}}+\sum_{(s^{\prime},d^{\prime})\in P,d^{\prime}=d}s^{\prime}.C_{\text{used}}\right)</annotation></semantics></math><span class="ltx_text" style="font-size:90%;">
</span>
</div>
<div class="ltx_listingline" id="alg1.l12">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">12:</span></span><span class="ltx_text" style="font-size:90%;">    </span><math alttext="P\leftarrow P\cup\{(s,d^{*})\}" class="ltx_Math" display="inline" id="alg1.l12.m1" intent=":literal"><semantics><mrow><mi mathsize="0.900em">P</mi><mo mathsize="0.900em" stretchy="false">←</mo><mrow><mi mathsize="0.900em">P</mi><mo mathsize="0.900em">∪</mo><mrow><mo maxsize="0.900em" minsize="0.900em">{</mo><mrow><mo maxsize="0.900em" minsize="0.900em">(</mo><mi mathsize="0.900em">s</mi><mo mathsize="0.900em">,</mo><msup><mi mathsize="0.900em">d</mi><mo mathsize="0.900em">∗</mo></msup><mo maxsize="0.900em" minsize="0.900em">)</mo></mrow><mo maxsize="0.900em" minsize="0.900em">}</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">P\leftarrow P\cup\{(s,d^{*})\}</annotation></semantics></math><span class="ltx_text" style="font-size:90%;">; </span><math alttext="\mathcal{E}\leftarrow\mathcal{E}\cup\{s\}" class="ltx_Math" display="inline" id="alg1.l12.m2" intent=":literal"><semantics><mrow><mi class="ltx_font_mathcaligraphic" mathsize="0.900em">ℰ</mi><mo mathsize="0.900em" stretchy="false">←</mo><mrow><mi class="ltx_font_mathcaligraphic" mathsize="0.900em">ℰ</mi><mo mathsize="0.900em">∪</mo><mrow><mo maxsize="0.900em" minsize="0.900em">{</mo><mi mathsize="0.900em">s</mi><mo maxsize="0.900em" minsize="0.900em">}</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\mathcal{E}\leftarrow\mathcal{E}\cup\{s\}</annotation></semantics></math><span class="ltx_text" style="font-size:90%;">
</span>
</div>
<div class="ltx_listingline" id="alg1.l13">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">13:</span></span><span class="ltx_text" style="font-size:90%;">  </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">return</span><span class="ltx_text" style="font-size:90%;"> </span><math alttext="P" class="ltx_Math" display="inline" id="alg1.l13.m1" intent=":literal"><semantics><mi mathsize="0.900em">P</mi><annotation encoding="application/x-tex">P</annotation></semantics></math><span class="ltx_text" style="font-size:90%;">
</span>
</div>
<div class="ltx_listingline" id="alg1.l14">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">14:</span></span><span class="ltx_text" style="font-size:90%;">  </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Procedure</span><span class="ltx_text" style="font-size:90%;"> CanFit(</span><math alttext="d,s,P" class="ltx_Math" display="inline" id="alg1.l14.m1" intent=":literal"><semantics><mrow><mi mathsize="0.900em">d</mi><mo mathsize="0.900em">,</mo><mi mathsize="0.900em">s</mi><mo mathsize="0.900em">,</mo><mi mathsize="0.900em">P</mi></mrow><annotation encoding="application/x-tex">d,s,P</annotation></semantics></math><span class="ltx_text" style="font-size:90%;">): 
</span>
</div>
<div class="ltx_listingline" id="alg1.l15">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">15:</span></span><span class="ltx_text" style="font-size:90%;">  </span><math alttext="\quad C_{\text{load}}\leftarrow d.C_{\text{used}}+\sum_{(s^{\prime},d^{\prime})\in P,d^{\prime}=d}s^{\prime}.C_{\text{used}}" class="ltx_Math" display="inline" id="alg1.l15.m1" intent=":literal"><semantics><mrow><mrow><msub><mi mathsize="0.900em">C</mi><mtext mathsize="0.900em">load</mtext></msub><mo mathsize="0.900em" stretchy="false">←</mo><mi mathsize="0.900em">d</mi></mrow><mo lspace="0em" mathsize="0.900em" rspace="0.167em">.</mo><mrow><msub><mi mathsize="0.900em">C</mi><mtext mathsize="0.900em">used</mtext></msub><mo mathsize="0.900em" rspace="0.055em">+</mo><mrow><msub><mo maxsize="0.900em" minsize="0.900em" stretchy="true">∑</mo><mrow><mrow><mrow><mo maxsize="0.900em" minsize="0.900em">(</mo><msup><mi mathsize="0.900em">s</mi><mo mathsize="0.900em">′</mo></msup><mo mathsize="0.900em">,</mo><msup><mi mathsize="0.900em">d</mi><mo mathsize="0.900em">′</mo></msup><mo maxsize="0.900em" minsize="0.900em">)</mo></mrow><mo mathsize="0.900em">∈</mo><mi mathsize="0.900em">P</mi></mrow><mo mathsize="0.900em">,</mo><mrow><msup><mi mathsize="0.900em">d</mi><mo mathsize="0.900em">′</mo></msup><mo mathsize="0.900em">=</mo><mi mathsize="0.900em">d</mi></mrow></mrow></msub><msup><mi mathsize="0.900em">s</mi><mo mathsize="0.900em">′</mo></msup></mrow></mrow><mo lspace="0em" mathsize="0.900em" rspace="0.167em">.</mo><msub><mi mathsize="0.900em">C</mi><mtext mathsize="0.900em">used</mtext></msub></mrow><annotation encoding="application/x-tex">\quad C_{\text{load}}\leftarrow d.C_{\text{used}}+\sum_{(s^{\prime},d^{\prime})\in P,d^{\prime}=d}s^{\prime}.C_{\text{used}}</annotation></semantics></math><span class="ltx_text" style="font-size:90%;">
</span>
</div>
<div class="ltx_listingline" id="alg1.l16">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">16:</span></span><span class="ltx_text" style="font-size:90%;">  </span><math alttext="\quad N_{\text{load}}\leftarrow d.N_{\text{reqs}}+\sum_{(s^{\prime},d^{\prime})\in P,d^{\prime}=d}s^{\prime}.N_{\text{reqs}}" class="ltx_Math" display="inline" id="alg1.l16.m1" intent=":literal"><semantics><mrow><mrow><msub><mi mathsize="0.900em">N</mi><mtext mathsize="0.900em">load</mtext></msub><mo mathsize="0.900em" stretchy="false">←</mo><mi mathsize="0.900em">d</mi></mrow><mo lspace="0em" mathsize="0.900em" rspace="0.167em">.</mo><mrow><msub><mi mathsize="0.900em">N</mi><mtext mathsize="0.900em">reqs</mtext></msub><mo mathsize="0.900em" rspace="0.055em">+</mo><mrow><msub><mo maxsize="0.900em" minsize="0.900em" stretchy="true">∑</mo><mrow><mrow><mrow><mo maxsize="0.900em" minsize="0.900em">(</mo><msup><mi mathsize="0.900em">s</mi><mo mathsize="0.900em">′</mo></msup><mo mathsize="0.900em">,</mo><msup><mi mathsize="0.900em">d</mi><mo mathsize="0.900em">′</mo></msup><mo maxsize="0.900em" minsize="0.900em">)</mo></mrow><mo mathsize="0.900em">∈</mo><mi mathsize="0.900em">P</mi></mrow><mo mathsize="0.900em">,</mo><mrow><msup><mi mathsize="0.900em">d</mi><mo mathsize="0.900em">′</mo></msup><mo mathsize="0.900em">=</mo><mi mathsize="0.900em">d</mi></mrow></mrow></msub><msup><mi mathsize="0.900em">s</mi><mo mathsize="0.900em">′</mo></msup></mrow></mrow><mo lspace="0em" mathsize="0.900em" rspace="0.167em">.</mo><msub><mi mathsize="0.900em">N</mi><mtext mathsize="0.900em">reqs</mtext></msub></mrow><annotation encoding="application/x-tex">\quad N_{\text{load}}\leftarrow d.N_{\text{reqs}}+\sum_{(s^{\prime},d^{\prime})\in P,d^{\prime}=d}s^{\prime}.N_{\text{reqs}}</annotation></semantics></math><span class="ltx_text" style="font-size:90%;">
</span>
</div>
<div class="ltx_listingline" id="alg1.l17">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">17:</span></span><span class="ltx_text" style="font-size:90%;">  </span><math alttext="\quad\textbf{return}\ (C_{\text{load}}+s.C_{\text{used}}\leq C_{\text{max}})\land(N_{\text{load}}+s.N_{\text{reqs}}\leq B)" class="ltx_math_unparsed" display="inline" id="alg1.l17.m1" intent=":literal"><semantics><mrow><mtext class="ltx_mathvariant_bold" mathsize="0.900em">return</mtext><mrow><mo lspace="0.460em" maxsize="0.900em" minsize="0.900em">(</mo><msub><mi mathsize="0.900em">C</mi><mtext mathsize="0.900em">load</mtext></msub><mo mathsize="0.900em">+</mo><mi mathsize="0.900em">s</mi><mo lspace="0em" mathsize="0.900em" rspace="0.167em">.</mo><msub><mi mathsize="0.900em">C</mi><mtext mathsize="0.900em">used</mtext></msub><mo mathsize="0.900em">≤</mo><msub><mi mathsize="0.900em">C</mi><mtext mathsize="0.900em">max</mtext></msub><mo maxsize="0.900em" minsize="0.900em">)</mo></mrow><mo mathsize="0.900em">∧</mo><mrow><mo maxsize="0.900em" minsize="0.900em">(</mo><msub><mi mathsize="0.900em">N</mi><mtext mathsize="0.900em">load</mtext></msub><mo mathsize="0.900em">+</mo><mi mathsize="0.900em">s</mi><mo lspace="0em" mathsize="0.900em" rspace="0.167em">.</mo><msub><mi mathsize="0.900em">N</mi><mtext mathsize="0.900em">reqs</mtext></msub><mo mathsize="0.900em">≤</mo><mi mathsize="0.900em">B</mi><mo maxsize="0.900em" minsize="0.900em">)</mo></mrow></mrow><annotation encoding="application/x-tex">\quad\textbf{return}\ (C_{\text{load}}+s.C_{\text{used}}\leq C_{\text{max}})\land(N_{\text{load}}+s.N_{\text{reqs}}\leq B)</annotation></semantics></math><span class="ltx_text" style="font-size:90%;">
</span>
</div>
</div>
</figure>
<div class="ltx_para" id="S5.SS2.p6">
<p class="ltx_p">The core of the algorithm is an iterative matching process to match source rollouts to destinations (Lines <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#alg1.l6" title="In Algorithm 1 ‣ 5.2. Online Trajecgtory Repacking ‣ 5. Bubble Elimination in Long-tail Trajectory Generation ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_tag">6</span></a>-<a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#alg1.l13" title="In Algorithm 1 ‣ 5.2. Online Trajecgtory Repacking ‣ 5. Bubble Elimination in Long-tail Trajectory Generation ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_tag">13</span></a>).
For each source rollout, a set of valid destinations, <math alttext="D_{s}" class="ltx_Math" display="inline" id="S5.SS2.p6.m1" intent=":literal"><semantics><msub><mi>D</mi><mi>s</mi></msub><annotation encoding="application/x-tex">D_{s}</annotation></semantics></math>, are identified.
A valid destination rollout is a candidate rollout
which is not already slated for release and has sufficient headroom (Line <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#alg1.l9" title="In Algorithm 1 ‣ 5.2. Online Trajecgtory Repacking ‣ 5. Bubble Elimination in Long-tail Trajectory Generation ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_tag">9</span></a>), as verified by the <span class="ltx_text ltx_font_typewriter">CanFit</span> procedure. In <span class="ltx_text ltx_font_typewriter">CanFit</span>, the destination’s current KVCache usage and request counts are summed with those from the source, plus any other workloads already assigned to this destination in the current plan <math alttext="P" class="ltx_Math" display="inline" id="S5.SS2.p6.m2" intent=":literal"><semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics></math>;
the projected total KVCache usage and request count
must not exceed <math alttext="C_{\text{max}}" class="ltx_Math" display="inline" id="S5.SS2.p6.m3" intent=":literal"><semantics><msub><mi>C</mi><mtext>max</mtext></msub><annotation encoding="application/x-tex">C_{\text{max}}</annotation></semantics></math> and <math alttext="B" class="ltx_Math" display="inline" id="S5.SS2.p6.m4" intent=":literal"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math>, respectively (Lines <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#alg1.l14" title="In Algorithm 1 ‣ 5.2. Online Trajecgtory Repacking ‣ 5. Bubble Elimination in Long-tail Trajectory Generation ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_tag">14</span></a>-<a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#alg1.l17" title="In Algorithm 1 ‣ 5.2. Online Trajecgtory Repacking ‣ 5. Bubble Elimination in Long-tail Trajectory Generation ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_tag">17</span></a>).
From this set of valid destinations, we select the one that will become most densely packed in terms of KVCache utilization after the trajectory transfers (Line <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#alg1.l11" title="In Algorithm 1 ‣ 5.2. Online Trajecgtory Repacking ‣ 5. Bubble Elimination in Long-tail Trajectory Generation ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_tag">11</span></a>).
This approach preserves capacity in other destinations for larger workloads.
If a valid destination is found, the trajectory move is recorded, and the source is marked for release.</p>
</div>
<div class="ltx_para" id="S5.SS2.p7">
<p class="ltx_p">The algorithm runs in <math alttext="O(|S|^{2})" class="ltx_Math" display="inline" id="S5.SS2.p7.m1" intent=":literal"><semantics><mrow><mi>O</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mrow><mo stretchy="false">|</mo><mi>S</mi><mo stretchy="false">|</mo></mrow><mn>2</mn></msup><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(|S|^{2})</annotation></semantics></math> complexity, where <math alttext="|S|" class="ltx_Math" display="inline" id="S5.SS2.p7.m2" intent=":literal"><semantics><mrow><mo stretchy="false">|</mo><mi>S</mi><mo stretchy="false">|</mo></mrow><annotation encoding="application/x-tex">|S|</annotation></semantics></math> is the number of candidate rollouts, efficient in practice due to the typically small size of <math alttext="|S|" class="ltx_Math" display="inline" id="S5.SS2.p7.m3" intent=":literal"><semantics><mrow><mo stretchy="false">|</mo><mi>S</mi><mo stretchy="false">|</mo></mrow><annotation encoding="application/x-tex">|S|</annotation></semantics></math>.
By creating larger batches on destination rollouts and maximizing the release of source rollouts, our algorithm simultaneously increases generation throughput and promotes the generation of on-policy data.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Trajectory-level Asynchrony Analysis</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p">Our asynchronous weight synchronization and repacking mechanism enable true trajectory-level asynchrony.
Trajectories are generated independently, each proceeding at its own pace with minimal bubbles in each rollout.
This asynchrony gives rise to a key property of each trajectory’s generation: <span class="ltx_text ltx_font_italic">inherent staleness</span>.
If a trajectory is generated using model version <math alttext="K" class="ltx_Math" display="inline" id="S6.p1.m1" intent=":literal"><semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation></semantics></math>
whose generation is finished when the actor model version becomes <math alttext="M" class="ltx_Math" display="inline" id="S6.p1.m2" intent=":literal"><semantics><mi>M</mi><annotation encoding="application/x-tex">M</annotation></semantics></math>, we define its inherent staleness as <math alttext="M-K" class="ltx_Math" display="inline" id="S6.p1.m3" intent=":literal"><semantics><mrow><mi>M</mi><mo>−</mo><mi>K</mi></mrow><annotation encoding="application/x-tex">M-K</annotation></semantics></math>.
This staleness is determined purely by the trajectory’s generation latency and the trainer’s model update speed.</p>
</div>
<figure class="ltx_figure" id="S6.F10"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="390" id="S6.F10.g1" src="x10.png" width="831"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 10</span>. </span><span class="ltx_text" style="font-size:90%;">Inherent staleness distribution over finish time ranges of trajectory generation during RL training of a 7B model on 64 H800 GPUs using Laminar.
</span></figcaption>
</figure>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p">Under our fully decoupled/asynchronous design, Laminar does not require an explicitly configured staleness bound. Instead, the staleness of trajectories is naturally decided according to system dynamics (resource configuration and generation/training progress), eliminating any manual tuning of staleness bound. Especially, minimal inherent staleness is pursued with our rollout update design, as we eliminate idle time and update model version on each rollout as soon as its batch generation is completed or released. Figure <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#S6.F10" title="Figure 10 ‣ 6. Trajectory-level Asynchrony Analysis ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_tag">10</span></a> shows that the inherent staleness of trajectories in Laminar remains consistently low (typically under 3).
Laminar fluidly caters to the shifting minimal staleness rendered by dynamic RL workloads, accommodating evolving trajectory lengths and variable generation latencies.</p>
</div>
<div class="ltx_para" id="S6.p3">
<p class="ltx_p">Upon completion, trajectories are moved to
the experience buffer for trainer sampling.
Various sampling strategies can be adopted by the trainer (e.g., priority-based sampling <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Wang et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib11" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>; <span class="ltx_text" style="font-size:90%;">Schaul et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib75" title=""><span class="ltx_text" style="font-size:90%;">2016</span></a>)</cite> on metrics like temporal-difference errors, importance ratio).
Experience sampling is a classic topic
in RL; developing effective sampling strategies utilizing massive experiences efficiently generated by Laminar is a
promising direction for future exploration, and is orthogonal to the current paper.</p>
</div>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7. </span>Implementation</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p">Laminar is implemented in ~11k lines of Python code (LoC).</p>
</div>
<div class="ltx_para ltx_noindent" id="S7.p2">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Decopuled RL training framework.</span>
The trainer module and rollout module are implemented with 3.8k LoC on top of verl <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Sheng et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib13" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>)</cite>, placed on different sets of GPUs.
The partial response pool stores the tokens and statistics of each ongoing trajectory from each rollout for fast recovery and online analysis.
Inter-module data transmission
is implemented with Ray <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Moritz et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib76" title=""><span class="ltx_text" style="font-size:90%;">2018</span></a>)</cite> through Remote Process Calls.</p>
</div>
<div class="ltx_para ltx_noindent" id="S7.p3">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Relay workers</span> are implemented in 1k LoC.
Each relay worker runs as a separate process,
colocated with rollouts on the same GPU machine.
For efficient intra-machine weight transfer, a relay worker uses pinned CPU shared memory.
This allows rollout workers to directly load model shards onto their GPUs via fast PCIe links.
We built a resilient communication layer
for chain-based pipelined broadcast
using Unified Communication X (UCX) <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">ucx</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib77" title=""><span class="ltx_text" style="font-size:90%;">2019</span></a>)</cite>.
To ensure robust operation, the layer also integrates fault tolerance through heartbeat monitoring and dynamic chain rebuilding.</p>
</div>
<div class="ltx_para ltx_noindent" id="S7.p4">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Repack mechanism</span> is supported by the rollout manager, which is implemented with 1.6k LoC.</p>
</div>
</section>
<section class="ltx_section" id="S8">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8. </span>Evaluation</h2>
<div class="ltx_para" id="S8.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Testbed.</span> We deploy Laminar on a cluster of 128 machines (1024 GPUs in total). Each machine is equipped with 8 NVIDIA H800-80GB GPUs inter-connected with 400GB/s NVLink. The inter-machine bandwidth is 8 <math alttext="\times" class="ltx_Math" display="inline" id="S8.p1.m1" intent=":literal"><semantics><mo>×</mo><annotation encoding="application/x-tex">\times</annotation></semantics></math> 400Gbps. Our experiments use the following software versions: CUDA 12.6, PyTorch 2.7.1, NCCL 2.26.2, and vLLM 0.9.0.</p>
</div>
<div class="ltx_para ltx_noindent" id="S8.p2">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Models.</span>
We choose Qwen2.5 models of sizes 7B, 32B, and 72B <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Team et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib78" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>, which are popular models for RL post-training research in academia and industry.</p>
</div>
<div class="ltx_para ltx_noindent" id="S8.p3">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Baselines.</span>
We compare Laminar with four categories of RL post-training frameworks: <span class="ltx_text ltx_font_italic">synchronous</span>, <span class="ltx_text ltx_font_italic">one-step staleness</span>, <span class="ltx_text ltx_font_italic">stream generation</span>,
and stream generation with <span class="ltx_text ltx_font_italic">partial rollout</span>:
<span class="ltx_text ltx_font_bold">(1)</span> For the synchronous baseline, we use verl <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Sheng et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib13" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>)</cite> v0.5.0, a state-of-the-art RL training framework, configured with its optimal colocate placement.
<span class="ltx_text ltx_font_bold">(2)</span> We implemented the asynchronous <span class="ltx_text ltx_font_italic">one-step staleness</span> and <span class="ltx_text ltx_font_italic">stream generation</span> pipelines ourselves on top of verl for fair comparison, alleviating implementation bias.
This is necessary as many prominent and concurrent asynchronous RL training systems are not open-sourced <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Zhong et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib18" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>; <span class="ltx_text" style="font-size:90%;">Wu et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib20" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>)</cite>, only runnable on Ascend NPUs <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Han et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib23" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>)</cite>, or with limited system optimization <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Noukhovitch et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib24" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>; <span class="ltx_text" style="font-size:90%;">Tan et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib21" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>)</cite> in training, generation, and weight syncing stages.
<span class="ltx_text ltx_font_bold">(3)</span> For partial rollout, we use AReaL <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Fu et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib19" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>)</cite> v0.3.0, a concurrent research that implements partial rollout with stream generation (truncating ongoing trajectory generation of rollouts and adopting updated weights to continue generation of these trajectories) and uses an algorithm
to mitigate the impact of different policy versions within each trajectory.</p>
</div>
<div class="ltx_para" id="S8.p4">
<p class="ltx_p">To ensure rigorous comparison, all baselines except AReaL
use vLLM <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Kwon et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib51" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite> for generation and PyTorch FSDP <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Zhao et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib79" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite> for training. AReaL utilizes its modified SGLang <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Zheng et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib53" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite> for generation
and only supports Megatron-LM <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Narayanan et al</span><span class="ltx_text" style="font-size:90%;">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib80" title=""><span class="ltx_text" style="font-size:90%;">2021</span></a>)</cite> for training.</p>
</div>
<figure class="ltx_figure" id="S8.fig5">
<figure class="ltx_figure" id="S8.F13.sf1">
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">(a) </span>Training throughput on single-turn task (mathematical reasoning).</figcaption>
</figure>
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" id="S8.fig1" style="width:113.9pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="409" id="S8.g1" src="x11.png" width="830"/>
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">Figure 13. </span>Training throughput on multi-turn task (tool calling). (7B, 1.21<math alttext="\times" class="ltx_Math" display="inline" id="S8.m4" intent=":literal"><semantics><mo>×</mo><annotation encoding="application/x-tex">\times</annotation></semantics></math><math alttext="\sim" class="ltx_Math" display="inline" id="S8.m5" intent=":literal"><semantics><mo>∼</mo><annotation encoding="application/x-tex">\sim</annotation></semantics></math>5.42<math alttext="\times" class="ltx_Math" display="inline" id="S8.m6" intent=":literal"><semantics><mo>×</mo><annotation encoding="application/x-tex">\times</annotation></semantics></math>)</figcaption>
<div class="ltx_sectional-block ltx_figure_panel ltx_minipage">
<figure class="ltx_figure" id="S8.F13.sf2.fig1">
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">(b) </span>Reward with respect to wall clock time.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S8.p5">
<p class="ltx_p"><span class="ltx_text ltx_font_bold" style="font-size:90%;">Datasets.</span><span class="ltx_text" style="font-size:90%;">
We perform RL post-training on ”DAPO-Math-17k” dataset </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" style="font-size:90%;">(</span><span class="ltx_text" style="font-size:90%;">Yu et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib3" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a><span class="ltx_text" style="font-size:90%;">)</span></cite><span class="ltx_text" style="font-size:90%;">, which is widely used for math solving and multi-turn tool-calling tasks </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" style="font-size:90%;">(</span><span class="ltx_text" style="font-size:90%;">Feng et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib39" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>; <span class="ltx_text" style="font-size:90%;">Qin et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib81" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a><span class="ltx_text" style="font-size:90%;">)</span></cite><span class="ltx_text" style="font-size:90%;">.
Maximum input and output lengths are set to 2K and 16K, respectively.
Distributions of model output length
are given in
Appendix </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#A1.SS1" style="font-size:90%;" title="A.1. Response Length Distribution of Each Model ‣ Appendix A Experiment Details ‣ 10. Conclusion ‣ 9. Related Work ‣ 8.5. Fault Tolerance Analysis ‣ 8.4. Repack Efficiency ‣ 8.3. Weight Synchronization OverheadIn 8.2. Training Convergence ‣ 8. Evaluation ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_tag">A.1</span></a><span class="ltx_text" style="font-size:90%;">.</span></p>
</div>
<div class="ltx_para ltx_noindent" id="S8.p6">
<p class="ltx_p"><span class="ltx_text ltx_font_bold" style="font-size:90%;">Settings.</span><span class="ltx_text" style="font-size:90%;">
We evaluate Laminar on both math reasoning and multi-turn tool-calling tasks.
For both tasks, we utilize an open-source implementation of GRPO algorithm </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" style="font-size:90%;">(</span><span class="ltx_text" style="font-size:90%;">Shao et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib38" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a><span class="ltx_text" style="font-size:90%;">)</span></cite><span class="ltx_text" style="font-size:90%;"> with a higher clipping range in the RL loss (i.e., Clip-Higher) </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" style="font-size:90%;">(</span><span class="ltx_text" style="font-size:90%;">Yu et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib3" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a><span class="ltx_text" style="font-size:90%;">)</span></cite><span class="ltx_text" style="font-size:90%;">, which is widely adopted for RL post-training.
The effectiveness of Laminar does not rely on any specific RL algorithm and can generalize to others such as PPO </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" style="font-size:90%;">(</span><span class="ltx_text" style="font-size:90%;">Schulman et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib41" title=""><span class="ltx_text" style="font-size:90%;">2017</span></a><span class="ltx_text" style="font-size:90%;">)</span></cite><span class="ltx_text" style="font-size:90%;">.
We utilize a rule-based reward function to
score trajectories
on both tasks following </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" style="font-size:90%;">(</span><span class="ltx_text" style="font-size:90%;">Guo et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib2" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>; <span class="ltx_text" style="font-size:90%;">Yu et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib3" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>; <span class="ltx_text" style="font-size:90%;">Feng et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib39" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a><span class="ltx_text" style="font-size:90%;">)</span></cite><span class="ltx_text" style="font-size:90%;">.
For tool-calling task, the rollout interacts with a code sandbox </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" style="font-size:90%;">(</span><span class="ltx_text" style="font-size:90%;">Cheng et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib31" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a><span class="ltx_text" style="font-size:90%;">)</span></cite><span class="ltx_text" style="font-size:90%;"> to generate reasoning steps
with multi-turn code execution for solving math problems.
The global training batch size is set to 8192 with 512 prompts each having 16 responses, and the number of mini-batch update steps per training iteration is 16; the maximum number of tool calls is set to 8, following previous research </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" style="font-size:90%;">(</span><span class="ltx_text" style="font-size:90%;">Yu et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib3" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>; <span class="ltx_text" style="font-size:90%;">Feng et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib39" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a><span class="ltx_text" style="font-size:90%;">)</span></cite><span class="ltx_text" style="font-size:90%;">.</span></p>
</div>
<div class="ltx_para ltx_noindent" id="S8.p7">
<p class="ltx_p"><span class="ltx_text ltx_font_bold" style="font-size:90%;">Metrics.</span><span class="ltx_text" style="font-size:90%;">
We use throughput (tokens/sec) as the main performance metric </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" style="font-size:90%;">(</span><span class="ltx_text" style="font-size:90%;">Sheng et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib13" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a><span class="ltx_text" style="font-size:90%;">)</span></cite><span class="ltx_text" style="font-size:90%;">, computed by dividing the total number of tokens in prompts and responses in a global training batch by one RL iteration time (the duration between consecutive actor update completions).
All reported performance numbers are averaged over 5 RL iterations after a 10-iteration warm-up.</span></p>
</div>
<section class="ltx_subsection" id="S8.SS1">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">8.1. </span>End-to-End Training Throughput</h3>
<div class="ltx_para" id="S8.SS1.p1">
<p class="ltx_p"><span class="ltx_text" style="font-size:90%;">Figures </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#S8.F13.sf1" style="font-size:90%;" title="In 8. Evaluation ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_tag">13(a)</span></a><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#S8" style="font-size:90%;" title="8. Evaluation ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_tag">8</span></a><span class="ltx_text" style="font-size:90%;"> show the training throughput of various RL frameworks under different model sizes.
We tune the optimal placement under each baseline by matching the generation and training throughput to reduce GPU idleness
between the two stages.
The placement configurations are listed in Appendix </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#A1.SS2" style="font-size:90%;" title="A.2. Hyperparameter Details ‣ A.1. Response Length Distribution of Each Model ‣ Appendix A Experiment Details ‣ 10. Conclusion ‣ 9. Related Work ‣ 8.5. Fault Tolerance Analysis ‣ 8.4. Repack Efficiency ‣ 8.3. Weight Synchronization OverheadIn 8.2. Training Convergence ‣ 8. Evaluation ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_tag">A.2</span></a><span class="ltx_text" style="font-size:90%;">, denoting a key performance factor when actor and rollouts are disaggregated </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" style="font-size:90%;">(</span><span class="ltx_text" style="font-size:90%;">Zhong et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib18" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a><span class="ltx_text" style="font-size:90%;">)</span></cite><span class="ltx_text" style="font-size:90%;">.
We set the staleness bound of stream generation to 1 following </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" style="font-size:90%;">(</span><span class="ltx_text" style="font-size:90%;">Zhong et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib18" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>; <span class="ltx_text" style="font-size:90%;">Han et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib23" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a><span class="ltx_text" style="font-size:90%;">)</span></cite><span class="ltx_text" style="font-size:90%;"> and set
AReaL’s to </span><math alttext="\infty" class="ltx_Math" display="inline" id="S8.SS1.p1.m1" intent=":literal"><semantics><mi mathsize="0.900em" mathvariant="normal">∞</mi><annotation encoding="application/x-tex">\infty</annotation></semantics></math><span class="ltx_text" style="font-size:90%;"> to achieve the largest training throughput.
The maximum staleness
in Laminar, as we observed in all experiment settings, is 4. The global training batch size remains fixed when the cluster scales, a strong scaling setting which is standard practice in RL research </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" style="font-size:90%;">(</span><span class="ltx_text" style="font-size:90%;">Sheng et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib13" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>; <span class="ltx_text" style="font-size:90%;">Fu et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib19" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>; <span class="ltx_text" style="font-size:90%;">Mei et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib15" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a><span class="ltx_text" style="font-size:90%;">)</span></cite><span class="ltx_text" style="font-size:90%;">.</span></p>
</div>
<div class="ltx_para ltx_noindent" id="S8.SS1.p2">
<p class="ltx_p"><span class="ltx_text ltx_font_bold" style="font-size:90%;">Overall performance.</span><span class="ltx_text" style="font-size:90%;">
We observe that Laminar consistently outperforms the baselines across all model and cluster scales.
In Figure </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#S8.F13.sf1" style="font-size:90%;" title="In 8. Evaluation ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_tag">13(a)</span></a><span class="ltx_text" style="font-size:90%;">, Laminar achieves an average speedup of 2.56</span><math alttext="\times" class="ltx_Math" display="inline" id="S8.SS1.p2.m1" intent=":literal"><semantics><mo mathsize="0.900em">×</mo><annotation encoding="application/x-tex">\times</annotation></semantics></math><span class="ltx_text" style="font-size:90%;"> (up to 5.49</span><math alttext="\times" class="ltx_Math" display="inline" id="S8.SS1.p2.m2" intent=":literal"><semantics><mo mathsize="0.900em">×</mo><annotation encoding="application/x-tex">\times</annotation></semantics></math><span class="ltx_text" style="font-size:90%;">) over verl, 1.98</span><math alttext="\times" class="ltx_Math" display="inline" id="S8.SS1.p2.m3" intent=":literal"><semantics><mo mathsize="0.900em">×</mo><annotation encoding="application/x-tex">\times</annotation></semantics></math><span class="ltx_text" style="font-size:90%;"> (up to 4.09</span><math alttext="\times" class="ltx_Math" display="inline" id="S8.SS1.p2.m4" intent=":literal"><semantics><mo mathsize="0.900em">×</mo><annotation encoding="application/x-tex">\times</annotation></semantics></math><span class="ltx_text" style="font-size:90%;">) over one-step staleness pipeline, 1.93</span><math alttext="\times" class="ltx_Math" display="inline" id="S8.SS1.p2.m5" intent=":literal"><semantics><mo mathsize="0.900em">×</mo><annotation encoding="application/x-tex">\times</annotation></semantics></math><span class="ltx_text" style="font-size:90%;"> (up to 4.06</span><math alttext="\times" class="ltx_Math" display="inline" id="S8.SS1.p2.m6" intent=":literal"><semantics><mo mathsize="0.900em">×</mo><annotation encoding="application/x-tex">\times</annotation></semantics></math><span class="ltx_text" style="font-size:90%;">) over stream generation, and 1.39</span><math alttext="\times" class="ltx_Math" display="inline" id="S8.SS1.p2.m7" intent=":literal"><semantics><mo mathsize="0.900em">×</mo><annotation encoding="application/x-tex">\times</annotation></semantics></math><span class="ltx_text" style="font-size:90%;"> (up to 1.81</span><math alttext="\times" class="ltx_Math" display="inline" id="S8.SS1.p2.m8" intent=":literal"><semantics><mo mathsize="0.900em">×</mo><annotation encoding="application/x-tex">\times</annotation></semantics></math><span class="ltx_text" style="font-size:90%;">) over AReaL.
Similar gains are achieved on the tool calling task (Figure </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#S8" style="font-size:90%;" title="8. Evaluation ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_tag">8</span></a><span class="ltx_text" style="font-size:90%;">), where Laminar delivers an average speedup of 2.62</span><math alttext="\times" class="ltx_Math" display="inline" id="S8.SS1.p2.m9" intent=":literal"><semantics><mo mathsize="0.900em">×</mo><annotation encoding="application/x-tex">\times</annotation></semantics></math><span class="ltx_text" style="font-size:90%;"> across all baselines.
The performance advantage of Laminar increases as we scale to more GPUs, stemming from two primary factors.</span></p>
</div>
<div class="ltx_para" id="S8.SS1.p3">
<p class="ltx_p"><span class="ltx_text ltx_font_italic" style="font-size:90%;">First</span><span class="ltx_text" style="font-size:90%;">, Laminar adopts trajectory-level asynchrony, allowing each rollout to proceed at its own pace and enhancing generation throughput.
In contrast, baselines are constrained by a global synchronization barrier, which forces faster rollouts to wait for the slowest ones, creating significant pipeline bubbles.
While AReaL attempts to mitigate this with partial rollouts, it introduces overhead due to trajectory generation aborting/resumption
and KVCache recomputation, which adversely affects generation throughput. </span><span class="ltx_text ltx_font_italic" style="font-size:90%;">Second</span><span class="ltx_text" style="font-size:90%;">, by minimizing rollout bubbles, Laminar sustains a higher generation throughput. This efficiency allows allocating more
GPUs to the trainer
within a given cluster
as compared to other asynchronous baselines, while still balancing
generation and training throughput.</span></p>
</div>
<div class="ltx_para ltx_noindent" id="S8.SS1.p4">
<p class="ltx_p"><span class="ltx_text ltx_font_bold" style="font-size:90%;">Scalability.</span><span class="ltx_text" style="font-size:90%;">
Figures </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#S8.F13.sf1" style="font-size:90%;" title="In 8. Evaluation ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_tag">13(a)</span></a><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#S8" style="font-size:90%;" title="8. Evaluation ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_tag">8</span></a><span class="ltx_text" style="font-size:90%;"> show that Laminar achieves better scalability than all baselines,
a strong scaling efficiency
of 53.7% (up to 68.2% on 32B model) on math tasks and 46.5% on tool-calling tasks. The scaling efficciency is computed by dividing </span><math alttext="\frac{\mbox{throughput in largest scale}}{\mbox{throughput in smallest scale}}" class="ltx_Math" display="inline" id="S8.SS1.p4.m1" intent=":literal"><semantics><mfrac><mtext mathsize="0.900em">throughput in largest scale</mtext><mtext mathsize="0.900em">throughput in smallest scale</mtext></mfrac><annotation encoding="application/x-tex">\frac{\mbox{throughput in largest scale}}{\mbox{throughput in smallest scale}}</annotation></semantics></math><span class="ltx_text" style="font-size:90%;"> by </span><math alttext="\frac{\mbox{max. \# of GPUs}}{\mbox{min. \# of GPUs}}" class="ltx_Math" display="inline" id="S8.SS1.p4.m2" intent=":literal"><semantics><mfrac><mtext mathsize="0.900em">max. # of GPUs</mtext><mtext mathsize="0.900em">min. # of GPUs</mtext></mfrac><annotation encoding="application/x-tex">\frac{\mbox{max. \# of GPUs}}{\mbox{min. \# of GPUs}}</annotation></semantics></math><span class="ltx_text" style="font-size:90%;"> </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" style="font-size:90%;">(</span><span class="ltx_text" style="font-size:90%;">Amdahl</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib82" title=""><span class="ltx_text" style="font-size:90%;">1967</span></a><span class="ltx_text" style="font-size:90%;">)</span></cite><span class="ltx_text" style="font-size:90%;">.
The best baselines only reach 33.6% (up to 39.2%) (AReaL on math) and 12.9% (stream generation on tool-calling).
The enhanced scalability of Laminar stems from its efficient resource utilization across all scales.
As the cluster scales up, Laminar effectively utilizes additional GPUs to boost training throughput with little bubbles, as shown in Figure </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#S2.F3" style="font-size:90%;" title="Figure 3 ‣ 2.2. Recent Trends in RL Post-Training ‣ 2. Background and Motivation ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_tag">3</span></a><span class="ltx_text" style="font-size:90%;">(e).
By avoiding straggler effects from global synchronization, Laminar exhibits increasing speedups as training scales. This results in an average speedup of 3.34</span><math alttext="\times" class="ltx_Math" display="inline" id="S8.SS1.p4.m3" intent=":literal"><semantics><mo mathsize="0.900em">×</mo><annotation encoding="application/x-tex">\times</annotation></semantics></math><span class="ltx_text" style="font-size:90%;"> at the largest cluster scales across all baselines and model sizes.
A more detailed analysis of this scalability is in Appendix </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#A2" style="font-size:90%;" title="Appendix B Detailed Experiment Analysis ‣ A.2. Hyperparameter Details ‣ A.1. Response Length Distribution of Each Model ‣ Appendix A Experiment Details ‣ 10. Conclusion ‣ 9. Related Work ‣ 8.5. Fault Tolerance Analysis ‣ 8.4. Repack Efficiency ‣ 8.3. Weight Synchronization OverheadIn 8.2. Training Convergence ‣ 8. Evaluation ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_tag">B</span></a><span class="ltx_text" style="font-size:90%;">.</span></p>
</div>
</section>
<section class="ltx_subsection" id="S8.SS2">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">8.2. </span>Training Convergence</h3>
<div class="ltx_para" id="S8.SS2.p1">
<p class="ltx_p"><span class="ltx_text" style="font-size:90%;">Model convergence speed is crucial in RL post-training, measured by the training reward improvement over time.
We compare Laminar with baselines using their respective tuned or open-sourced hyperparameters for the GRPO algorithm, with staleness bound under 1 for the stream generation baseline.
AReaL utilizes its proposed Decoupled PPO </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" style="font-size:90%;">(</span><span class="ltx_text" style="font-size:90%;">Hilton et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib83" title=""><span class="ltx_text" style="font-size:90%;">2022</span></a><span class="ltx_text" style="font-size:90%;">)</span></cite><span class="ltx_text" style="font-size:90%;"> algorithm to mitigate errors introduced by partial rollout with a suggested optimal staleness of 4.
Laminar’s maximum staleness is 4 for a direct comparison
(detailed settings in Appendix </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#A1.SS2" style="font-size:90%;" title="A.2. Hyperparameter Details ‣ A.1. Response Length Distribution of Each Model ‣ Appendix A Experiment Details ‣ 10. Conclusion ‣ 9. Related Work ‣ 8.5. Fault Tolerance Analysis ‣ 8.4. Repack Efficiency ‣ 8.3. Weight Synchronization OverheadIn 8.2. Training Convergence ‣ 8. Evaluation ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_tag">A.2</span></a><span class="ltx_text" style="font-size:90%;">).
As shown in Figure </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#S8.F13.sf2.fig1" style="font-size:90%;" title="In 8. Evaluation ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_tag">13(b)</span></a><span class="ltx_text" style="font-size:90%;">, Laminar converges about 1.77</span><math alttext="\times" class="ltx_Math" display="inline" id="S8.SS2.p1.m1" intent=":literal"><semantics><mo mathsize="0.900em">×</mo><annotation encoding="application/x-tex">\times</annotation></semantics></math><span class="ltx_text" style="font-size:90%;"> faster for 7B and 1.59</span><math alttext="\times" class="ltx_Math" display="inline" id="S8.SS2.p1.m2" intent=":literal"><semantics><mo mathsize="0.900em">×</mo><annotation encoding="application/x-tex">\times</annotation></semantics></math><span class="ltx_text" style="font-size:90%;"> faster for 32B than the best baseline (on-policy verl) on the math reasoning task.
Laminar achieves this by significantly increasing the training throughput while minimizing staleness without introducing additional biases; however, in other asynchronous systems, the throughput improvement is outweighed by staleness or additional biases, such as mixing multiple policy versions within a single trajectory in AReaL.</span></p>
</div>
<figure class="ltx_figure" id="S8.F13.sf3">
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">(c) </span>Rollout waiting time during weight syncing. Trainer has the same number of GPUs as all rollouts’ GPUs.</figcaption>
<div class="ltx_block">
<figure class="ltx_figure" id="S8.F14"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="258" id="S8.F14.g1" src="x12.png" width="830"/>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">Figure 14. </span>Training with a rollout machine failure.</figcaption>
</figure>
<section class="ltx_subsection" id="S8.SS3">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">8.3. </span>Weight Synchronization Overhead</h3>
<div class="ltx_para" id="S8.SS3.p1">
<p class="ltx_p"><span class="ltx_text" style="font-size:90%;">We evaluate the weight synchronization overhead of our relay worker design by measuring rollout waiting time and actor stalling time.
We compare Laminar against the GPU-based global synchronization approach adopted by recent RL systems </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" style="font-size:90%;">(</span><span class="ltx_text" style="font-size:90%;">Sheng et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib13" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>; <span class="ltx_text" style="font-size:90%;">Hu et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib17" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>; <span class="ltx_text" style="font-size:90%;">Wu et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib20" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>; <span class="ltx_text" style="font-size:90%;">Zhong et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib18" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>; <span class="ltx_text" style="font-size:90%;">Han et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib23" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a><span class="ltx_text" style="font-size:90%;">)</span></cite><span class="ltx_text" style="font-size:90%;">,
that each actor model partition is broadcast to the corresponding model shards on each rollout. It follows the mapping between actor and rollout parallel groups and uses NCCL to achieve zero-copy transfer </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" style="font-size:90%;">(</span><span class="ltx_text" style="font-size:90%;">Wu et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib20" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>; <span class="ltx_text" style="font-size:90%;">Hu et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib17" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a><span class="ltx_text" style="font-size:90%;">)</span></cite><span class="ltx_text" style="font-size:90%;">.</span></p>
</div>
<div class="ltx_para" id="S8.SS3.p2">
<p class="ltx_p"><span class="ltx_text" style="font-size:90%;">Figure </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#S8.F13.sf3" style="font-size:90%;" title="In 8.2. Training Convergence ‣ 8. Evaluation ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_tag">13(c)</span></a><span class="ltx_text" style="font-size:90%;"> compares the rollout waiting time, measured
from when each rollout begins updating to the latest weights until the update is complete.
From 64 to 1024 GPUs, Laminar consistently outperforms the GPU-based global synchronization.
It reduces the average and best-case waiting times by up to 37% and 47%, respectively.
Laminar’s best-case occurs when the latest weights are already cached on the relay worker’s CPU memory, allowing for loading model shards in parallel over the fast PCIe bus.
Notably, Laminar’s average rollout waiting time remains close to its best-case, due to our RDMA-based relay broadcast (Figure </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#A1.F19" style="font-size:90%;" title="Figure 19 ‣ D.1. Communication Model ‣ Appendix D Theoretical Analysis of Chain-Based Pipelined Broadcast ‣ A.2. Hyperparameter Details ‣ A.1. Response Length Distribution of Each Model ‣ Appendix A Experiment Details ‣ 10. Conclusion ‣ 9. Related Work ‣ 8.5. Fault Tolerance Analysis ‣ 8.4. Repack Efficiency ‣ 8.3. Weight Synchronization OverheadIn 8.2. Training Convergence ‣ 8. Evaluation ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_tag">19</span></a><span class="ltx_text" style="font-size:90%;"> in Appendix </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#A4" style="font-size:90%;" title="Appendix D Theoretical Analysis of Chain-Based Pipelined Broadcast ‣ A.2. Hyperparameter Details ‣ A.1. Response Length Distribution of Each Model ‣ Appendix A Experiment Details ‣ 10. Conclusion ‣ 9. Related Work ‣ 8.5. Fault Tolerance Analysis ‣ 8.4. Repack Efficiency ‣ 8.3. Weight Synchronization OverheadIn 8.2. Training Convergence ‣ 8. Evaluation ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_tag">D</span></a><span class="ltx_text" style="font-size:90%;">) and trajectory-level asynchrony.
By eliminating global synchronization points, few rollouts request the latest weights exactly when the actor finishes its update.
Consequently, most rollouts can fetch weights locally over PCIe
without waiting for a network broadcast.</span></p>
</div>
<div class="ltx_para" id="S8.SS3.p3">
<p class="ltx_p"><span class="ltx_text" style="font-size:90%;">Laminar also minimizes actor stalling time thanks to our hierarchical relay worker design.
The actor only transfers weights to a single master relay, and its communication overhead remains constant regardless of the number of rollout replicas.
Therefore, the actor stalls only 0.64 and 1.40 seconds for 32B and 72B models, respectively.</span></p>
</div>
<section class="ltx_subsection" id="S8.SS4">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">8.4. </span>Repack Efficiency</h3>
<div class="ltx_para" id="S8.SS4.p1">
<p class="ltx_p"><span class="ltx_text" style="font-size:90%;">We validate the benefit of our repack mechanism in eliminating remaining pipeline bubbles during generation. In this experiment, we use the same placement setting as the end-to-end experiment on the 32B model with 128 GPUs
(§</span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#S8.SS1" style="font-size:90%;" title="8.1. End-to-End Training Throughput ‣ 8. Evaluation ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_tag">8.1</span></a><span class="ltx_text" style="font-size:90%;">):
64 GPUs for the trainer and 64 GPUs for rollouts, with each rollout occupying 4 GPUs (totaling 16 rollouts).</span></p>
</div>
<div class="ltx_para" id="S8.SS4.p2">
<p class="ltx_p"><span class="ltx_text" style="font-size:90%;">As shown in Figure </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#S8.F15" style="font-size:90%;" title="Figure 15 ‣ 8.4. Repack Efficiency ‣ 8.3. Weight Synchronization OverheadIn 8.2. Training Convergence ‣ 8. Evaluation ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_tag">15</span></a><span class="ltx_text" style="font-size:90%;">, enabling the repack mechanism boosts generation throughput by 26% than without.
Laminar detects underutilized rollouts by monitoring their GPU KVCache utilization. As shown in Table </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#S8.T1" style="font-size:90%;" title="Table 1 ‣ 8.4. Repack Efficiency ‣ 8.3. Weight Synchronization OverheadIn 8.2. Training Convergence ‣ 8. Evaluation ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_tag">1</span></a><span class="ltx_text" style="font-size:90%;">, the repack mechanism increases average KVCache utilization by 14.8%. This improvement is achieved with a negligible repacking overhead of 0.69s and, crucially, does not increase the latency of trajectories generation.</span></p>
</div>
<figure class="ltx_table" id="S8.T1">
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 1. </span>
Statistics of the rollouts with and without repack.
</figcaption>
<div class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:60pt;vertical-align:-27.3pt;"><span class="ltx_transformed_inner" style="transform:translate(36.7pt,-5.1pt) scale(1.20344349663195,1.20344349663195) ;">
<table class="ltx_tabular ltx_align_middle">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span class="ltx_text" style="font-size:90%;">Laminar</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">
<span class="ltx_text" style="font-size:90%;"> </span><span class="ltx_text" style="font-size:90%;">
<span class="ltx_tabular ltx_align_middle">
<span class="ltx_tr">
<span class="ltx_td ltx_nopad_r ltx_align_center">Average/max latency of</span></span>
<span class="ltx_tr">
<span class="ltx_td ltx_nopad_r ltx_align_center">generating trajectories (s)</span></span>
</span></span><span class="ltx_text" style="font-size:90%;"></span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">
<span class="ltx_text" style="font-size:90%;"> </span><span class="ltx_text" style="font-size:90%;">
<span class="ltx_tabular ltx_align_middle">
<span class="ltx_tr">
<span class="ltx_td ltx_nopad_r ltx_align_center">Repack</span></span>
<span class="ltx_tr">
<span class="ltx_td ltx_nopad_r ltx_align_center">overhead (s)</span></span>
</span></span><span class="ltx_text" style="font-size:90%;"></span>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt">
<span class="ltx_text" style="font-size:90%;"> </span><span class="ltx_text" style="font-size:90%;">
<span class="ltx_tabular ltx_align_middle">
<span class="ltx_tr">
<span class="ltx_td ltx_nopad_r ltx_align_center">Average</span></span>
<span class="ltx_tr">
<span class="ltx_td ltx_nopad_r ltx_align_center">KVCache util.</span></span>
</span></span><span class="ltx_text" style="font-size:90%;"></span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<span class="ltx_text" style="font-size:90%;"> </span><span class="ltx_text" style="font-size:90%;">
<span class="ltx_tabular ltx_align_middle">
<span class="ltx_tr">
<span class="ltx_td ltx_nopad_r ltx_align_center">w/ repack</span></span>
</span></span><span class="ltx_text" style="font-size:90%;"></span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text" style="font-size:90%;">290/828</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text" style="font-size:90%;">0.69</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">82.2%</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t">
<span class="ltx_text" style="font-size:90%;"> </span><span class="ltx_text" style="font-size:90%;">
<span class="ltx_tabular ltx_align_middle">
<span class="ltx_tr">
<span class="ltx_td ltx_nopad_r ltx_align_center">w/o repack</span></span>
</span></span><span class="ltx_text" style="font-size:90%;"></span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t"><span class="ltx_text" style="font-size:90%;">296/826</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t"><span class="ltx_text" style="font-size:90%;">/</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span class="ltx_text" style="font-size:90%;">71.6%</span></td>
</tr>
</table>
</span></div>
</figure>
<figure class="ltx_figure" id="S8.F15"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="252" id="S8.F15.g1" src="x13.png" width="830"/>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">Figure 15. </span>Repack efficiency.</figcaption>
</figure>
<section class="ltx_subsection" id="S8.SS5">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">8.5. </span>Fault Tolerance Analysis</h3>
<div class="ltx_para" id="S8.SS5.p1">
<p class="ltx_p"><span class="ltx_text" style="font-size:90%;">We evaluate the robustness of Laminar when encountering failures during RL training.
We manually kill a rollout machine containing two replicas during a training job that has the same setting as in §</span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#S8.SS4" style="font-size:90%;" title="8.4. Repack Efficiency ‣ 8.3. Weight Synchronization OverheadIn 8.2. Training Convergence ‣ 8. Evaluation ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_tag">8.4</span></a><span class="ltx_text" style="font-size:90%;">.
As shown in Figure </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#S8.F14" style="font-size:90%;" title="Figure 14In 8.2. Training Convergence ‣ 8. Evaluation ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_tag">14</span></a><span class="ltx_text" style="font-size:90%;">, this event causes an immediate drop in generation throughput due to the loss of rollouts.
Training throughput remains unaffected or experiences a slight drop while waiting for adequate trajectories from remaining healthy rollouts.
The system recovers in approximately 252 seconds, including allocating a new machine and initializing the rollouts on it.
After recovery, both generation throughput and training throughput recover to original levels.
This demonstrates that Laminar handles machine failures without disrupting training progress.</span></p>
</div>
<section class="ltx_section" id="S9">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">9. </span>Related Work</h2>
<div class="ltx_para ltx_noindent" id="S9.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold" style="font-size:90%;">RL frameworks for LLMs.</span><span class="ltx_text" style="font-size:90%;">
Early reinforcement learning (RL) frameworks for LLMs primarily focused on synchronous algorithms </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" style="font-size:90%;">(</span><span class="ltx_text" style="font-size:90%;">Corporation</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib84" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>; <span class="ltx_text" style="font-size:90%;">von Werra et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib85" title=""><span class="ltx_text" style="font-size:90%;">2020</span></a>; <span class="ltx_text" style="font-size:90%;">Yao et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib12" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>; <span class="ltx_text" style="font-size:90%;">Shen et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib46" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>; <span class="ltx_text" style="font-size:90%;">Xiao et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib47" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>; <span class="ltx_text" style="font-size:90%;">Hu et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib17" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>; <span class="ltx_text" style="font-size:90%;">Sheng et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib13" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>; <span class="ltx_text" style="font-size:90%;">Mei et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib15" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>; <span class="ltx_text" style="font-size:90%;">Team et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib9" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>; <span class="ltx_text" style="font-size:90%;">nem</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib86" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>; <span class="ltx_text" style="font-size:90%;">Zhong et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib16" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>; <span class="ltx_text" style="font-size:90%;">Griggs et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib87" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>; <span class="ltx_text" style="font-size:90%;">Zhu et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib88" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>; <span class="ltx_text" style="font-size:90%;">Guo et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib2" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a><span class="ltx_text" style="font-size:90%;">)</span></cite><span class="ltx_text" style="font-size:90%;">. More recent work has begun to explore asynchronous RL </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" style="font-size:90%;">(</span><span class="ltx_text" style="font-size:90%;">Wang et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib11" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>; <span class="ltx_text" style="font-size:90%;">Tan et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib21" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>; <span class="ltx_text" style="font-size:90%;">Zhong et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib18" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>; <span class="ltx_text" style="font-size:90%;">Team et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib22" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>; <span class="ltx_text" style="font-size:90%;">Wu et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib20" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>; <span class="ltx_text" style="font-size:90%;">Han et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib23" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a><span class="ltx_text" style="font-size:90%;">)</span></cite><span class="ltx_text" style="font-size:90%;">.
However, these emerging asynchronous systems still depend on a global weight synchronization point. This design forces faster rollouts to idle while waiting for stragglers, creating significant long-tail performance bubbles.
Some systems attempt to mitigate this issue with partial rollouts </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" style="font-size:90%;">(</span><span class="ltx_text" style="font-size:90%;">Team et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib9" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>; <span class="ltx_text" style="font-size:90%;">Seed et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib8" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>; <span class="ltx_text" style="font-size:90%;">Wu et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib20" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>; <span class="ltx_text" style="font-size:90%;">Fu et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib19" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a><span class="ltx_text" style="font-size:90%;">)</span></cite><span class="ltx_text" style="font-size:90%;">.
But this technique introduces severe penalties: the high overhead of recomputing KVCache after each update and the undesirable mixing of multiple policy versions within a single trajectory. These issues are notably exacerbated at scale. In contrast, Laminar’s fully decoupled architecture and trajectory-level asynchrony eliminates these long-tail bubbles while guaranteeing that each trajectory is generated with a single, consistent policy version.</span></p>
</div>
<div class="ltx_para ltx_noindent" id="S9.p2">
<p class="ltx_p"><span class="ltx_text ltx_font_bold" style="font-size:90%;">Asynchronous deep RL systems.</span><span class="ltx_text" style="font-size:90%;"> Extensive research exists on asynchronous deep RL systems </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" style="font-size:90%;">(</span><span class="ltx_text" style="font-size:90%;">Hesse et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib89" title=""><span class="ltx_text" style="font-size:90%;">2017</span></a>; <span class="ltx_text" style="font-size:90%;">Hafner et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib90" title=""><span class="ltx_text" style="font-size:90%;">2017</span></a>; <span class="ltx_text" style="font-size:90%;">Bou et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib91" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>; <span class="ltx_text" style="font-size:90%;">Espeholt et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib92" title=""><span class="ltx_text" style="font-size:90%;">2018</span></a>; <span class="ltx_text" style="font-size:90%;">Liang et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib93" title=""><span class="ltx_text" style="font-size:90%;">2018</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib94" title=""><span class="ltx_text" style="font-size:90%;">2021</span></a>; <span class="ltx_text" style="font-size:90%;">Weng et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib95" title=""><span class="ltx_text" style="font-size:90%;">2022</span></a>; <span class="ltx_text" style="font-size:90%;">Vinyals et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib45" title=""><span class="ltx_text" style="font-size:90%;">2019</span></a>; <span class="ltx_text" style="font-size:90%;">Berner et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib25" title=""><span class="ltx_text" style="font-size:90%;">2019</span></a><span class="ltx_text" style="font-size:90%;">)</span></cite><span class="ltx_text" style="font-size:90%;">. However, they target small-scale DNNs, not designed for the efficient inference, generation, and training patterns of modern LLMs.
Some systems employ fully asynchronous parameter updates where gradients are computed from stale parameters </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" style="font-size:90%;">(</span><span class="ltx_text" style="font-size:90%;">Recht et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib96" title=""><span class="ltx_text" style="font-size:90%;">2011</span></a>; <span class="ltx_text" style="font-size:90%;">Mnih et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib97" title=""><span class="ltx_text" style="font-size:90%;">2015</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib98" title=""><span class="ltx_text" style="font-size:90%;">2016</span></a><span class="ltx_text" style="font-size:90%;">)</span></cite><span class="ltx_text" style="font-size:90%;">.
This approach can introduce training instability and is thus rarely used for LLMs.
The success of state-of-the-art deep RL systems like AlphaStar </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" style="font-size:90%;">(</span><span class="ltx_text" style="font-size:90%;">Vinyals et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib45" title=""><span class="ltx_text" style="font-size:90%;">2019</span></a><span class="ltx_text" style="font-size:90%;">)</span></cite><span class="ltx_text" style="font-size:90%;"> and OpenAI Five </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" style="font-size:90%;">(</span><span class="ltx_text" style="font-size:90%;">OpenAI et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib67" title=""><span class="ltx_text" style="font-size:90%;">2019</span></a><span class="ltx_text" style="font-size:90%;">)</span></cite><span class="ltx_text" style="font-size:90%;"> has been attributed to massive scale-out, with OpenAI Five, for instance, leveraging 57,200 rollout workers on 51,200 CPUs for 10 months </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" style="font-size:90%;">(</span><span class="ltx_text" style="font-size:90%;">Berner et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib25" title=""><span class="ltx_text" style="font-size:90%;">2019</span></a><span class="ltx_text" style="font-size:90%;">)</span></cite><span class="ltx_text" style="font-size:90%;">. This emphasis on scale has also spurred optimizations in related components, such as distributed experience buffers </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" style="font-size:90%;">(</span><span class="ltx_text" style="font-size:90%;">Horgan et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib99" title=""><span class="ltx_text" style="font-size:90%;">2018</span></a>; <span class="ltx_text" style="font-size:90%;">Cassirer et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib100" title=""><span class="ltx_text" style="font-size:90%;">2021</span></a>; <span class="ltx_text" style="font-size:90%;">Wang et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib101" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a><span class="ltx_text" style="font-size:90%;">)</span></cite><span class="ltx_text" style="font-size:90%;">.</span></p>
</div>
<div class="ltx_para ltx_noindent" id="S9.p3">
<p class="ltx_p"><span class="ltx_text ltx_font_bold" style="font-size:90%;">Fault-tolerant training.</span><span class="ltx_text" style="font-size:90%;">
Previous studies </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" style="font-size:90%;">(</span><span class="ltx_text" style="font-size:90%;">Zu et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib102" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>; <span class="ltx_text" style="font-size:90%;">Hu et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib103" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>; <span class="ltx_text" style="font-size:90%;">Jiang et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib104" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>; <span class="ltx_text" style="font-size:90%;">Dong et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib105" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>; <span class="ltx_text" style="font-size:90%;">Deng et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib106" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>; <span class="ltx_text" style="font-size:90%;">Wan et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib107" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a><span class="ltx_text" style="font-size:90%;">)</span></cite><span class="ltx_text" style="font-size:90%;"> have introduced robust AI infrastructures to automatically detect and localize various training failures through real-time monitoring and stop-time diagnosis.
The sophisticated techniques they rely on, such as RDMA-level metric collection and </span><span class="ltx_text ltx_font_typewriter" style="font-size:90%;">dmesg</span><span class="ltx_text" style="font-size:90%;"> inspection, can be paired with our heartbeat-based failover mechanism to improve observability of rollout replicas and trainer workers in the RL system further.
Many diagnostic tools have been developed to pinpoint network faults, both inter-host </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" style="font-size:90%;">(</span><span class="ltx_text" style="font-size:90%;">Zhu et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib108" title=""><span class="ltx_text" style="font-size:90%;">2015</span></a>; <span class="ltx_text" style="font-size:90%;">Li et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib109" title=""><span class="ltx_text" style="font-size:90%;">2016</span></a>; <span class="ltx_text" style="font-size:90%;">Tan et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib110" title=""><span class="ltx_text" style="font-size:90%;">2019</span></a><span class="ltx_text" style="font-size:90%;">)</span></cite><span class="ltx_text" style="font-size:90%;"> and intra-host </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" style="font-size:90%;">(</span><span class="ltx_text" style="font-size:90%;">Liu et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib111" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a><span class="ltx_text" style="font-size:90%;">)</span></cite><span class="ltx_text" style="font-size:90%;">.
Rather than delving deep into the network stack to expose the exact root cause, our broadcast chain rebuilding mechanism works around communication failures by routing around faulty relay workers during weight synchronization.
Checkpointing optimizations </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" style="font-size:90%;">(</span><span class="ltx_text" style="font-size:90%;">Mohan et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib112" title=""><span class="ltx_text" style="font-size:90%;">2021</span></a>; <span class="ltx_text" style="font-size:90%;">Eisenman et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib113" title=""><span class="ltx_text" style="font-size:90%;">2022</span></a>; <span class="ltx_text" style="font-size:90%;">Wang et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib114" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>; <span class="ltx_text" style="font-size:90%;">Wan et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib61" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a><span class="ltx_text" style="font-size:90%;">)</span></cite><span class="ltx_text" style="font-size:90%;"> further mitigate stalls and support flexible transfer </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" style="font-size:90%;">(</span><span class="ltx_text" style="font-size:90%;">Wan et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib61" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>; <span class="ltx_text" style="font-size:90%;">Wagenländer et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib60" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a><span class="ltx_text" style="font-size:90%;">)</span></cite><span class="ltx_text" style="font-size:90%;">.
By incorporating these methods, our trainer can recover from failures more quickly.</span></p>
</div>
<section class="ltx_section" id="S10">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">10. </span>Conclusion</h2>
<div class="ltx_para" id="S10.p1">
<p class="ltx_p"><span class="ltx_text" style="font-size:90%;">Laminar is an RL framework that addresses the limited scalability and long-tail trajectory generation problem in LLM post-training.
We enable trajectory-level asynchrony, with each trajectory generated and consumed independently at its own optimal pace, eliminating rigid global synchronization.
This design is achieved through a fully decoupled architecture with relay workers enabling asynchronous weight synchronization, allowing rollouts to pull new model parameters anytime without stalling computation.
This architecture accommodates evolving trajectory lengths in RL training, while isolating component failures to ensure robust fault tolerance for long-running jobs.
Our dynamic repack mechanism further consolidates long-tail trajectories to maximize generation throughput while minimizing staleness.
We conducted extensive evaluation at scales up to 1024 GPUs. Our results demonstrate that Laminar achieves up to 5.48</span><math alttext="\times" class="ltx_Math" display="inline" id="S10.p1.m1" intent=":literal"><semantics><mo mathsize="0.900em">×</mo><annotation encoding="application/x-tex">\times</annotation></semantics></math><span class="ltx_text" style="font-size:90%;"> speedup over state-of-the-art systems and reduces model convergence time.</span></p>
</div>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">(1)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Guo et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2025)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> 2025.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2501.12948</em><span class="ltx_text" style="font-size:90%;"> (2025).
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Yu et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2025)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Guangming Sheng, Yuxuan Tong, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Jinhua Zhu, Jiaze Chen, Jiangjie Chen, Chengyi Wang, Hongli Yu, Yuxuan Song, Xiangpeng Wei, Hao Zhou, Jingjing Liu, Wei-Ying Ma, Ya-Qin Zhang, Lin Yan, Mu Qiao,
Yonghui Wu, and Mingxuan Wang. 2025.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">DAPO: An Open-Source LLM Reinforcement Learning System at Scale.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">arXiv:2503.14476 [cs.LG]

</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2503.14476" style="font-size:90%;" title="">https://arxiv.org/abs/2503.14476</a><span class="ltx_text" style="font-size:90%;">
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Kumar et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2025)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Komal Kumar, Tajamul Ashraf, Omkar Thawakar, Rao Muhammad Anwer, Hisham Cholakkal, Mubarak Shah, Ming-Hsuan Yang, Phillip HS Torr, Fahad Shahbaz Khan, and Salman Khan. 2025.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">Llm post-training: A deep dive into reasoning large language models.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2502.21321</em><span class="ltx_text" style="font-size:90%;"> (2025).
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">OpenAI (2025)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
OpenAI. 2025.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">Learning to reason with llms.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openai.com/index/learning-to-reason-with-llms/" style="font-size:90%;" title="">https://openai.com/index/learning-to-reason-with-llms/</a><span class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">xAI (2025)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
xAI. 2025.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">Grok 4.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://x.ai/news/grok-4" style="font-size:90%;" title="">https://x.ai/news/grok-4</a><span class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Jimenez et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2023)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Carlos E. Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. 2023.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">SWE-bench: Can Language Models Resolve Real-World GitHub Issues?
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv: 2310.06770</em><span class="ltx_text" style="font-size:90%;"> (2023).
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Seed et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2025)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
ByteDance Seed, Jiaze Chen, Tiantian Fan, Xin Liu, Lingjun Liu, Zhiqi Lin, Mingxuan Wang, Chengyi Wang, Xiangpeng Wei, Wenyuan Xu, et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> 2025.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">Seed1. 5-thinking: Advancing superb reasoning models with reinforcement learning.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2504.13914</em><span class="ltx_text" style="font-size:90%;"> (2025).
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Team et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2025)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> 2025.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">Kimi k1. 5: Scaling reinforcement learning with llms.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2501.12599</em><span class="ltx_text" style="font-size:90%;"> (2025).
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Li et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2025)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Zhong-Zhi Li, Duzhen Zhang, Ming-Liang Zhang, Jiaxin Zhang, Zengyan Liu, Yuxuan Yao, Haotian Xu, Junhao Zheng, Pei-Jie Wang, Xiuyi Chen, Yingying Zhang, Fei Yin, Jiahua Dong, Zhiwei Li, Bao-Long Bi, Ling-Rui Mei, Junfeng Fang, Xiao Liang, Zhijiang Guo, Le Song, and Cheng-Lin Liu. 2025.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">From System 1 to System 2: A Survey of Reasoning Large Language Models.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv: 2502.17419</em><span class="ltx_text" style="font-size:90%;"> (2025).
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Wang et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2025)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Taiyi Wang, Zhihao Wu, Jianheng Liu, Jianye Hao, Jun Wang, and Kun Shao. 2025.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">DistRL: An Asynchronous Distributed Reinforcement Learning Framework for On-Device Control Agents.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">arXiv:2410.14803 [cs.LG]

</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2410.14803" style="font-size:90%;" title="">https://arxiv.org/abs/2410.14803</a><span class="ltx_text" style="font-size:90%;">
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Yao et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2023)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Zhewei Yao, Reza Yazdani Aminabadi, Olatunji Ruwase, Samyam Rajbhandari, Xiaoxia Wu, Ammar Ahmad Awan, Jeff Rasley, Minjia Zhang, Conglong Li, Connor Holmes, Zhongzhu Zhou, Michael Wyatt, Molly Smith, Lev Kurilenko, Heyang Qin, Masahiro Tanaka, Shuai Che, Shuaiwen Leon Song, and Yuxiong He. 2023.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">arXiv:2308.01320 [cs.LG]

</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2308.01320" style="font-size:90%;" title="">https://arxiv.org/abs/2308.01320</a><span class="ltx_text" style="font-size:90%;">
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Sheng et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2025)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. 2025.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">HybridFlow: A Flexible and Efficient RLHF Framework. In </span><em class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the Twentieth European Conference on Computer Systems</em><span class="ltx_text" style="font-size:90%;"> (Rotterdam, Netherlands) </span><em class="ltx_emph ltx_font_italic" style="font-size:90%;">(EuroSys ’25)</em><span class="ltx_text" style="font-size:90%;">. Association for Computing Machinery, New York, NY, USA, 1279–1297.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
</span><a class="ltx_ref ltx_href" href="https://doi.org/10.1145/3689031.3696075" style="font-size:90%;" title="">doi:<span class="ltx_ref ltx_nolink">10.1145/3689031.3696075</span></a><span class="ltx_text" style="font-size:90%;">
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Lei et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2024)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Kinman Lei, Yuyang Jin, Mingshu Zhai, Kezhao Huang, Haoxing Ye, and Jidong Zhai. 2024.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">PUZZLE: Efficiently Aligning Large Language Models through Light-Weight Context Switch. In </span><em class="ltx_emph ltx_font_italic" style="font-size:90%;">2024 USENIX Annual Technical Conference (USENIX ATC 24)</em><span class="ltx_text" style="font-size:90%;">. USENIX Association, Santa Clara, CA, 127–140.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.usenix.org/conference/atc24/presentation/lei" style="font-size:90%;" title="">https://www.usenix.org/conference/atc24/presentation/lei</a><span class="ltx_text" style="font-size:90%;">
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Mei et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2024)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Zhiyu Mei, Wei Fu, Kaiwei Li, Guangju Wang, Huanchen Zhang, and Yi Wu. 2024.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">ReaL: Efficient RLHF Training of Large Language Models with Parameter Reallocation.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv: 2406.14088</em><span class="ltx_text" style="font-size:90%;"> (2024).
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Zhong et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2024)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Yinmin Zhong, Zili Zhang, Bingyang Wu, Shengyu Liu, Yukun Chen, Changyi Wan, Hanpeng Hu, Lei Xia, Ranchen Ming, Yibo Zhu, et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> 2024.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">Rlhfuse: Efficient rlhf training for large language models with inter-and intra-stage fusion.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2409.13221</em><span class="ltx_text" style="font-size:90%;"> (2024).
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Hu et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2024)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Jian Hu, Xibin Wu, Zilin Zhu, Xianyu, Weixun Wang, Dehao Zhang, and Yu Cao. 2024.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">OpenRLHF: An Easy-to-use, Scalable and High-performance RLHF Framework.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">arXiv:2405.11143 [cs.AI]

</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2405.11143" style="font-size:90%;" title="">https://arxiv.org/abs/2405.11143</a><span class="ltx_text" style="font-size:90%;">
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Zhong et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2025)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Yinmin Zhong, Zili Zhang, Xiaoniu Song, Hanpeng Hu, Chao Jin, Bingyang Wu, Nuo Chen, Yukun Chen, Yu Zhou, Changyi Wan, Hongyu Zhou, Yimin Jiang, Yibo Zhu, and Daxin Jiang. 2025.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">StreamRL: Scalable, Heterogeneous, and Elastic RL for LLMs with Disaggregated Stream Generation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">arXiv:2504.15930 [cs]
</span><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2504.15930" style="font-size:90%;" title="">doi:<span class="ltx_ref ltx_nolink">10.48550/arXiv.2504.15930</span></a><span class="ltx_text" style="font-size:90%;">
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Fu et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2025)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Wei Fu, Jiaxuan Gao, Xujie Shen, Chen Zhu, Zhiyu Mei, Chuyi He, Shusheng Xu, Guo Wei, Jun Mei, Jiashu Wang, Tongkai Yang, Binhang Yuan, and Yi Wu. 2025.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">AReaL: A Large-Scale Asynchronous Reinforcement Learning System for Language Reasoning.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">arXiv:2505.24298 [cs]
</span><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2505.24298" style="font-size:90%;" title="">doi:<span class="ltx_ref ltx_nolink">10.48550/arXiv.2505.24298</span></a><span class="ltx_text" style="font-size:90%;">
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Wu et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2025)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Bo Wu, Sid Wang, Yunhao Tang, Jia Ding, Eryk Helenowski, Liang Tan, Tengyu Xu, Tushar Gowda, Zhengxing Chen, Chen Zhu, Xiaocheng Tang, Yundi Qian, Beibei Zhu, and Rui Hou. 2025.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">LlamaRL: A Distributed Asynchronous Reinforcement Learning Framework for Efficient Large-scale LLM Training.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">arXiv:2505.24034 [cs]
</span><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2505.24034" style="font-size:90%;" title="">doi:<span class="ltx_ref ltx_nolink">10.48550/arXiv.2505.24034</span></a><span class="ltx_text" style="font-size:90%;">
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Tan et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2025)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Sijun Tan, Michael Luo, Colin Cai, Tarun Venkat, Kyle Montgomery, Aaron Hao, Tianhao Wu, Arnav Balyan, Manan Roongta, Chenguang Wang, Li Erran Li, Raluca Ada Popa, and Ion Stoica. 2025.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">rLLM: A Framework for Post-Training Language Agents.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://pretty-radio-b75.notion.site/rLLM-A-Framework-for-Post-Training-Language-Agents-21b81902c146819db63cd98a54ba5f31" style="font-size:90%;" title="">https://pretty-radio-b75.notion.site/rLLM-A-Framework-for-Post-Training-Language-Agents-21b81902c146819db63cd98a54ba5f31</a><span class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">Notion Blog.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Team et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2025)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Prime Intellect Team, Sami Jaghouar, Justus Mattern, Jack Min Ong, Jannik Straube, Manveer Basra, Aaron Pazdera, Kushal Thaman, Matthew Di Ferrante, Felix Gabriel, Fares Obeid, Kemal Erdem, Michael Keiblinger, and Johannes Hagemann. 2025.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">INTELLECT-2: A Reasoning Model Trained Through Globally Decentralized Reinforcement Learning.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">arXiv:2505.07291 [cs.LG]

</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2505.07291" style="font-size:90%;" title="">https://arxiv.org/abs/2505.07291</a><span class="ltx_text" style="font-size:90%;">
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Han et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2025)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Zhenyu Han, Ansheng You, Haibo Wang, Kui Luo, Guang Yang, Wenqi Shi, Menglong Chen, Sicheng Zhang, Zeshun Lan, Chunshi Deng, Huazhong Ji, Wenjie Liu, Yu Huang, Yixiang Zhang, Chenyi Pan, Jing Wang, Xin Huang, Chunsheng Li, and Jianping Wu. 2025.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">AsyncFlow: An Asynchronous Streaming RL Framework for Efficient LLM Post-Training.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">arXiv:2507.01663 [cs.LG]

</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2507.01663" style="font-size:90%;" title="">https://arxiv.org/abs/2507.01663</a><span class="ltx_text" style="font-size:90%;">
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Noukhovitch et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2025)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Michael Noukhovitch, Shengyi Huang, Rishabh Agarwal, Aaron Courville, Sophie Xhonneux, and Arian Hosseini. 2025.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">ASYNCHRONOUS RLHF: FASTER AND MORE EFFICIENT OFF-POLICY RL FOR LANGUAGE MODELS.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">(2025).
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Berner et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2019)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemysław Dębiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> 2019.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">Dota 2 with large scale deep reinforcement learning.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1912.06680</em><span class="ltx_text" style="font-size:90%;"> (2019).
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Li et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2018)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Youjie Li, Mingchao Yu, Songze Li, Salman Avestimehr, Nam Sung Kim, and Alexander Schwing. 2018.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">Pipe-SGD: A decentralized pipelined SGD framework for distributed deep net training.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span class="ltx_text" style="font-size:90%;"> 31 (2018).
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Barkai et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2019)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Saar Barkai, Ido Hakimi, and Assaf Schuster. 2019.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">Gap aware mitigation of gradient staleness.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1909.10802</em><span class="ltx_text" style="font-size:90%;"> (2019).
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Chen et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2022)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Yangrui Chen, Cong Xie, Meng Ma, Juncheng Gu, Yanghua Peng, Haibin Lin, Chuan Wu, and Yibo Zhu. 2022.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">SAPipe: Staleness-Aware Pipeline for Data Parallel DNN Training. In </span><em class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Sheng et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2024)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Guangming Sheng, Junwei Su, Chao Huang, and Chuan Wu. 2024.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">Mspipe: Efficient temporal gnn training via staleness-aware pipeline. In </span><em class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</em><span class="ltx_text" style="font-size:90%;">. 2651–2662.
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Jimenez et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2023)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. 2023.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">Swe-bench: Can language models resolve real-world github issues?
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2310.06770</em><span class="ltx_text" style="font-size:90%;"> (2023).
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Cheng et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2025)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Yao Cheng, Jianfeng Chen, Jie Chen, Li Chen, Liyu Chen, and et al. 2025.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">FullStack Bench: Evaluating LLMs as Full Stack Coders.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">arXiv:2412.00535 [cs.AI]

</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2412.00535" style="font-size:90%;" title="">https://arxiv.org/abs/2412.00535</a><span class="ltx_text" style="font-size:90%;">
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">OpenAI (2025)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
OpenAI. 2025.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">Introducing deep research.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openai.com/index/introducing-deep-research/" style="font-size:90%;" title="">https://openai.com/index/introducing-deep-research/</a><span class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Ouyang et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2022)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> 2022.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">Training language models to follow instructions with human feedback.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span class="ltx_text" style="font-size:90%;"> 35 (2022), 27730–27744.
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Bai et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2022)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> 2022.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">Training a helpful and harmless assistant with reinforcement learning from human feedback.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2204.05862</em><span class="ltx_text" style="font-size:90%;"> (2022).
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Dai et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2024)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Josef Dai, Xuehai Pan, Ruiyang Sun, Jiaming Ji, Xinbo Xu, Mickel Liu, Yizhou Wang, and Yaodong Yang. 2024.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">Safe RLHF: Safe Reinforcement Learning from Human Feedback. In </span><em class="ltx_emph ltx_font_italic" style="font-size:90%;">The Twelfth International Conference on Learning Representations</em><span class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=TyFrPOKYXw" style="font-size:90%;" title="">https://openreview.net/forum?id=TyFrPOKYXw</a><span class="ltx_text" style="font-size:90%;">
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Zheng et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2023)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Rui Zheng, Wei Shen, Yuan Hua, Wenbin Lai, Shihan Dou, Yuhao Zhou, Zhiheng Xi, Xiao Wang, Haoran Huang, Tao Gui, et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> 2023.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">Improving generalization of alignment with human preferences through group invariant learning.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2310.11971</em><span class="ltx_text" style="font-size:90%;"> (2023).
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Lee et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2023)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop, Victor Carbune, and Abhinav Rastogi. 2023.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">Rlaif: Scaling reinforcement learning from human feedback with ai feedback.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2309.00267</em><span class="ltx_text" style="font-size:90%;"> (2023).
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Shao et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2024)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Y Wu, et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> 2024.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">Deepseekmath: Pushing the limits of mathematical reasoning in open language models.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2402.03300</em><span class="ltx_text" style="font-size:90%;"> (2024).
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Feng et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2025)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Jiazhan Feng, Shijue Huang, Xingwei Qu, Ge Zhang, Yujia Qin, Baoquan Zhong, Chengquan Jiang, Jinxin Chi, and Wanjun Zhong. 2025.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">ReTool: Reinforcement Learning for Strategic Tool Use in LLMs.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv: 2504.11536</em><span class="ltx_text" style="font-size:90%;"> (2025).
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">of America (2024)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Mathematical Association of America. 2024.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">AIME 2024.
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Schulman et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2017)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">Proximal policy optimization algorithms.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1707.06347</em><span class="ltx_text" style="font-size:90%;"> (2017).
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Ahmadian et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2024)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Arash Ahmadian, Chris Cremer, Matthias Gallé, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin, Ahmet Üstün, and Sara Hooker. 2024.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">Back to Basics: Revisiting REINFORCE Style Optimization for Learning from Human Feedback in LLMs.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">arXiv:2402.14740 [cs.LG]

</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2402.14740" style="font-size:90%;" title="">https://arxiv.org/abs/2402.14740</a><span class="ltx_text" style="font-size:90%;">
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Mialon et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2023)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Grégoire Mialon, Clémentine Fourrier, Craig Swift, Thomas Wolf, Yann LeCun, and Thomas Scialom. 2023.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">GAIA: a benchmark for General AI Assistants.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">arXiv:2311.12983 [cs.CL]

</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2311.12983" style="font-size:90%;" title="">https://arxiv.org/abs/2311.12983</a><span class="ltx_text" style="font-size:90%;">
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Xie et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2024)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Jing Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, Yitao Liu, Yiheng Xu, Shuyan Zhou, Silvio Savarese, Caiming Xiong, Victor Zhong, and Tao Yu. 2024.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">arXiv:2404.07972 [cs.AI]

</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2404.07972" style="font-size:90%;" title="">https://arxiv.org/abs/2404.07972</a><span class="ltx_text" style="font-size:90%;">
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Vinyals et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2019)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> 2019.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">Grandmaster level in StarCraft II using multi-agent reinforcement learning.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" style="font-size:90%;">nature</em><span class="ltx_text" style="font-size:90%;"> 575, 7782 (2019), 350–354.
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Shen et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2024)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Gerald Shen, Zhilin Wang, Olivier Delalleau, Jiaqi Zeng, Yi Dong, Daniel Egert, Shengyang Sun, Jimmy Zhang, Sahil Jain, Ali Taghibakhshi, Markel Sanz Ausin, Ashwath Aithal, and Oleksii Kuchaiev. 2024.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">NeMo-Aligner: Scalable Toolkit for Efficient Model Alignment.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">arXiv:2405.01481 [cs.CL]

</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2405.01481" style="font-size:90%;" title="">https://arxiv.org/abs/2405.01481</a><span class="ltx_text" style="font-size:90%;">
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Xiao et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2023)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Youshao Xiao, Zhenglei Zhou, Fagui Mao, Weichang Wu, Shangchun Zhao, Lin Ju, Lei Liang, Xiaolu Zhang, and Jun Zhou. 2023.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">An Adaptive Placement and Parallelism Framework for Accelerating RLHF Training.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv: 2312.11819</em><span class="ltx_text" style="font-size:90%;"> (2023).
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Gu et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2017)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Shixiang Gu, Ethan Holly, Timothy Lillicrap, and Sergey Levine. 2017.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates. In </span><em class="ltx_emph ltx_font_italic" style="font-size:90%;">2017 IEEE international conference on robotics and automation (ICRA)</em><span class="ltx_text" style="font-size:90%;">. IEEE, 3389–3396.
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Kahn et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2018)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Gregory Kahn, Adam Villaflor, Pieter Abbeel, and Sergey Levine. 2018.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">Composable action-conditioned predictors: Flexible off-policy learning for robot navigation. In </span><em class="ltx_emph ltx_font_italic" style="font-size:90%;">Conference on robot learning</em><span class="ltx_text" style="font-size:90%;">. PMLR, 806–816.
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Qin et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2025)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Ruoyu Qin, Zheming Li, Weiran He, Jialei Cui, Feng Ren, Mingxing Zhang, Yongwei Wu, Weimin Zheng, and Xinran Xu. 2025.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">Mooncake: Trading more storage for less computation—a </span><math alttext="\{" class="ltx_Math" display="inline" id="bib.bib50.m1" intent=":literal"><semantics><mo maxsize="0.900em" minsize="0.900em">{</mo><annotation encoding="application/x-tex">\{</annotation></semantics></math><span class="ltx_text" style="font-size:90%;">KVCache-centric</span><math alttext="\}" class="ltx_Math" display="inline" id="bib.bib50.m2" intent=":literal"><semantics><mo maxsize="0.900em" minsize="0.900em">}</mo><annotation encoding="application/x-tex">\}</annotation></semantics></math><span class="ltx_text" style="font-size:90%;"> architecture for serving </span><math alttext="\{" class="ltx_Math" display="inline" id="bib.bib50.m3" intent=":literal"><semantics><mo maxsize="0.900em" minsize="0.900em">{</mo><annotation encoding="application/x-tex">\{</annotation></semantics></math><span class="ltx_text" style="font-size:90%;">LLM</span><math alttext="\}" class="ltx_Math" display="inline" id="bib.bib50.m4" intent=":literal"><semantics><mo maxsize="0.900em" minsize="0.900em">}</mo><annotation encoding="application/x-tex">\}</annotation></semantics></math><span class="ltx_text" style="font-size:90%;"> chatbot. In </span><em class="ltx_emph ltx_font_italic" style="font-size:90%;">23rd USENIX Conference on File and Storage Technologies (FAST 25)</em><span class="ltx_text" style="font-size:90%;">. 155–170.
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Kwon et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2023)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. 2023.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">Efficient memory management for large language model serving with pagedattention. In </span><em class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 29th Symposium on Operating Systems Principles</em><span class="ltx_text" style="font-size:90%;">. 611–626.
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Zhu et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2025)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Kan Zhu, Yufei Gao, Yilong Zhao, Liangyu Zhao, Gefei Zuo, Yile Gu, Dedong Xie, Zihao Ye, Keisuke Kamahori, Chien-Yu Lin, et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> 2025.
</span>
</span>
<span class="ltx_bibblock"><math alttext="\{" class="ltx_Math" display="inline" id="bib.bib52.m1" intent=":literal"><semantics><mo maxsize="0.900em" minsize="0.900em">{</mo><annotation encoding="application/x-tex">\{</annotation></semantics></math><span class="ltx_text" style="font-size:90%;">NanoFlow</span><math alttext="\}" class="ltx_Math" display="inline" id="bib.bib52.m2" intent=":literal"><semantics><mo maxsize="0.900em" minsize="0.900em">}</mo><annotation encoding="application/x-tex">\}</annotation></semantics></math><span class="ltx_text" style="font-size:90%;">: Towards Optimal Large Language Model Serving Throughput. In </span><em class="ltx_emph ltx_font_italic" style="font-size:90%;">19th USENIX Symposium on Operating Systems Design and Implementation (OSDI 25)</em><span class="ltx_text" style="font-size:90%;">. 749–765.
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Zheng et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2023)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Jeff Huang, Chuyue Sun, Cody Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph E. Gonzalez, Clark Barrett, and Ying Sheng. 2023.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">Efficiently Programming Large Language Models using SGLang.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv: 2312.07104</em><span class="ltx_text" style="font-size:90%;"> (2023).
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Agrawal et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2024)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Amey Agrawal, Nitin Kedia, Ashish Panwar, Jayashree Mohan, Nipun Kwatra, Bhargav Gulavani, Alexey Tumanov, and Ramachandran Ramjee. 2024.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">Taming </span><math alttext="\{" class="ltx_Math" display="inline" id="bib.bib54.m1" intent=":literal"><semantics><mo maxsize="0.900em" minsize="0.900em">{</mo><annotation encoding="application/x-tex">\{</annotation></semantics></math><span class="ltx_text" style="font-size:90%;">Throughput-Latency</span><math alttext="\}" class="ltx_Math" display="inline" id="bib.bib54.m2" intent=":literal"><semantics><mo maxsize="0.900em" minsize="0.900em">}</mo><annotation encoding="application/x-tex">\}</annotation></semantics></math><span class="ltx_text" style="font-size:90%;"> tradeoff in </span><math alttext="\{" class="ltx_Math" display="inline" id="bib.bib54.m3" intent=":literal"><semantics><mo maxsize="0.900em" minsize="0.900em">{</mo><annotation encoding="application/x-tex">\{</annotation></semantics></math><span class="ltx_text" style="font-size:90%;">LLM</span><math alttext="\}" class="ltx_Math" display="inline" id="bib.bib54.m4" intent=":literal"><semantics><mo maxsize="0.900em" minsize="0.900em">}</mo><annotation encoding="application/x-tex">\}</annotation></semantics></math><span class="ltx_text" style="font-size:90%;"> inference with </span><math alttext="\{" class="ltx_Math" display="inline" id="bib.bib54.m5" intent=":literal"><semantics><mo maxsize="0.900em" minsize="0.900em">{</mo><annotation encoding="application/x-tex">\{</annotation></semantics></math><span class="ltx_text" style="font-size:90%;">Sarathi-Serve</span><math alttext="\}" class="ltx_Math" display="inline" id="bib.bib54.m6" intent=":literal"><semantics><mo maxsize="0.900em" minsize="0.900em">}</mo><annotation encoding="application/x-tex">\}</annotation></semantics></math><span class="ltx_text" style="font-size:90%;">. In </span><em class="ltx_emph ltx_font_italic" style="font-size:90%;">18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24)</em><span class="ltx_text" style="font-size:90%;">. 117–134.
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Sun et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2024)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Biao Sun, Ziming Huang, Hanyu Zhao, Wencong Xiao, Xinyi Zhang, Yong Li, and Wei Lin. 2024.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">Llumnix: Dynamic scheduling for large language model serving. In </span><em class="ltx_emph ltx_font_italic" style="font-size:90%;">18th USENIX symposium on operating systems design and implementation (OSDI 24)</em><span class="ltx_text" style="font-size:90%;">. 173–191.
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Zhu et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2025)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Ruidong Zhu, Ziheng Jiang, Chao Jin, Peng Wu, Cesar A Stuardo, Dongyang Wang, Xinlei Zhang, Huaping Zhou, Haoran Wei, Yang Cheng, et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> 2025.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">MegaScale-Infer: Serving Mixture-of-Experts at Scale with Disaggregated Expert Parallelism.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2504.02263</em><span class="ltx_text" style="font-size:90%;"> (2025).
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Team et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2025)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Kimi Team, Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, Yanru Chen, Yuankun Chen, Yutian Chen, et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> 2025.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">Kimi K2: Open Agentic Intelligence.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2507.20534</em><span class="ltx_text" style="font-size:90%;"> (2025).
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">ncc (2023)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
2023.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">NVIDIA Collective Communications Library (NCCL).
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://developer.nvidia.com/nccl" style="font-size:90%;" title="">https://developer.nvidia.com/nccl</a><span class="ltx_text" style="font-size:90%;">
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Hu et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2025)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Zhiyi Hu, Siyuan Shen, Tommaso Bonato, Sylvain Jeaugey, Cedell Alexander, Eric Spada, James Dinan, Jeff Hammond, and Torsten Hoefler. 2025.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">Demystifying NCCL: An In-depth Analysis of GPU Communication Protocols and Algorithms.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">arXiv:2507.04786 [cs.DC]

</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2507.04786" style="font-size:90%;" title="">https://arxiv.org/abs/2507.04786</a><span class="ltx_text" style="font-size:90%;">
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Wagenländer et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2024)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Marcel Wagenländer, Guo Li, Bo Zhao, Luo Mai, and Peter Pietzuch. 2024.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">Tenplex: Dynamic parallelism for deep learning using parallelizable tensor collections. In </span><em class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the ACM SIGOPS 30th Symposium on Operating Systems Principles</em><span class="ltx_text" style="font-size:90%;">. 195–210.
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Wan et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2025)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Borui Wan, Mingji Han, Yiyao Sheng, Yanghua Peng, Haibin Lin, Mofan Zhang, Zhichao Lai, Menghan Yu, Junda Zhang, Zuquan Song, Xin Liu, and Chuan Wu. 2025.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">ByteCheckpoint: A Unified Checkpointing System for Large Foundation Model Development. In </span><em class="ltx_emph ltx_font_italic" style="font-size:90%;">22nd USENIX Symposium on Networked Systems Design and Implementation (NSDI 25)</em><span class="ltx_text" style="font-size:90%;">. USENIX Association, Philadelphia, PA, 559–578.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.usenix.org/conference/nsdi25/presentation/wan-borui" style="font-size:90%;" title="">https://www.usenix.org/conference/nsdi25/presentation/wan-borui</a><span class="ltx_text" style="font-size:90%;">
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Xingda Wei and Chen (2025)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Tianle Sun Yingyi Hao Rong Chen Mingcong Han Jinyu Gu Xingda Wei, Zhuobin Huang and Haibo Chen. 2025.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">PhoenixOS: Concurrent OS-level GPU Checkpoint and Restore with Validated Speculation. In </span><em class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the ACM SIGOPS 31th Symposium on Operating Systems Principles</em><span class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">ama (2023)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
2023.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">SageMaker.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://docs.aws.amazon.com/sagemaker/index.html" style="font-size:90%;" title="">https://docs.aws.amazon.com/sagemaker/index.html</a><span class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Hu et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2024)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Qinghao Hu, Zhisheng Ye, Zerui Wang, Guoteng Wang, Meng Zhang, Qiaoling Chen, Peng Sun, Dahua Lin, Xiaolin Wang, Yingwei Luo, et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> 2024.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">Characterization of large language model development in the datacenter. In </span><em class="ltx_emph ltx_font_italic" style="font-size:90%;">21st USENIX Symposium on Networked Systems Design and Implementation (NSDI 24)</em><span class="ltx_text" style="font-size:90%;">. 709–729.
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Mei et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2023)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Zhiyu Mei, Wei Fu, Guang Wang, Huanchen Zhang, and Yi Wu. 2023.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">SRL: Scaling Distributed Reinforcement Learning to Over Ten Thousand Cores.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" style="font-size:90%;">International Conference on Learning Representations</em><span class="ltx_text" style="font-size:90%;"> (2023).
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2306.16688" style="font-size:90%;" title="">doi:<span class="ltx_ref ltx_nolink">10.48550/arXiv.2306.16688</span></a><span class="ltx_text" style="font-size:90%;">
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Batsakis et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2009)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Alexandros Batsakis, Randal Burns, Arkady Kanevsky, James Lentini, and Thomas Talpey. 2009.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">Ca-nfs: A congestion-aware network file system.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" style="font-size:90%;">ACM Transactions on Storage (TOS)</em><span class="ltx_text" style="font-size:90%;"> 5, 4 (2009), 1–24.
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">OpenAI et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2019)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
OpenAI, :, Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemysław Dębiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, Rafal Józefowicz, Scott Gray, Catherine Olsson, Jakub Pachocki, Michael Petrov, Henrique P. d. O. Pinto, Jonathan Raiman, Tim Salimans, Jeremy Schlatter, Jonas Schneider, Szymon Sidor, Ilya Sutskever, Jie Tang, Filip Wolski, and Susan Zhang. 2019.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">Dota 2 with Large Scale Deep Reinforcement Learning.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv: 1912.06680</em><span class="ltx_text" style="font-size:90%;"> (2019).
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">S (2009)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Sanfilippo S. 2009.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">“Redis In-memory Data Structure Server”.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://redis.io/" style="font-size:90%;" title="">https://redis.io/</a><span class="ltx_text" style="font-size:90%;">
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Ikkert et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2009)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Michael Ikkert, Tim Kieritz, and Peter Sanders. 2009.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" style="font-size:90%;">Parallele Algorithmen</em><span class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">Technical Report. Karlsruher Institut für Technologie (KIT).
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ae.iti.kit.edu/documents/teaching/parallele-algorithmen/ws18/skript.pdf" style="font-size:90%;" title="">https://ae.iti.kit.edu/documents/teaching/parallele-algorithmen/ws18/skript.pdf</a><span class="ltx_text" style="font-size:90%;">
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">Stand: 16. Oktober 2009.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Johnson (1974)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
David S Johnson. 1974.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">Fast algorithms for bin packing.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" style="font-size:90%;">J. Comput. System Sci.</em><span class="ltx_text" style="font-size:90%;"> 8, 3 (1974), 272–314.
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Yao (1980)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Andrew Chi-Chih Yao. 1980.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">New algorithms for bin packing.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" style="font-size:90%;">Journal of the ACM (JACM)</em><span class="ltx_text" style="font-size:90%;"> 27, 2 (1980), 207–227.
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Johnson et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (1974)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
David S. Johnson, Alan Demers, Jeffrey D. Ullman, Michael R Garey, and Ronald L. Graham. 1974.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">Worst-case performance bounds for simple one-dimensional packing algorithms.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" style="font-size:90%;">SIAM Journal on computing</em><span class="ltx_text" style="font-size:90%;"> 3, 4 (1974), 299–325.
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Williams et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2009)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Samuel Williams, Andrew Waterman, and David Patterson. 2009.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">Roofline: an insightful visual performance model for multicore architectures.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" style="font-size:90%;">Commun. ACM</em><span class="ltx_text" style="font-size:90%;"> 52, 4 (2009), 65–76.
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Yuan et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2024)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Zhihang Yuan, Yuzhang Shang, Yang Zhou, Zhen Dong, Zhe Zhou, Chenhao Xue, Bingzhe Wu, Zhikai Li, Qingyi Gu, Yong Jae Lee, Yan Yan, Beidi Chen, Guangyu Sun, and Kurt Keutzer. 2024.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">LLM Inference Unveiled: Survey and Roofline Model Insights.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">arXiv:2402.16363 [cs.CL]

</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2402.16363" style="font-size:90%;" title="">https://arxiv.org/abs/2402.16363</a><span class="ltx_text" style="font-size:90%;">
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Schaul et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2016)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. 2016.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">Prioritized Experience Replay.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">arXiv:1511.05952
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Moritz et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2018)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Philipp Moritz, Robert Nishihara, Stephanie Wang, Alexey Tumanov, Richard Liaw, Eric Liang, Melih Elibol, Zongheng Yang, William Paul, Michael I Jordan, et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> 2018.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">Ray: A distributed framework for emerging </span><math alttext="\{" class="ltx_Math" display="inline" id="bib.bib76.m1" intent=":literal"><semantics><mo maxsize="0.900em" minsize="0.900em">{</mo><annotation encoding="application/x-tex">\{</annotation></semantics></math><span class="ltx_text" style="font-size:90%;">AI</span><math alttext="\}" class="ltx_Math" display="inline" id="bib.bib76.m2" intent=":literal"><semantics><mo maxsize="0.900em" minsize="0.900em">}</mo><annotation encoding="application/x-tex">\}</annotation></semantics></math><span class="ltx_text" style="font-size:90%;"> applications. In </span><em class="ltx_emph ltx_font_italic" style="font-size:90%;">13th USENIX symposium on operating systems design and implementation (OSDI 18)</em><span class="ltx_text" style="font-size:90%;">. 561–577.
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">ucx (2019)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
2019.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">Unified Communication X.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/openucx/ucx" style="font-size:90%;" title="">https://github.com/openucx/ucx</a><span class="ltx_text" style="font-size:90%;">
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Team et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2024)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Qwen Team, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang
Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. 2024.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">Qwen2.5 Technical Report.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv: 2412.15115</em><span class="ltx_text" style="font-size:90%;"> (2024).
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Zhao et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2023)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Yanli Zhao, A. Gu, R. Varma, Liangchen Luo, Chien chin Huang, Min Xu, Less Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, Alban Desmaison, Can Balioglu, Bernard Nguyen, Geeta Chauhan, Y. Hao, and Shen Li. 2023.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the VLDB Endowment</em><span class="ltx_text" style="font-size:90%;"> (2023).
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.14778/3611540.3611569" style="font-size:90%;" title="">doi:<span class="ltx_ref ltx_nolink">10.14778/3611540.3611569</span></a><span class="ltx_text" style="font-size:90%;">
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Narayanan et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2021)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa Patwary, Vijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer, Bryan Catanzaro, et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> 2021.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">Efficient large-scale language model training on gpu clusters using megatron-lm. In </span><em class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis</em><span class="ltx_text" style="font-size:90%;">. 1–15.
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Qin et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2024)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Xuanhe Zhou, Yufei Huang, Chaojun Xiao, et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> 2024.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">Tool learning with foundation models.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" style="font-size:90%;">Comput. Surveys</em><span class="ltx_text" style="font-size:90%;"> 57, 4 (2024), 1–40.
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Amdahl (1967)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Gene M Amdahl. 1967.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">Validity of the single processor approach to achieving large scale computing capabilities. In </span><em class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the April 18-20, 1967, spring joint computer conference</em><span class="ltx_text" style="font-size:90%;">. 483–485.
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Hilton et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2022)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Jacob Hilton, Karl Cobbe, and John Schulman. 2022.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">Batch Size-Invariance for Policy Optimization.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">arXiv:2110.00641
</span><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2110.00641" style="font-size:90%;" title="">doi:<span class="ltx_ref ltx_nolink">10.48550/arXiv.2110.00641</span></a><span class="ltx_text" style="font-size:90%;">
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Corporation (2023)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Collosal-AI Corporation. 2023.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" style="font-size:90%;">Collosal-Chat.</em><span class="ltx_text" style="font-size:90%;">
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/binmakeswell/ColossalChat" style="font-size:90%;" title="">https://github.com/binmakeswell/ColossalChat</a><span class="ltx_text" style="font-size:90%;">
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib85">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">von Werra et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2020)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Leandro von Werra, Younes Belkada, Lewis Tunstall, Edward Beeching, Tristan Thrush, Nathan Lambert, Shengyi Huang, Kashif Rasul, and Quentin Gallouédec. 2020.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">TRL: Transformer Reinforcement Learning.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/huggingface/trl" style="font-size:90%;" title="">https://github.com/huggingface/trl</a><span class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib86">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">nem (2025)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
2025.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">NeMo RL: A Scalable and Efficient Post-Training Library.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/NVIDIA-NeMo/RL" style="font-size:90%;" title="">https://github.com/NVIDIA-NeMo/RL</a><span class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">GitHub repository.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib87">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Griggs et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2025)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Tyler Griggs, Sumanth Hegde, Eric Tang, Shu Liu, Shiyi Cao, Dacheng Li, Charlie Ruan, Philipp Moritz, Kourosh Hakhamaneshi, Richard Liaw, Akshay Malik, Matei Zaharia, Joseph E. Gonzalez, and Ion Stoica. 2025.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">Evolving SkyRL into a Highly-Modular RL Framework.
</span>
</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">Notion Blog.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib88">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Zhu et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2025)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Zilin Zhu, Chengxing Xie, Xin Lv, and slime Contributors. 2025.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">slime: An LLM post-training framework for RL Scaling.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/THUDM/slime" style="font-size:90%;" title="">https://github.com/THUDM/slime</a><span class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">GitHub repository. Corresponding author: Xin Lv.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib89">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Hesse et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2017)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
C. Hesse, M. Plappert, A. Radford, J. Schulman, S. Sidor, and Y. Wu. 2017.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" style="font-size:90%;">OpenAI baselines</em><span class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/openai/baselines" style="font-size:90%;" title="">https://github.com/openai/baselines</a><span class="ltx_text" style="font-size:90%;">
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib90">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Hafner et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2017)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Danijar Hafner, James Davidson, and Vincent Vanhoucke. 2017.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">Tensorflow agents: Efficient batched reinforcement learning in tensorflow.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1709.02878</em><span class="ltx_text" style="font-size:90%;"> (2017).
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib91">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Bou et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2023)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Albert Bou, Matteo Bettini, Sebastian Dittert, Vikash Kumar, Shagun Sodhani, Xiaomeng Yang, Gianni De Fabritiis, and Vincent Moens. 2023.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">TorchRL: A data-driven decision-making library for PyTorch.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">arXiv:2306.00577 [cs.LG]

</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2306.00577" style="font-size:90%;" title="">https://arxiv.org/abs/2306.00577</a><span class="ltx_text" style="font-size:90%;">
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib92">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Espeholt et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2018)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Volodymir Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, Shane Legg, and Koray Kavukcuoglu. 2018.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">arXiv:1802.01561 [cs]
</span><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.1802.01561" style="font-size:90%;" title="">doi:<span class="ltx_ref ltx_nolink">10.48550/arXiv.1802.01561</span></a><span class="ltx_text" style="font-size:90%;">
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib93">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Liang et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2018)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Eric Liang, Richard Liaw, Robert Nishihara, Philipp Moritz, Roy Fox, Ken Goldberg, Joseph Gonzalez, Michael Jordan, and Ion Stoica. 2018.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">RLlib: Abstractions for Distributed Reinforcement Learning. In </span><em class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 35th International Conference on Machine Learning</em><span class="ltx_text" style="font-size:90%;"> </span><em class="ltx_emph ltx_font_italic" style="font-size:90%;">(Proceedings of Machine Learning Research, Vol. 80)</em><span class="ltx_text" style="font-size:90%;">, Jennifer Dy and Andreas Krause (Eds.). PMLR, 3053–3062.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.mlr.press/v80/liang18b.html" style="font-size:90%;" title="">https://proceedings.mlr.press/v80/liang18b.html</a><span class="ltx_text" style="font-size:90%;">
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib94">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Liang et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2021)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Eric Liang, Zhanghao Wu, Michael Luo, Sven Mika, Joseph E Gonzalez, and Ion Stoica. 2021.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">RLlib Flow: Distributed Reinforcement Learning is a Dataflow Problem. In </span><em class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span class="ltx_text" style="font-size:90%;">, M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (Eds.), Vol. 34. Curran Associates, Inc., 5506–5517.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.neurips.cc/paper_files/paper/2021/file/2bce32ed409f5ebcee2a7b417ad9beed-Paper.pdf" style="font-size:90%;" title="">https://proceedings.neurips.cc/paper_files/paper/2021/file/2bce32ed409f5ebcee2a7b417ad9beed-Paper.pdf</a><span class="ltx_text" style="font-size:90%;">
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib95">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Weng et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2022)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Jiayi Weng, Huayu Chen, Dong Yan, Kaichao You, Alexis Duburcq, Minghao Zhang, Yi Su, Hang Su, and Jun Zhu. 2022.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">Tianshou: A Highly Modularized Deep Reinforcement Learning Library.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" style="font-size:90%;">Journal of Machine Learning Research</em><span class="ltx_text" style="font-size:90%;"> 23, 267 (2022), 1–6.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="http://jmlr.org/papers/v23/21-1127.html" style="font-size:90%;" title="">http://jmlr.org/papers/v23/21-1127.html</a><span class="ltx_text" style="font-size:90%;">
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib96">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Recht et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2011)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Benjamin Recht, Christopher Re, Stephen Wright, and Feng Niu. 2011.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">Hogwild!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent. In </span><em class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span class="ltx_text" style="font-size:90%;">, J. Shawe-Taylor, R. Zemel, P. Bartlett, F. Pereira, and K.Q. Weinberger (Eds.), Vol. 24. Curran Associates, Inc.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.neurips.cc/paper_files/paper/2011/file/218a0aefd1d1a4be65601cc6ddc1520e-Paper.pdf" style="font-size:90%;" title="">https://proceedings.neurips.cc/paper_files/paper/2011/file/218a0aefd1d1a4be65601cc6ddc1520e-Paper.pdf</a><span class="ltx_text" style="font-size:90%;">
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib97">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Mnih et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2015)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. 2015.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">Human-Level Control through Deep Reinforcement Learning.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" style="font-size:90%;">Nature</em><span class="ltx_text" style="font-size:90%;"> 518, 7540 (Feb. 2015), 529–533.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
</span><a class="ltx_ref ltx_href" href="https://doi.org/10.1038/nature14236" style="font-size:90%;" title="">doi:<span class="ltx_ref ltx_nolink">10.1038/nature14236</span></a><span class="ltx_text" style="font-size:90%;">
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib98">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Mnih et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2016)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Volodymyr Mnih, Adrià Puigdomènech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. 2016.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">Asynchronous Methods for Deep Reinforcement Learning.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">(June 2016).
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">arXiv:1602.01783 [cs.LG]
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib99">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Horgan et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2018)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Dan Horgan, John Quan, David Budden, Gabriel Barth-Maron, Matteo Hessel, Hado van Hasselt, and David Silver. 2018.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">Distributed Prioritized Experience Replay.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">arXiv:1803.00933 [cs]
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib100">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Cassirer et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2021)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Albin Cassirer, Gabriel Barth-Maron, Eugene Brevdo, Sabela Ramos, Toby Boyd, Thibault Sottiaux, and Manuel Kroiss. 2021.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">Reverb: A Framework For Experience Replay.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">arXiv:2102.04736 [cs.LG]
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib101">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Wang et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2023)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Hanjing Wang, Man-Kit Sit, Congjie He, Ying Wen, Weinan Zhang, Jun Wang, Yaodong Yang, and Luo Mai. 2023.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">GEAR: A GPU-Centric Experience Replay System for Large Reinforcement Learning Models. In </span><em class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 40th International Conference on Machine Learning</em><span class="ltx_text" style="font-size:90%;">. PMLR, 36380–36390.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib102">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Zu et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2024)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Yazhou Zu, Alireza Ghaffarkhah, Hoang-Vu Dang, Brian Towles, Steven Hand, Safeen Huda, Adekunle Bello, Alexander Kolbasov, Arash Rezaei, Dayou Du, Steve Lacy, Hang Wang, Aaron Wisner, Chris Lewis, and Henri Bahini. 2024.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">Resiliency at Scale: Managing Google’s TPUv4 Machine Learning Supercomputer. In </span><em class="ltx_emph ltx_font_italic" style="font-size:90%;">21st USENIX Symposium on Networked Systems Design and Implementation (NSDI 24)</em><span class="ltx_text" style="font-size:90%;">. USENIX Association, Santa Clara, CA, 761–774.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.usenix.org/conference/nsdi24/presentation/zu" style="font-size:90%;" title="">https://www.usenix.org/conference/nsdi24/presentation/zu</a><span class="ltx_text" style="font-size:90%;">
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib103">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Hu et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2024)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Qinghao Hu, Zhisheng Ye, Zerui Wang, Guoteng Wang, Meng Zhang, Qiaoling Chen, Peng Sun, Dahua Lin, Xiaolin Wang, Yingwei Luo, Yonggang Wen, and Tianwei Zhang. 2024.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">Characterization of Large Language Model Development in the Datacenter. In </span><em class="ltx_emph ltx_font_italic" style="font-size:90%;">21st USENIX Symposium on Networked Systems Design and Implementation (NSDI 24)</em><span class="ltx_text" style="font-size:90%;">. USENIX Association, Santa Clara, CA, 709–729.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.usenix.org/conference/nsdi24/presentation/hu" style="font-size:90%;" title="">https://www.usenix.org/conference/nsdi24/presentation/hu</a><span class="ltx_text" style="font-size:90%;">
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib104">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Jiang et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2024)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Ziheng Jiang, Haibin Lin, Yinmin Zhong, Qi Huang, Yangrui Chen, Zhi Zhang, Yanghua Peng, Xiang Li, Cong Xie, Shibiao Nong, Yulu Jia, Sun He, Hongmin Chen, Zhihao Bai, Qi Hou, Shipeng Yan, Ding Zhou, Yiyao Sheng, Zhuo Jiang, Haohan Xu, Haoran Wei, Zhang Zhang, Pengfei Nie, Leqi Zou, Sida Zhao, Liang Xiang, Zherui Liu, Zhe Li, Xiaoying Jia, Jianxi Ye, Xin Jin, and Xin Liu. 2024.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">MegaScale: Scaling Large Language Model Training to More Than 10,000 GPUs. In </span><em class="ltx_emph ltx_font_italic" style="font-size:90%;">21st USENIX Symposium on Networked Systems Design and Implementation (NSDI 24)</em><span class="ltx_text" style="font-size:90%;">. USENIX Association, Santa Clara, CA, 745–760.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.usenix.org/conference/nsdi24/presentation/jiang-ziheng" style="font-size:90%;" title="">https://www.usenix.org/conference/nsdi24/presentation/jiang-ziheng</a><span class="ltx_text" style="font-size:90%;">
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib105">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Dong et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2025)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Jianbo Dong, Kun Qian, Pengcheng Zhang, Zhilong Zheng, Liang Chen, Fei Feng, Yichi Xu, Yikai Zhu, Gang Lu, Xue Li, Zhihui Ren, Zhicheng Wang, Bin Luo, Peng Zhang, Yang Liu, Yanqing Chen, Yu Guan, Weicheng Wang, Chaojie Yang, Yang Zhang, Man Yuan, Hanyu Zhao, Yong Li, Zihan Zhao, Shan Li, Xianlong Zeng, Zhiping Yao, Binzhang Fu, Ennan Zhai, Wei Lin, Chao Wang, and Dennis Cai. 2025.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">Evolution of Aegis: Fault Diagnosis for AI Model Training Service in Production. In </span><em class="ltx_emph ltx_font_italic" style="font-size:90%;">22nd USENIX Symposium on Networked Systems Design and Implementation (NSDI 25)</em><span class="ltx_text" style="font-size:90%;">. USENIX Association, Philadelphia, PA, 865–881.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.usenix.org/conference/nsdi25/presentation/dong" style="font-size:90%;" title="">https://www.usenix.org/conference/nsdi25/presentation/dong</a><span class="ltx_text" style="font-size:90%;">
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib106">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Deng et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2025)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Yangtao Deng, Lei Zhang, Qinlong Wang, Xiaoyun Zhi, Xinlei Zhang, Zhuo Jiang, Haohan Xu, Lei Wang, Zuquan Song, Gaohong Liu, et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> 2025.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">Mycroft: Tracing Dependencies in Collective Communication Towards Reliable LLM Training.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2509.03018</em><span class="ltx_text" style="font-size:90%;"> (2025).
</span>
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib107">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Wan et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2025)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Borui Wan, Gaohong Liu, Zuquan Song, Jun Wang, Yun Zhang, Guangming Sheng, Shuguang Wang, Houmin Wei, Chenyuan Wang, Weiqiang Lou, Xi Yang, Mofan Zhang, Kaihua Jiang, Cheng Ren, Xiaoyun Zhi, Menghan Yu, Zhe Nan, Zhuolin Zheng, Baoquan Zhong, Qinlong Wang, Huan Yu, Jinxin Chi, Wang Zhang, Yuhan Li, Zixian Du, Sida Zhao, Yongqiang Zhang, Jingzhe Tang, Zherui Liu, Chuan Wu, Yanghua Peng, Haibin Lin, Wencong
Xiao, Xin Liu, and Liang Xiang. 2025.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">Robust LLM Training Infrastructure at ByteDance.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">arXiv:2509.16293 [cs.LG]

</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2509.16293" style="font-size:90%;" title="">https://arxiv.org/abs/2509.16293</a><span class="ltx_text" style="font-size:90%;">
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib108">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Zhu et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2015)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Yibo Zhu, Nanxi Kang, Jiaxin Cao, Albert Greenberg, Guohan Lu, Ratul Mahajan, Dave Maltz, Lihua Yuan, Ming Zhang, Ben Y. Zhao, and Haitao Zheng. 2015.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">Packet-Level Telemetry in Large Datacenter Networks.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" style="font-size:90%;">SIGCOMM Comput. Commun. Rev.</em><span class="ltx_text" style="font-size:90%;"> 45, 4 (Aug. 2015), 479–491.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
</span><a class="ltx_ref ltx_href" href="https://doi.org/10.1145/2829988.2787483" style="font-size:90%;" title="">doi:<span class="ltx_ref ltx_nolink">10.1145/2829988.2787483</span></a><span class="ltx_text" style="font-size:90%;">
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib109">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Li et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2016)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Yuliang Li, Rui Miao, Changhoon Kim, and Minlan Yu. 2016.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">LossRadar: Fast Detection of Lost Packets in Data Center Networks. In </span><em class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 12th International on Conference on Emerging Networking EXperiments and Technologies</em><span class="ltx_text" style="font-size:90%;"> (Irvine, California, USA) </span><em class="ltx_emph ltx_font_italic" style="font-size:90%;">(CoNEXT ’16)</em><span class="ltx_text" style="font-size:90%;">. Association for Computing Machinery, New York, NY, USA, 481–495.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
</span><a class="ltx_ref ltx_href" href="https://doi.org/10.1145/2999572.2999609" style="font-size:90%;" title="">doi:<span class="ltx_ref ltx_nolink">10.1145/2999572.2999609</span></a><span class="ltx_text" style="font-size:90%;">
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib110">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Tan et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2019)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Cheng Tan, Ze Jin, Chuanxiong Guo, Tianrong Zhang, Haitao Wu, Karl Deng, Dongming Bi, and Dong Xiang. 2019.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">NetBouncer: Active Device and Link Failure Localization in Data Center Networks. In </span><em class="ltx_emph ltx_font_italic" style="font-size:90%;">16th USENIX Symposium on Networked Systems Design and Implementation (NSDI 19)</em><span class="ltx_text" style="font-size:90%;">. USENIX Association, Boston, MA, 599–614.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.usenix.org/conference/nsdi19/presentation/tan" style="font-size:90%;" title="">https://www.usenix.org/conference/nsdi19/presentation/tan</a><span class="ltx_text" style="font-size:90%;">
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib111">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Liu et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2023)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Kefei Liu, Zhuo Jiang, Jiao Zhang, Haoran Wei, Xiaolong Zhong, Lizhuang Tan, Tian Pan, and Tao Huang. 2023.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">Hostping: Diagnosing Intra-host Network Bottlenecks in RDMA Servers. In </span><em class="ltx_emph ltx_font_italic" style="font-size:90%;">20th USENIX Symposium on Networked Systems Design and Implementation (NSDI 23)</em><span class="ltx_text" style="font-size:90%;">. USENIX Association, Boston, MA, 15–29.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.usenix.org/conference/nsdi23/presentation/liu-kefei" style="font-size:90%;" title="">https://www.usenix.org/conference/nsdi23/presentation/liu-kefei</a><span class="ltx_text" style="font-size:90%;">
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib112">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Mohan et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2021)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Jayashree Mohan, Amar Phanishayee, and Vijay Chidambaram. 2021.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">CheckFreq: Frequent, Fine-Grained DNN Checkpointing. In </span><em class="ltx_emph ltx_font_italic" style="font-size:90%;">19th USENIX Conference on File and Storage Technologies (FAST 21)</em><span class="ltx_text" style="font-size:90%;">. USENIX Association, 203–216.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.usenix.org/conference/fast21/presentation/mohan" style="font-size:90%;" title="">https://www.usenix.org/conference/fast21/presentation/mohan</a><span class="ltx_text" style="font-size:90%;">
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib113">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Eisenman et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2022)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Assaf Eisenman, Kiran Kumar Matam, Steven Ingram, Dheevatsa Mudigere, Raghuraman Krishnamoorthi, Krishnakumar Nair, Misha Smelyanskiy, and Murali Annavaram. 2022.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">Check-N-Run: a Checkpointing System for Training Deep Learning Recommendation Models. In </span><em class="ltx_emph ltx_font_italic" style="font-size:90%;">19th USENIX Symposium on Networked Systems Design and Implementation (NSDI 22)</em><span class="ltx_text" style="font-size:90%;">. USENIX Association, Renton, WA, 929–943.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.usenix.org/conference/nsdi22/presentation/eisenman" style="font-size:90%;" title="">https://www.usenix.org/conference/nsdi22/presentation/eisenman</a><span class="ltx_text" style="font-size:90%;">
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib114">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" style="font-size:90%;">Wang et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;"> (2023)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Zhuang Wang, Zhen Jia, Shuai Zheng, Zhen Zhang, Xinwei Fu, T. S. Eugene Ng, and Yida Wang. 2023.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">GEMINI: Fast Failure Recovery in Distributed Training with In-Memory Checkpoints. In </span><em class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 29th Symposium on Operating Systems Principles</em><span class="ltx_text" style="font-size:90%;"> (Koblenz, Germany) </span><em class="ltx_emph ltx_font_italic" style="font-size:90%;">(SOSP ’23)</em><span class="ltx_text" style="font-size:90%;">. Association for Computing Machinery, New York, NY, USA, 364–381.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
</span><a class="ltx_ref ltx_href" href="https://doi.org/10.1145/3600006.3613145" style="font-size:90%;" title="">doi:<span class="ltx_ref ltx_nolink">10.1145/3600006.3613145</span></a><span class="ltx_text" style="font-size:90%;">
</span>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix" style="font-size:90%;">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Experiment Details</h2>
<section class="ltx_subsection" id="A1.SS1">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">A.1. </span>Response Length Distribution of Each Model</h3>
<div class="ltx_para" id="A1.SS1.p1">
<p class="ltx_p"><span class="ltx_text" style="font-size:90%;">The response length distributions of each model used in the throughput experiments are listed in Figure </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#A1.F19.sf1" style="font-size:90%;" title="In A.1. Response Length Distribution of Each Model ‣ Appendix A Experiment Details ‣ 10. Conclusion ‣ 9. Related Work ‣ 8.5. Fault Tolerance Analysis ‣ 8.4. Repack Efficiency ‣ 8.3. Weight Synchronization Overhead ‣ 13(c) ‣ 8.2. Training Convergence ‣ 8. Evaluation ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_tag">19(a)</span></a><span class="ltx_text" style="font-size:90%;">.</span></p>
</div>
<figure class="ltx_figure" id="A1.SS1.fig4">
<div class="ltx_block">
<figure class="ltx_figure" id="A1.F19.sf1">
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">(a) </span>The response length distributions on the DAPO-Math-17k dataset of the model checkpoints used in the throughput experiments.
</figcaption>
</figure>
<section class="ltx_subsection" id="A1.SS2">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">A.2. </span>Hyperparameter Details</h3>
<figure class="ltx_table" id="A1.T2">
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 2. </span>GPU allocation of the systems in the throughput experiments. For verl, we adopt the colocated allocation which uses all the GPUs for train and rollout alternately.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_tt" rowspan="2"><span class="ltx_text ltx_font_bold" style="font-size:90%;">System</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3"><span class="ltx_text ltx_font_bold" style="font-size:90%;">7B</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3"><span class="ltx_text ltx_font_bold" style="font-size:90%;">32B</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3"><span class="ltx_text ltx_font_bold" style="font-size:90%;">72B</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text ltx_font_bold" style="font-size:90%;">Total</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text ltx_font_bold" style="font-size:90%;">Train</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text ltx_font_bold" style="font-size:90%;">Rollout</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text ltx_font_bold" style="font-size:90%;">Total</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text ltx_font_bold" style="font-size:90%;">Train</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text ltx_font_bold" style="font-size:90%;">Rollout</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text ltx_font_bold" style="font-size:90%;">Total</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text ltx_font_bold" style="font-size:90%;">Train</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text ltx_font_bold" style="font-size:90%;">Rollout</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_t" rowspan="5"><span class="ltx_text" style="font-size:90%;">
<span class="ltx_tabular ltx_align_middle">
<span class="ltx_tr">
<span class="ltx_td ltx_nopad_r ltx_align_center">verl</span></span>
</span> </span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text" style="font-size:90%;">16</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="2" rowspan="5"><span class="ltx_text" style="font-size:90%;">Colocated</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text" style="font-size:90%;">32</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="2" rowspan="5"><span class="ltx_text" style="font-size:90%;">Colocated</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text" style="font-size:90%;">64</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="2" rowspan="5"><span class="ltx_text" style="font-size:90%;">Colocated</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">32</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">64</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">128</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">64</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">128</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">256</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">128</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">256</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">512</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">256</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">512</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">1024</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_t" rowspan="5"><span class="ltx_text" style="font-size:90%;">
<span class="ltx_tabular ltx_align_middle">
<span class="ltx_tr">
<span class="ltx_td ltx_nopad_r ltx_align_center">One-step</span></span>
<span class="ltx_tr">
<span class="ltx_td ltx_nopad_r ltx_align_center">Staleness</span></span>
</span> </span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text" style="font-size:90%;">16</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text" style="font-size:90%;">8</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text" style="font-size:90%;">8</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text" style="font-size:90%;">32</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text" style="font-size:90%;">16</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text" style="font-size:90%;">16</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text" style="font-size:90%;">64</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text" style="font-size:90%;">32</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text" style="font-size:90%;">32</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">32</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">8</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">24</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">64</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">32</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">32</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">128</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">64</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">64</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">64</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">16</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">48</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">128</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">48</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">80</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">256</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">96</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">160</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">128</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">32</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">96</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">256</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">64</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">192</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">512</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">192</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">320</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">256</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">40</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">216</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">512</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">80</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">432</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">1024</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">256</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">768</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_t" rowspan="5"><span class="ltx_text" style="font-size:90%;">
<span class="ltx_tabular ltx_align_middle">
<span class="ltx_tr">
<span class="ltx_td ltx_nopad_r ltx_align_center">Stream</span></span>
<span class="ltx_tr">
<span class="ltx_td ltx_nopad_r ltx_align_center">Generation</span></span>
</span> </span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text" style="font-size:90%;">16</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text" style="font-size:90%;">8</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text" style="font-size:90%;">8</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text" style="font-size:90%;">32</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text" style="font-size:90%;">16</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text" style="font-size:90%;">16</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text" style="font-size:90%;">64</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text" style="font-size:90%;">32</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text" style="font-size:90%;">32</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">32</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">8</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">24</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">64</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">32</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">32</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">128</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">64</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">64</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">64</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">16</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">48</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">128</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">48</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">80</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">256</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">96</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">160</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">128</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">32</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">96</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">256</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">64</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">192</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">512</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">192</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">320</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">256</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">40</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">216</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">512</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">80</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">432</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">1024</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">256</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">768</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_t" rowspan="5"><span class="ltx_text" style="font-size:90%;">
<span class="ltx_tabular ltx_align_middle">
<span class="ltx_tr">
<span class="ltx_td ltx_nopad_r ltx_align_center">AReaL</span></span>
</span> </span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text" style="font-size:90%;">16</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text" style="font-size:90%;">8</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text" style="font-size:90%;">8</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text" style="font-size:90%;">32</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text" style="font-size:90%;">16</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text" style="font-size:90%;">16</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text" style="font-size:90%;">64</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text" style="font-size:90%;">32</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text" style="font-size:90%;">32</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">32</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">16</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">16</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">64</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">32</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">32</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">128</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">64</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">64</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">64</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">32</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">32</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">128</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">64</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">64</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">256</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">128</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">128</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">128</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">64</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">64</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">256</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">128</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">128</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">512</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">320</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">192</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">256</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">128</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">128</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">512</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">256</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">256</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">1024</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">640</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">384</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" rowspan="5"><span class="ltx_text" style="font-size:90%;">
<span class="ltx_tabular ltx_align_middle">
<span class="ltx_tr">
<span class="ltx_td ltx_nopad_r ltx_align_center">Laminar</span></span>
</span> </span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text" style="font-size:90%;">16</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text" style="font-size:90%;">8</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text" style="font-size:90%;">8</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text" style="font-size:90%;">32</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text" style="font-size:90%;">16</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text" style="font-size:90%;">16</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text" style="font-size:90%;">64</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text" style="font-size:90%;">32</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text" style="font-size:90%;">32</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">32</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">24</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">8</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">64</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">32</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">32</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">128</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">64</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">64</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">64</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">40</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">24</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">128</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">64</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">64</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">256</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">128</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">128</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">128</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">80</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">48</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">256</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">128</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">128</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">512</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">320</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text" style="font-size:90%;">192</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_right ltx_border_bb"><span class="ltx_text" style="font-size:90%;">256</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb"><span class="ltx_text" style="font-size:90%;">192</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb"><span class="ltx_text" style="font-size:90%;">64</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb"><span class="ltx_text" style="font-size:90%;">512</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb"><span class="ltx_text" style="font-size:90%;">256</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb"><span class="ltx_text" style="font-size:90%;">256</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb"><span class="ltx_text" style="font-size:90%;">1024</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb"><span class="ltx_text" style="font-size:90%;">768</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb"><span class="ltx_text" style="font-size:90%;">256</span></td>
</tr>
</table>
</figure>
<div class="ltx_para ltx_noindent" id="A1.SS2.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold" style="font-size:90%;">Throughput experiments.</span><span class="ltx_text" style="font-size:90%;">
We evaluate system throughput on two tasks: math and tool-calling. For the math task, we use three intermediate checkpoints of different scales during the RL training from </span><span class="ltx_text ltx_font_typewriter" style="font-size:90%;">Qwen2.5-Math-7B</span><span class="ltx_text" style="font-size:90%;">, </span><span class="ltx_text ltx_font_typewriter" style="font-size:90%;">Qwen2.5-32B</span><span class="ltx_text" style="font-size:90%;">, and </span><span class="ltx_text ltx_font_typewriter" style="font-size:90%;">Qwen2.5-Math-72B</span><span class="ltx_text" style="font-size:90%;">, respectively. For the tool-calling task, we use the 7B checkpoint for the RL training in the open-source implementation of ReTool. Table </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#A1.T2" style="font-size:90%;" title="Table 2 ‣ A.2. Hyperparameter Details ‣ A.1. Response Length Distribution of Each Model ‣ Appendix A Experiment Details ‣ 10. Conclusion ‣ 9. Related Work ‣ 8.5. Fault Tolerance Analysis ‣ 8.4. Repack Efficiency ‣ 8.3. Weight Synchronization OverheadIn 8.2. Training Convergence ‣ 8. Evaluation ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_tag">2</span></a><span class="ltx_text" style="font-size:90%;"> details the GPU allocation for each experiment, which we tuned to maximize throughput in our environment.</span></p>
</div>
<div class="ltx_para" id="A1.SS2.p2">
<p class="ltx_p"><span class="ltx_text" style="font-size:90%;">During rollout, we use temperature sampling with </span><math alttext="t=1" class="ltx_Math" display="inline" id="A1.SS2.p2.m1" intent=":literal"><semantics><mrow><mi mathsize="0.900em">t</mi><mo mathsize="0.900em">=</mo><mn mathsize="0.900em">1</mn></mrow><annotation encoding="application/x-tex">t=1</annotation></semantics></math><span class="ltx_text" style="font-size:90%;">. We do not use top-</span><math alttext="P" class="ltx_Math" display="inline" id="A1.SS2.p2.m2" intent=":literal"><semantics><mi mathsize="0.900em">P</mi><annotation encoding="application/x-tex">P</annotation></semantics></math><span class="ltx_text" style="font-size:90%;"> or top-</span><math alttext="k" class="ltx_Math" display="inline" id="A1.SS2.p2.m3" intent=":literal"><semantics><mi mathsize="0.900em">k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math><span class="ltx_text" style="font-size:90%;"> sampling. For verl, One-step Staleness, and Stream Generation, the global rollout batch size is 8192, matching the training batch size. For AReaL and Laminar, we set the maximum concurrency to 1024 trajectories per rollout replica.
The rollout tensor parallelism (TP) sizes are set to 4 and 8 for the 32B and 72B models, respectively. For the 7B model, the size varies according to the system’s throughput characteristics. In AReaL and Laminar, we set the TP size to 1 to maximize throughput. In verl, One-step Staleness, and Stream Generation, we set the TP size to 2, which balances generation throughput against latency, mitigating the long-tail bottlenecks.</span></p>
</div>
<div class="ltx_para" id="A1.SS2.p3">
<p class="ltx_p"><span class="ltx_text" style="font-size:90%;">For AReaL, we use Megatron-LM’s hybrid of data (DP), tensor (TP), and pipeline parallelism (PP). The TP and PP sizes are set to 2 and 1 for the 7B model, 4 and 2 for the 32B model, and 4 and 4 for the 72B model. The DP size is adaptively set to </span><math alttext="\frac{\text{\# of Train GPUs}}{\text{TP Size}\times\text{PP Size}}" class="ltx_Math" display="inline" id="A1.SS2.p3.m1" intent=":literal"><semantics><mfrac><mtext mathsize="0.900em"># of Train GPUs</mtext><mrow><mtext mathsize="0.900em">TP Size</mtext><mo lspace="0.222em" mathsize="0.900em" rspace="0.222em">×</mo><mtext mathsize="0.900em">PP Size</mtext></mrow></mfrac><annotation encoding="application/x-tex">\frac{\text{\# of Train GPUs}}{\text{TP Size}\times\text{PP Size}}</annotation></semantics></math><span class="ltx_text" style="font-size:90%;">. For other systems, we combine Torch DDP, FSDP, and Ulysses Sequence Parallelism (SP). The FSDP and SP sizes are set to 8 and 4 for the 7B model, 16 and 8 for the 32B model, and 32 and 8 for the 72B model. The DDP size is set to </span><math alttext="\frac{\text{\# of Train GPUs}}{\text{FSDP Size}}" class="ltx_Math" display="inline" id="A1.SS2.p3.m2" intent=":literal"><semantics><mfrac><mtext mathsize="0.900em"># of Train GPUs</mtext><mtext mathsize="0.900em">FSDP Size</mtext></mfrac><annotation encoding="application/x-tex">\frac{\text{\# of Train GPUs}}{\text{FSDP Size}}</annotation></semantics></math><span class="ltx_text" style="font-size:90%;">.</span></p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS2.p4">
<p class="ltx_p"><span class="ltx_text ltx_font_bold" style="font-size:90%;">Convergence experiments.</span><span class="ltx_text" style="font-size:90%;">
We conduct convergence experiments using two base models: </span><span class="ltx_text ltx_font_typewriter" style="font-size:90%;">Qwen2.5-Math-7B</span><span class="ltx_text" style="font-size:90%;"> and </span><span class="ltx_text ltx_font_typewriter" style="font-size:90%;">Qwen2.5-32B</span><span class="ltx_text" style="font-size:90%;">. The GPU allocation is identical to that of the throughput experiments. Table </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#A1.T3" style="font-size:90%;" title="Table 3 ‣ A.2. Hyperparameter Details ‣ A.1. Response Length Distribution of Each Model ‣ Appendix A Experiment Details ‣ 10. Conclusion ‣ 9. Related Work ‣ 8.5. Fault Tolerance Analysis ‣ 8.4. Repack Efficiency ‣ 8.3. Weight Synchronization OverheadIn 8.2. Training Convergence ‣ 8. Evaluation ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_tag">3</span></a><span class="ltx_text" style="font-size:90%;"> lists the other hyperparameters. For AReaL and Laminar, we reduce the maximum per-rollout concurrency to 256.
We also adopt the default FIFO sampling strategy for these two systems.</span></p>
</div>
<figure class="ltx_table" id="A1.T3">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 3. </span>Hyperparameters for convergence experiments. Configurations are based on the original AReaL paper and DAPO. For asynchronous systems (One-step Staleness, Stream Generation, and Laminar), we increase the mini-batch size to 2048 to stabilize training with off-policy data, aligning with AReaL’s configuration.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_tt"><span class="ltx_text ltx_font_bold" style="font-size:90%;">System</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt"><span class="ltx_text ltx_font_bold" style="font-size:90%;">verl</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt">
<span class="ltx_text" style="font-size:90%;"> </span><span class="ltx_text" style="font-size:90%;">
<span class="ltx_tabular ltx_align_middle">
<span class="ltx_tr">
<span class="ltx_td ltx_nopad_r ltx_align_center"><span class="ltx_text ltx_font_bold">One-step</span></span></span>
<span class="ltx_tr">
<span class="ltx_td ltx_nopad_r ltx_align_center"><span class="ltx_text ltx_font_bold">Staleness</span></span></span>
</span></span><span class="ltx_text" style="font-size:90%;"></span>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt">
<span class="ltx_text" style="font-size:90%;"> </span><span class="ltx_text" style="font-size:90%;">
<span class="ltx_tabular ltx_align_middle">
<span class="ltx_tr">
<span class="ltx_td ltx_nopad_r ltx_align_center"><span class="ltx_text ltx_font_bold">Stream</span></span></span>
<span class="ltx_tr">
<span class="ltx_td ltx_nopad_r ltx_align_center"><span class="ltx_text ltx_font_bold">Generation</span></span></span>
</span></span><span class="ltx_text" style="font-size:90%;"></span>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt"><span class="ltx_text ltx_font_bold" style="font-size:90%;">AReaL</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt"><span class="ltx_text ltx_font_bold" style="font-size:90%;">Laminar</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_t" rowspan="2"><span class="ltx_text" style="font-size:90%;">Algorithm</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" rowspan="2"><span class="ltx_text" style="font-size:90%;">GRPO</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" rowspan="2"><span class="ltx_text" style="font-size:90%;">GRPO</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" rowspan="2"><span class="ltx_text" style="font-size:90%;">GRPO</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">Decoupled</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" rowspan="2"><span class="ltx_text" style="font-size:90%;">GRPO</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">PPO</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left"><span class="ltx_text" style="font-size:90%;">Learning Rate</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">1e-6</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">1e-6</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">1e-6</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">2e-5</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">1e-6</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left"><span class="ltx_text" style="font-size:90%;">Weight Decay</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">0.1</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">0.1</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">0.1</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">0.05</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">0.1</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">
<span class="ltx_text" style="font-size:90%;">Clip </span><math alttext="\varepsilon_{\text{high}}" class="ltx_Math" display="inline" id="A1.T3.m1" intent=":literal"><semantics><msub><mi mathsize="0.900em">ε</mi><mtext mathsize="0.900em">high</mtext></msub><annotation encoding="application/x-tex">\varepsilon_{\text{high}}</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">0.28</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">0.28</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">0.28</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">0.2</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">0.28</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">
<span class="ltx_text" style="font-size:90%;">Clip </span><math alttext="\varepsilon_{\text{low}}" class="ltx_Math" display="inline" id="A1.T3.m2" intent=":literal"><semantics><msub><mi mathsize="0.900em">ε</mi><mtext mathsize="0.900em">low</mtext></msub><annotation encoding="application/x-tex">\varepsilon_{\text{low}}</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">0.2</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">0.2</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">0.2</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">0.2</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">0.2</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">
<span class="ltx_text" style="font-size:90%;">Discount </span><math alttext="\gamma" class="ltx_Math" display="inline" id="A1.T3.m3" intent=":literal"><semantics><mi mathsize="0.900em">γ</mi><annotation encoding="application/x-tex">\gamma</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">1.0</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">1.0</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">1.0</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">1.0</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">1.0</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">
<span class="ltx_text" style="font-size:90%;">GAE </span><math alttext="\lambda" class="ltx_Math" display="inline" id="A1.T3.m4" intent=":literal"><semantics><mi mathsize="0.900em">λ</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">1.0</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">1.0</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">1.0</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">1.0</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">1.0</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left"><span class="ltx_text" style="font-size:90%;">Group Size</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">16</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">16</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">16</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">16</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">16</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left"><span class="ltx_text" style="font-size:90%;">Training Global Batch Size</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">8192</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">8192</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">8192</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">8192</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">8192</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left"><span class="ltx_text" style="font-size:90%;">Training Mini-Batch Size</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">512</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">2048</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">2048</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">2048</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">2048</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left"><span class="ltx_text" style="font-size:90%;">Per-rollout Max Concurrency</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">N/A</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">N/A</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">N/A</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">256</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">256</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left"><span class="ltx_text" style="font-size:90%;">Sampling Strategy</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">N/A</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">N/A</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">N/A</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">FIFO</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">FIFO</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_bb"><span class="ltx_text" style="font-size:90%;">Max Staleness Bound</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text" style="font-size:90%;">0</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text" style="font-size:90%;">1</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text" style="font-size:90%;">1</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text" style="font-size:90%;">4</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text" style="font-size:90%;">4 (observed)</span></td>
</tr>
</table>
</figure>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix" style="font-size:90%;">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Detailed Experiment Analysis</h2>
<div class="ltx_para ltx_noindent" id="A2.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold" style="font-size:90%;">Performance in small cluster scales.</span><span class="ltx_text" style="font-size:90%;">
In relatively small cluster scales, Laminar provides a moderate but consistent 1.41</span><math alttext="\times" class="ltx_Math" display="inline" id="A2.p1.m1" intent=":literal"><semantics><mo mathsize="0.900em">×</mo><annotation encoding="application/x-tex">\times</annotation></semantics></math><span class="ltx_text" style="font-size:90%;"> speedup on average across all the baselines and model sizes on at most 64 GPUs, as shown in Figure </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#S8.F13.sf1" style="font-size:90%;" title="In 8. Evaluation ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_tag">13(a)</span></a><span class="ltx_text" style="font-size:90%;">.
The performance in these configurations is constrained by the fact that both training and rollout of LLMs require a number of GPUs to accommodate the model via parallelism. This severely limits the possible resource allocation choices, forcing different systems with similar placements, as shown in Table </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#A1.T2" style="font-size:90%;" title="Table 2 ‣ A.2. Hyperparameter Details ‣ A.1. Response Length Distribution of Each Model ‣ Appendix A Experiment Details ‣ 10. Conclusion ‣ 9. Related Work ‣ 8.5. Fault Tolerance Analysis ‣ 8.4. Repack Efficiency ‣ 8.3. Weight Synchronization OverheadIn 8.2. Training Convergence ‣ 8. Evaluation ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_tag">2</span></a><span class="ltx_text" style="font-size:90%;">. Furthermore, these placements are usually suboptimal since the training and rollout throughput can not be effectively balanced. Within such limited configurations, Laminar improves the throughput mainly by eliminating long-tail bubbles.</span></p>
</div>
<div class="ltx_para ltx_noindent" id="A2.p2">
<p class="ltx_p"><span class="ltx_text ltx_font_bold" style="font-size:90%;">Performance in large-scale clusters.</span><span class="ltx_text" style="font-size:90%;">
The performance advantage of Laminar becomes much more pronounced in large-scale clusters, achieving an average speedup of 3.34</span><math alttext="\times" class="ltx_Math" display="inline" id="A2.p2.m1" intent=":literal"><semantics><mo mathsize="0.900em">×</mo><annotation encoding="application/x-tex">\times</annotation></semantics></math><span class="ltx_text" style="font-size:90%;"> at the largest cluster scales across all the baselines and model sizes, as is shown in Figure </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#S8.F13.sf1" style="font-size:90%;" title="In 8. Evaluation ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_tag">13(a)</span></a><span class="ltx_text" style="font-size:90%;">.
Systems like verl, one-step staleness, and stream generation are heavily bottlenecked by the global weight synchronization.
With the end-to-end latency of the generation stage bound by the long-tail trajectories, adding more rollout resources provides marginal returns on throughput.
AReaL mitigates this bottleneck by applying partial rollout, but its scalability is then limited by the KVCache recomputation overhead. As the cluster size scales up, this overhead worsens as the number of trajectories and the model update frequency increase, capping the system’s performance.
In contrast, Laminar effectively resolves the global weight synchronization constraint and eliminates the long-tail bubbles while introducing minimal overhead through the relay design and repack mechanism. Furthermore, as the cluster size scales up, the repacking becomes increasingly effective with more rollout replicas to manage.</span></p>
</div>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_title_appendix" style="font-size:90%;">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Discussion</h2>
<div class="ltx_para ltx_noindent" id="A3.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold" style="font-size:90%;">Partial rollout.</span><span class="ltx_text" style="font-size:90%;">
Laminar’s trajectory-level asynchrony effectively mitigates long-tail latency bubbles by decoupling trajectory generation from global synchronization.
Recently, an orthogonal approach has emerged that
updates weights mid-generation to reduce these bubbles
known as partial rollout systems </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" style="font-size:90%;">(</span><span class="ltx_text" style="font-size:90%;">Team et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib9" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>; <span class="ltx_text" style="font-size:90%;">Seed et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib8" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>; <span class="ltx_text" style="font-size:90%;">Fu et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib19" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>; <span class="ltx_text" style="font-size:90%;">Wu et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib20" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a><span class="ltx_text" style="font-size:90%;">)</span></cite><span class="ltx_text" style="font-size:90%;">.
These systems interrupt all ongoing generation and update rollout replicas once the latest actor weights become available.
The aborted trajectories then continue generating with the new weights.
This technique can be integrated with synchronous, k-step staleness, and Laminar to further reduce idle time.
However, it introduces a challenge to training stability.
Updating weights amid generation
creates mixed-version trajectories, where a single long trajectory is produced using several successive weight versions. The expected number of versions grows linearly with sequence length and the actor-update frequency. These mixed-version trajectories distort the natural output-length distribution. They can also hinder convergence in valueless RL algorithms, which rely on forming trajectory groups from a consistent weight version. As update frequency rises, the contamination worsens, creating an undesirable coupling between system throughput and training stability. As shown in Figure </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#S8.F13.sf2.fig1" style="font-size:90%;" title="In 8. Evaluation ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_tag">13(b)</span></a><span class="ltx_text" style="font-size:90%;">, applying partial rollout can converge at a slower speed,
requiring more algorithmic research to stabilize such training.</span></p>
</div>
<div class="ltx_para ltx_noindent" id="A3.p2">
<p class="ltx_p"><span class="ltx_text ltx_font_bold" style="font-size:90%;">Effective experiences sampling.</span><span class="ltx_text" style="font-size:90%;">
In Laminar, as scaling out RL tasks,
the scaling of trajectory generation allows rollout throughput to outpace the trainer’s consumption speed (analyzed in §</span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#S8.SS1" style="font-size:90%;" title="8.1. End-to-End Training Throughput ‣ 8. Evaluation ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_tag">8.1</span></a><span class="ltx_text" style="font-size:90%;">).
This creates an abundance of diverse experiences, but presents a key challenge: </span><span class="ltx_text ltx_font_italic" style="font-size:90%;">how to utilize these experiences effectively</span><span class="ltx_text" style="font-size:90%;">.
Experiences sampling is a classic problem
in traditional RL.
For instance, OpenAI’s work on Dota 2 highlighted the negative impact of data staleness on training speed </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" style="font-size:90%;">(</span><span class="ltx_text" style="font-size:90%;">Berner et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib25" title=""><span class="ltx_text" style="font-size:90%;">2019</span></a><span class="ltx_text" style="font-size:90%;">)</span></cite><span class="ltx_text" style="font-size:90%;">.
Recent research advocates for priority-based sampling </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" style="font-size:90%;">(</span><span class="ltx_text" style="font-size:90%;">Wang et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib11" title=""><span class="ltx_text" style="font-size:90%;">2025</span></a>; <span class="ltx_text" style="font-size:90%;">Espeholt et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib92" title=""><span class="ltx_text" style="font-size:90%;">2018</span></a>; <span class="ltx_text" style="font-size:90%;">Schaul et al</span><span class="ltx_text" style="font-size:90%;">.</span><span class="ltx_text" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#bib.bib75" title=""><span class="ltx_text" style="font-size:90%;">2016</span></a><span class="ltx_text" style="font-size:90%;">)</span></cite><span class="ltx_text" style="font-size:90%;">, which considers metrics like TD errors, importance sampling, and entropy to
prioritize the most informative transitions.</span></p>
</div>
<div class="ltx_para" id="A3.p3">
<p class="ltx_p"><span class="ltx_text" style="font-size:90%;">However,
experience sampling remains an underexplored area in large-scale LLM post-training.
The ability of Laminar to efficiently generate massive experiences is a key strength, but this potential can only be fully realized with an effective sampling mechanism,
which is crucial for translating
diverse experiences
and avoid learning on low-utility experiences.
Thus, developing effective sampling strategies for utilizing generated experiences represents a
promising area for future work in large-scale RL systems.</span></p>
</div>
</section>
<section class="ltx_appendix" id="A4">
<h2 class="ltx_title ltx_title_appendix" style="font-size:90%;">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Theoretical Analysis of Chain-Based Pipelined Broadcast</h2>
<div class="ltx_para" id="A4.p1">
<p class="ltx_p"><span class="ltx_text" style="font-size:90%;">We provide a formal analysis of the latency for a chain-based pipelined broadcast. The goal is to show that for large messages, such as LLM weights, the total broadcast time is dominated by a term independent of the number of nodes, making the approach highly scalable.</span></p>
</div>
<section class="ltx_subsection" id="A4.SS1">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">D.1. </span>Communication Model</h3>
<div class="ltx_para" id="A4.SS1.p1">
<p class="ltx_p"><span class="ltx_text" style="font-size:90%;">We model the broadcast as a linear pipeline, where a master relay sends a message to </span><math alttext="p-1" class="ltx_Math" display="inline" id="A4.SS1.p1.m1" intent=":literal"><semantics><mrow><mi mathsize="0.900em">p</mi><mo mathsize="0.900em">−</mo><mn mathsize="0.900em">1</mn></mrow><annotation encoding="application/x-tex">p-1</annotation></semantics></math><span class="ltx_text" style="font-size:90%;"> other relays organized in a logical chain. Let the total number of nodes (master + relays) be </span><math alttext="p" class="ltx_Math" display="inline" id="A4.SS1.p1.m2" intent=":literal"><semantics><mi mathsize="0.900em">p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math><span class="ltx_text" style="font-size:90%;">. The communication cost between any two adjacent nodes is defined by two parameters:</span></p>
</div>
<div class="ltx_para ltx_noindent" id="A4.SS1.p2">
<p class="ltx_p"><math alttext="\bullet" class="ltx_Math" display="inline" id="A4.SS1.p2.m1" intent=":literal"><semantics><mo mathsize="0.900em">∙</mo><annotation encoding="application/x-tex">\bullet</annotation></semantics></math><span class="ltx_text" style="font-size:90%;">
</span><math alttext="T_{\text{start}}" class="ltx_Math" display="inline" id="A4.SS1.p2.m2" intent=":literal"><semantics><msub><mi mathsize="0.900em">T</mi><mtext mathsize="0.900em">start</mtext></msub><annotation encoding="application/x-tex">T_{\text{start}}</annotation></semantics></math><span class="ltx_text" style="font-size:90%;">: The startup latency for sending a message, independent of its size. This includes costs like connection setup and initial processing.</span></p>
</div>
<div class="ltx_para ltx_noindent" id="A4.SS1.p3">
<p class="ltx_p"><math alttext="\bullet" class="ltx_Math" display="inline" id="A4.SS1.p3.m1" intent=":literal"><semantics><mo mathsize="0.900em">∙</mo><annotation encoding="application/x-tex">\bullet</annotation></semantics></math><span class="ltx_text" style="font-size:90%;">
</span><math alttext="T_{\text{byte}}" class="ltx_Math" display="inline" id="A4.SS1.p3.m2" intent=":literal"><semantics><msub><mi mathsize="0.900em">T</mi><mtext mathsize="0.900em">byte</mtext></msub><annotation encoding="application/x-tex">T_{\text{byte}}</annotation></semantics></math><span class="ltx_text" style="font-size:90%;">: The per-byte transmission time, determined by the network bandwidth (</span><math alttext="T_{\text{byte}}=1/\text{Bandwidth}" class="ltx_Math" display="inline" id="A4.SS1.p3.m3" intent=":literal"><semantics><mrow><msub><mi mathsize="0.900em">T</mi><mtext mathsize="0.900em">byte</mtext></msub><mo mathsize="0.900em">=</mo><mrow><mn mathsize="0.900em">1</mn><mo maxsize="0.900em" minsize="0.900em" stretchy="true" symmetric="true">/</mo><mtext mathsize="0.900em">Bandwidth</mtext></mrow></mrow><annotation encoding="application/x-tex">T_{\text{byte}}=1/\text{Bandwidth}</annotation></semantics></math><span class="ltx_text" style="font-size:90%;">).
The time </span><math alttext="t" class="ltx_Math" display="inline" id="A4.SS1.p3.m4" intent=":literal"><semantics><mi mathsize="0.900em">t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math><span class="ltx_text" style="font-size:90%;"> to transmit a single message of size </span><math alttext="s" class="ltx_Math" display="inline" id="A4.SS1.p3.m5" intent=":literal"><semantics><mi mathsize="0.900em">s</mi><annotation encoding="application/x-tex">s</annotation></semantics></math><span class="ltx_text" style="font-size:90%;"> between two nodes is </span><math alttext="t=s\cdot T_{\text{byte}}+T_{\text{start}}" class="ltx_Math" display="inline" id="A4.SS1.p3.m6" intent=":literal"><semantics><mrow><mi mathsize="0.900em">t</mi><mo mathsize="0.900em">=</mo><mrow><mrow><mi mathsize="0.900em">s</mi><mo lspace="0.222em" mathsize="0.900em" rspace="0.222em">⋅</mo><msub><mi mathsize="0.900em">T</mi><mtext mathsize="0.900em">byte</mtext></msub></mrow><mo mathsize="0.900em">+</mo><msub><mi mathsize="0.900em">T</mi><mtext mathsize="0.900em">start</mtext></msub></mrow></mrow><annotation encoding="application/x-tex">t=s\cdot T_{\text{byte}}+T_{\text{start}}</annotation></semantics></math><span class="ltx_text" style="font-size:90%;">.</span></p>
</div>
<div class="ltx_para" id="A4.SS1.p4">
<p class="ltx_p"><span class="ltx_text" style="font-size:90%;">In our pipelined approach, the total model weights, of size </span><math alttext="M" class="ltx_Math" display="inline" id="A4.SS1.p4.m1" intent=":literal"><semantics><mi mathsize="0.900em">M</mi><annotation encoding="application/x-tex">M</annotation></semantics></math><span class="ltx_text" style="font-size:90%;"> bytes, are divided into </span><math alttext="k" class="ltx_Math" display="inline" id="A4.SS1.p4.m2" intent=":literal"><semantics><mi mathsize="0.900em">k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math><span class="ltx_text" style="font-size:90%;"> smaller chunks, each of size </span><math alttext="M/k" class="ltx_Math" display="inline" id="A4.SS1.p4.m3" intent=":literal"><semantics><mrow><mi mathsize="0.900em">M</mi><mo maxsize="0.900em" minsize="0.900em" stretchy="true" symmetric="true">/</mo><mi mathsize="0.900em">k</mi></mrow><annotation encoding="application/x-tex">M/k</annotation></semantics></math><span class="ltx_text" style="font-size:90%;">.</span></p>
</div>
<figure class="ltx_figure" id="A1.F19"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="252" id="A1.F19.g1" src="x14.png" width="830"/>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">Figure 19. </span>Relay broadcast latency with testbed in §<a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#S8" title="8. Evaluation ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_tag">8</span></a>.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="A4.SS2">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">D.2. </span>Latency Derivation</h3>
<div class="ltx_para" id="A4.SS2.p1">
<p class="ltx_p"><span class="ltx_text" style="font-size:90%;">The total time for the broadcast, </span><math alttext="T(p,k)" class="ltx_Math" display="inline" id="A4.SS2.p1.m1" intent=":literal"><semantics><mrow><mi mathsize="0.900em">T</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="0.900em" minsize="0.900em">(</mo><mi mathsize="0.900em">p</mi><mo mathsize="0.900em">,</mo><mi mathsize="0.900em">k</mi><mo maxsize="0.900em" minsize="0.900em">)</mo></mrow></mrow><annotation encoding="application/x-tex">T(p,k)</annotation></semantics></math><span class="ltx_text" style="font-size:90%;">, is the time from when the master sends the first chunk until the last relay receives the last chunk. This can be modeled as the sum of two components:</span></p>
<ol class="ltx_enumerate" id="A4.I1">
<li class="ltx_item" id="A4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(1)</span>
<div class="ltx_para" id="A4.I1.i1.p1">
<p class="ltx_p"><span class="ltx_text" style="font-size:90%;">The time for the first chunk to travel down the entire chain of </span><math alttext="p-1" class="ltx_Math" display="inline" id="A4.I1.i1.p1.m1" intent=":literal"><semantics><mrow><mi mathsize="0.900em">p</mi><mo mathsize="0.900em">−</mo><mn mathsize="0.900em">1</mn></mrow><annotation encoding="application/x-tex">p-1</annotation></semantics></math><span class="ltx_text" style="font-size:90%;"> hops.</span></p>
</div>
</li>
<li class="ltx_item" id="A4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(2)</span>
<div class="ltx_para" id="A4.I1.i2.p1">
<p class="ltx_p"><span class="ltx_text" style="font-size:90%;">The time for the remaining </span><math alttext="k-1" class="ltx_Math" display="inline" id="A4.I1.i2.p1.m1" intent=":literal"><semantics><mrow><mi mathsize="0.900em">k</mi><mo mathsize="0.900em">−</mo><mn mathsize="0.900em">1</mn></mrow><annotation encoding="application/x-tex">k-1</annotation></semantics></math><span class="ltx_text" style="font-size:90%;"> chunks to be received by the final relay after it has received the first one.</span></p>
</div>
</li>
</ol>
<p class="ltx_p"><span class="ltx_text" style="font-size:90%;">The time to transmit one chunk between two nodes is </span><math alttext="t_{\text{chunk}}=\frac{M}{k}T_{\text{byte}}+T_{\text{start}}" class="ltx_Math" display="inline" id="A4.SS2.p1.m2" intent=":literal"><semantics><mrow><msub><mi mathsize="0.900em">t</mi><mtext mathsize="0.900em">chunk</mtext></msub><mo mathsize="0.900em">=</mo><mrow><mrow><mfrac><mi mathsize="0.900em">M</mi><mi mathsize="0.900em">k</mi></mfrac><mo lspace="0em" rspace="0em">​</mo><msub><mi mathsize="0.900em">T</mi><mtext mathsize="0.900em">byte</mtext></msub></mrow><mo mathsize="0.900em">+</mo><msub><mi mathsize="0.900em">T</mi><mtext mathsize="0.900em">start</mtext></msub></mrow></mrow><annotation encoding="application/x-tex">t_{\text{chunk}}=\frac{M}{k}T_{\text{byte}}+T_{\text{start}}</annotation></semantics></math><span class="ltx_text" style="font-size:90%;">.
The first chunk must traverse </span><math alttext="p-1" class="ltx_Math" display="inline" id="A4.SS2.p1.m3" intent=":literal"><semantics><mrow><mi mathsize="0.900em">p</mi><mo mathsize="0.900em">−</mo><mn mathsize="0.900em">1</mn></mrow><annotation encoding="application/x-tex">p-1</annotation></semantics></math><span class="ltx_text" style="font-size:90%;"> network hops to reach the final relay. Due to the pipelined nature, this takes </span><math alttext="(p-1)\times t_{\text{chunk}}" class="ltx_Math" display="inline" id="A4.SS2.p1.m4" intent=":literal"><semantics><mrow><mrow><mo maxsize="0.900em" minsize="0.900em">(</mo><mrow><mi mathsize="0.900em">p</mi><mo mathsize="0.900em">−</mo><mn mathsize="0.900em">1</mn></mrow><mo maxsize="0.900em" minsize="0.900em" rspace="0.055em">)</mo></mrow><mo mathsize="0.900em" rspace="0.222em">×</mo><msub><mi mathsize="0.900em">t</mi><mtext mathsize="0.900em">chunk</mtext></msub></mrow><annotation encoding="application/x-tex">(p-1)\times t_{\text{chunk}}</annotation></semantics></math><span class="ltx_text" style="font-size:90%;">. Once the first chunk arrives at the final relay, the subsequent </span><math alttext="k-1" class="ltx_Math" display="inline" id="A4.SS2.p1.m5" intent=":literal"><semantics><mrow><mi mathsize="0.900em">k</mi><mo mathsize="0.900em">−</mo><mn mathsize="0.900em">1</mn></mrow><annotation encoding="application/x-tex">k-1</annotation></semantics></math><span class="ltx_text" style="font-size:90%;"> chunks arrive sequentially, with one chunk arriving every </span><math alttext="t_{\text{chunk}}" class="ltx_Math" display="inline" id="A4.SS2.p1.m6" intent=":literal"><semantics><msub><mi mathsize="0.900em">t</mi><mtext mathsize="0.900em">chunk</mtext></msub><annotation encoding="application/x-tex">t_{\text{chunk}}</annotation></semantics></math><span class="ltx_text" style="font-size:90%;">. This adds </span><math alttext="(k-1)\times t_{\text{chunk}}" class="ltx_Math" display="inline" id="A4.SS2.p1.m7" intent=":literal"><semantics><mrow><mrow><mo maxsize="0.900em" minsize="0.900em">(</mo><mrow><mi mathsize="0.900em">k</mi><mo mathsize="0.900em">−</mo><mn mathsize="0.900em">1</mn></mrow><mo maxsize="0.900em" minsize="0.900em" rspace="0.055em">)</mo></mrow><mo mathsize="0.900em" rspace="0.222em">×</mo><msub><mi mathsize="0.900em">t</mi><mtext mathsize="0.900em">chunk</mtext></msub></mrow><annotation encoding="application/x-tex">(k-1)\times t_{\text{chunk}}</annotation></semantics></math><span class="ltx_text" style="font-size:90%;"> to the total time.
Therefore, the total latency is:</span></p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A4.EGx1">
<tbody id="A4.Ex1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle T(p,k)" class="ltx_Math" display="inline" id="A4.Ex1.m1" intent=":literal"><semantics><mrow><mi mathsize="0.900em">T</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="0.900em" minsize="0.900em">(</mo><mi mathsize="0.900em">p</mi><mo mathsize="0.900em">,</mo><mi mathsize="0.900em">k</mi><mo maxsize="0.900em" minsize="0.900em">)</mo></mrow></mrow><annotation encoding="application/x-tex">\displaystyle T(p,k)</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=(p-1)\cdot t_{\text{chunk}}+(k-1)\cdot t_{\text{chunk}}" class="ltx_Math" display="inline" id="A4.Ex1.m2" intent=":literal"><semantics><mrow><mi></mi><mo mathsize="0.900em">=</mo><mrow><mrow><mrow><mo maxsize="0.900em" minsize="0.900em">(</mo><mrow><mi mathsize="0.900em">p</mi><mo mathsize="0.900em">−</mo><mn mathsize="0.900em">1</mn></mrow><mo maxsize="0.900em" minsize="0.900em" rspace="0.055em">)</mo></mrow><mo mathsize="0.900em" rspace="0.222em">⋅</mo><msub><mi mathsize="0.900em">t</mi><mtext mathsize="0.900em">chunk</mtext></msub></mrow><mo mathsize="0.900em">+</mo><mrow><mrow><mo maxsize="0.900em" minsize="0.900em">(</mo><mrow><mi mathsize="0.900em">k</mi><mo mathsize="0.900em">−</mo><mn mathsize="0.900em">1</mn></mrow><mo maxsize="0.900em" minsize="0.900em" rspace="0.055em">)</mo></mrow><mo mathsize="0.900em" rspace="0.222em">⋅</mo><msub><mi mathsize="0.900em">t</mi><mtext mathsize="0.900em">chunk</mtext></msub></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle=(p-1)\cdot t_{\text{chunk}}+(k-1)\cdot t_{\text{chunk}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A4.Ex2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=(p+k-2)\cdot t_{\text{chunk}}" class="ltx_Math" display="inline" id="A4.Ex2.m1" intent=":literal"><semantics><mrow><mi></mi><mo mathsize="0.900em">=</mo><mrow><mrow><mo maxsize="0.900em" minsize="0.900em">(</mo><mrow><mrow><mi mathsize="0.900em">p</mi><mo mathsize="0.900em">+</mo><mi mathsize="0.900em">k</mi></mrow><mo mathsize="0.900em">−</mo><mn mathsize="0.900em">2</mn></mrow><mo maxsize="0.900em" minsize="0.900em" rspace="0.055em">)</mo></mrow><mo mathsize="0.900em" rspace="0.222em">⋅</mo><msub><mi mathsize="0.900em">t</mi><mtext mathsize="0.900em">chunk</mtext></msub></mrow></mrow><annotation encoding="application/x-tex">\displaystyle=(p+k-2)\cdot t_{\text{chunk}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A4.Ex3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=(p+k-2)\left(\frac{M}{k}T_{\text{byte}}+T_{\text{start}}\right)" class="ltx_Math" display="inline" id="A4.Ex3.m1" intent=":literal"><semantics><mrow><mi></mi><mo mathsize="0.900em">=</mo><mrow><mrow><mo maxsize="0.900em" minsize="0.900em">(</mo><mrow><mrow><mi mathsize="0.900em">p</mi><mo mathsize="0.900em">+</mo><mi mathsize="0.900em">k</mi></mrow><mo mathsize="0.900em">−</mo><mn mathsize="0.900em">2</mn></mrow><mo maxsize="0.900em" minsize="0.900em">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mrow><mrow><mstyle displaystyle="true"><mfrac><mi mathsize="0.900em">M</mi><mi mathsize="0.900em">k</mi></mfrac></mstyle><mo lspace="0em" rspace="0em">​</mo><msub><mi mathsize="0.900em">T</mi><mtext mathsize="0.900em">byte</mtext></msub></mrow><mo mathsize="0.900em">+</mo><msub><mi mathsize="0.900em">T</mi><mtext mathsize="0.900em">start</mtext></msub></mrow><mo>)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle=(p+k-2)\left(\frac{M}{k}T_{\text{byte}}+T_{\text{start}}\right)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p"><span class="ltx_text" style="font-size:90%;">Expanding this expression:</span></p>
<table class="ltx_equation ltx_eqn_table" id="A4.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(1)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="T(p,k)=MT_{\text{byte}}+(p-2)T_{\text{start}}+kT_{\text{start}}+\frac{(p-2)M}{k}T_{\text{byte}}" class="ltx_Math" display="block" id="A4.E1.m1" intent=":literal"><semantics><mrow><mrow><mi mathsize="0.900em">T</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="0.900em" minsize="0.900em">(</mo><mi mathsize="0.900em">p</mi><mo mathsize="0.900em">,</mo><mi mathsize="0.900em">k</mi><mo maxsize="0.900em" minsize="0.900em">)</mo></mrow></mrow><mo mathsize="0.900em">=</mo><mrow><mrow><mi mathsize="0.900em">M</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi mathsize="0.900em">T</mi><mtext mathsize="0.900em">byte</mtext></msub></mrow><mo mathsize="0.900em">+</mo><mrow><mrow><mo maxsize="0.900em" minsize="0.900em">(</mo><mrow><mi mathsize="0.900em">p</mi><mo mathsize="0.900em">−</mo><mn mathsize="0.900em">2</mn></mrow><mo maxsize="0.900em" minsize="0.900em">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msub><mi mathsize="0.900em">T</mi><mtext mathsize="0.900em">start</mtext></msub></mrow><mo mathsize="0.900em">+</mo><mrow><mi mathsize="0.900em">k</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi mathsize="0.900em">T</mi><mtext mathsize="0.900em">start</mtext></msub></mrow><mo mathsize="0.900em">+</mo><mrow><mfrac><mrow><mrow><mo maxsize="0.900em" minsize="0.900em">(</mo><mrow><mi mathsize="0.900em">p</mi><mo mathsize="0.900em">−</mo><mn mathsize="0.900em">2</mn></mrow><mo maxsize="0.900em" minsize="0.900em">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi mathsize="0.900em">M</mi></mrow><mi mathsize="0.900em">k</mi></mfrac><mo lspace="0em" rspace="0em">​</mo><msub><mi mathsize="0.900em">T</mi><mtext mathsize="0.900em">byte</mtext></msub></mrow></mrow></mrow><annotation encoding="application/x-tex">T(p,k)=MT_{\text{byte}}+(p-2)T_{\text{start}}+kT_{\text{start}}+\frac{(p-2)M}{k}T_{\text{byte}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
</section>
<section class="ltx_subsection" id="A4.SS3">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">D.3. </span>Scalability Analysis</h3>
<div class="ltx_para" id="A4.SS3.p1">
<p class="ltx_p"><span class="ltx_text" style="font-size:90%;">To understand the scalability with respect to the number of nodes </span><math alttext="p" class="ltx_Math" display="inline" id="A4.SS3.p1.m1" intent=":literal"><semantics><mi mathsize="0.900em">p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math><span class="ltx_text" style="font-size:90%;">, we analyze the terms in Equation </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#A4.E1" style="font-size:90%;" title="In D.2. Latency Derivation ‣ Appendix D Theoretical Analysis of Chain-Based Pipelined Broadcast ‣ A.2. Hyperparameter Details ‣ A.1. Response Length Distribution of Each Model ‣ Appendix A Experiment Details ‣ 10. Conclusion ‣ 9. Related Work ‣ 8.5. Fault Tolerance Analysis ‣ 8.4. Repack Efficiency ‣ 8.3. Weight Synchronization Overhead ‣ 13(c) ‣ 8.2. Training Convergence ‣ 8. Evaluation ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_tag">1</span></a><span class="ltx_text" style="font-size:90%;">. The choice of </span><math alttext="k" class="ltx_Math" display="inline" id="A4.SS3.p1.m2" intent=":literal"><semantics><mi mathsize="0.900em">k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math><span class="ltx_text" style="font-size:90%;"> (the number of chunks) is critical. We can find the optimal </span><math alttext="k" class="ltx_Math" display="inline" id="A4.SS3.p1.m3" intent=":literal"><semantics><mi mathsize="0.900em">k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math><span class="ltx_text" style="font-size:90%;"> that minimizes </span><math alttext="T(p,k)" class="ltx_Math" display="inline" id="A4.SS3.p1.m4" intent=":literal"><semantics><mrow><mi mathsize="0.900em">T</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="0.900em" minsize="0.900em">(</mo><mi mathsize="0.900em">p</mi><mo mathsize="0.900em">,</mo><mi mathsize="0.900em">k</mi><mo maxsize="0.900em" minsize="0.900em">)</mo></mrow></mrow><annotation encoding="application/x-tex">T(p,k)</annotation></semantics></math><span class="ltx_text" style="font-size:90%;"> by taking the derivative with respect to </span><math alttext="k" class="ltx_Math" display="inline" id="A4.SS3.p1.m5" intent=":literal"><semantics><mi mathsize="0.900em">k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math><span class="ltx_text" style="font-size:90%;"> and setting it to zero:</span></p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A4.EGx2">
<tbody id="A4.Ex4"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\frac{\partial T}{\partial k}" class="ltx_Math" display="inline" id="A4.Ex4.m1" intent=":literal"><semantics><mstyle displaystyle="true"><mfrac><mrow><mo mathsize="0.900em" rspace="0em">∂</mo><mi mathsize="0.900em">T</mi></mrow><mrow><mo mathsize="0.900em" rspace="0em">∂</mo><mi mathsize="0.900em">k</mi></mrow></mfrac></mstyle><annotation encoding="application/x-tex">\displaystyle\frac{\partial T}{\partial k}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=T_{\text{start}}-\frac{(p-2)M}{k^{2}}T_{\text{byte}}=0" class="ltx_Math" display="inline" id="A4.Ex4.m2" intent=":literal"><semantics><mrow><mi></mi><mo mathsize="0.900em">=</mo><mrow><msub><mi mathsize="0.900em">T</mi><mtext mathsize="0.900em">start</mtext></msub><mo mathsize="0.900em">−</mo><mrow><mstyle displaystyle="true"><mfrac><mrow><mrow><mo maxsize="0.900em" minsize="0.900em">(</mo><mrow><mi mathsize="0.900em">p</mi><mo mathsize="0.900em">−</mo><mn mathsize="0.900em">2</mn></mrow><mo maxsize="0.900em" minsize="0.900em">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi mathsize="0.900em">M</mi></mrow><msup><mi mathsize="0.900em">k</mi><mn mathsize="0.900em">2</mn></msup></mfrac></mstyle><mo lspace="0em" rspace="0em">​</mo><msub><mi mathsize="0.900em">T</mi><mtext mathsize="0.900em">byte</mtext></msub></mrow></mrow><mo mathsize="0.900em">=</mo><mn mathsize="0.900em">0</mn></mrow><annotation encoding="application/x-tex">\displaystyle=T_{\text{start}}-\frac{(p-2)M}{k^{2}}T_{\text{byte}}=0</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A4.Ex5"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle k^{2}" class="ltx_Math" display="inline" id="A4.Ex5.m1" intent=":literal"><semantics><msup><mi mathsize="0.900em">k</mi><mn mathsize="0.900em">2</mn></msup><annotation encoding="application/x-tex">\displaystyle k^{2}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\frac{(p-2)MT_{\text{byte}}}{T_{\text{start}}}" class="ltx_Math" display="inline" id="A4.Ex5.m2" intent=":literal"><semantics><mrow><mi></mi><mo mathsize="0.900em">=</mo><mstyle displaystyle="true"><mfrac><mrow><mrow><mo maxsize="0.900em" minsize="0.900em">(</mo><mrow><mi mathsize="0.900em">p</mi><mo mathsize="0.900em">−</mo><mn mathsize="0.900em">2</mn></mrow><mo maxsize="0.900em" minsize="0.900em">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi mathsize="0.900em">M</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi mathsize="0.900em">T</mi><mtext mathsize="0.900em">byte</mtext></msub></mrow><msub><mi mathsize="0.900em">T</mi><mtext mathsize="0.900em">start</mtext></msub></mfrac></mstyle></mrow><annotation encoding="application/x-tex">\displaystyle=\frac{(p-2)MT_{\text{byte}}}{T_{\text{start}}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A4.Ex6"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle k^{*}" class="ltx_Math" display="inline" id="A4.Ex6.m1" intent=":literal"><semantics><msup><mi mathsize="0.900em">k</mi><mo mathsize="0.900em">∗</mo></msup><annotation encoding="application/x-tex">\displaystyle k^{*}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\sqrt{\frac{(p-2)MT_{\text{byte}}}{T_{\text{start}}}}" class="ltx_Math" display="inline" id="A4.Ex6.m2" intent=":literal"><semantics><mrow><mi></mi><mo mathsize="0.900em">=</mo><msqrt><mstyle displaystyle="true"><mfrac><mrow><mrow><mo maxsize="0.900em" minsize="0.900em">(</mo><mrow><mi mathsize="0.900em">p</mi><mo mathsize="0.900em">−</mo><mn mathsize="0.900em">2</mn></mrow><mo maxsize="0.900em" minsize="0.900em">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi mathsize="0.900em">M</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi mathsize="0.900em">T</mi><mtext mathsize="0.900em">byte</mtext></msub></mrow><msub><mi mathsize="0.900em">T</mi><mtext mathsize="0.900em">start</mtext></msub></mfrac></mstyle></msqrt></mrow><annotation encoding="application/x-tex">\displaystyle=\sqrt{\frac{(p-2)MT_{\text{byte}}}{T_{\text{start}}}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p"><span class="ltx_text" style="font-size:90%;">Substituting </span><math alttext="k^{*}" class="ltx_Math" display="inline" id="A4.SS3.p1.m6" intent=":literal"><semantics><msup><mi mathsize="0.900em">k</mi><mo mathsize="0.900em">∗</mo></msup><annotation encoding="application/x-tex">k^{*}</annotation></semantics></math><span class="ltx_text" style="font-size:90%;"> back into Equation </span><a class="ltx_ref" href="https://arxiv.org/html/2510.12633v1#A4.E1" style="font-size:90%;" title="In D.2. Latency Derivation ‣ Appendix D Theoretical Analysis of Chain-Based Pipelined Broadcast ‣ A.2. Hyperparameter Details ‣ A.1. Response Length Distribution of Each Model ‣ Appendix A Experiment Details ‣ 10. Conclusion ‣ 9. Related Work ‣ 8.5. Fault Tolerance Analysis ‣ 8.4. Repack Efficiency ‣ 8.3. Weight Synchronization Overhead ‣ 13(c) ‣ 8.2. Training Convergence ‣ 8. Evaluation ‣ Laminar: A Scalable Asynchronous RL Post-Training Framework"><span class="ltx_text ltx_ref_tag">1</span></a><span class="ltx_text" style="font-size:90%;">, we find the minimum possible broadcast time for a given </span><math alttext="p" class="ltx_Math" display="inline" id="A4.SS3.p1.m7" intent=":literal"><semantics><mi mathsize="0.900em">p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math><span class="ltx_text" style="font-size:90%;"> and </span><math alttext="M" class="ltx_Math" display="inline" id="A4.SS3.p1.m8" intent=":literal"><semantics><mi mathsize="0.900em">M</mi><annotation encoding="application/x-tex">M</annotation></semantics></math><span class="ltx_text" style="font-size:90%;">:</span></p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A4.EGx3">
<tbody id="A4.Ex7"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle T^{*}(p)" class="ltx_Math" display="inline" id="A4.Ex7.m1" intent=":literal"><semantics><mrow><msup><mi mathsize="0.900em">T</mi><mo mathsize="0.900em">∗</mo></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="0.900em" minsize="0.900em">(</mo><mi mathsize="0.900em">p</mi><mo maxsize="0.900em" minsize="0.900em">)</mo></mrow></mrow><annotation encoding="application/x-tex">\displaystyle T^{*}(p)</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=MT_{\text{byte}}+(p-2)T_{\text{start}}+2\sqrt{(p-2)MT_{\text{byte}}T_{\text{start}}}" class="ltx_Math" display="inline" id="A4.Ex7.m2" intent=":literal"><semantics><mrow><mi></mi><mo mathsize="0.900em">=</mo><mrow><mrow><mi mathsize="0.900em">M</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi mathsize="0.900em">T</mi><mtext mathsize="0.900em">byte</mtext></msub></mrow><mo mathsize="0.900em">+</mo><mrow><mrow><mo maxsize="0.900em" minsize="0.900em">(</mo><mrow><mi mathsize="0.900em">p</mi><mo mathsize="0.900em">−</mo><mn mathsize="0.900em">2</mn></mrow><mo maxsize="0.900em" minsize="0.900em">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msub><mi mathsize="0.900em">T</mi><mtext mathsize="0.900em">start</mtext></msub></mrow><mo mathsize="0.900em">+</mo><mrow><mn mathsize="0.900em">2</mn><mo lspace="0em" rspace="0em">​</mo><msqrt><mrow><mrow><mo maxsize="0.900em" minsize="0.900em">(</mo><mrow><mi mathsize="0.900em">p</mi><mo mathsize="0.900em">−</mo><mn mathsize="0.900em">2</mn></mrow><mo maxsize="0.900em" minsize="0.900em">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi mathsize="0.900em">M</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi mathsize="0.900em">T</mi><mtext mathsize="0.900em">byte</mtext></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi mathsize="0.900em">T</mi><mtext mathsize="0.900em">start</mtext></msub></mrow></msqrt></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle=MT_{\text{byte}}+(p-2)T_{\text{start}}+2\sqrt{(p-2)MT_{\text{byte}}T_{\text{start}}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p"><span class="ltx_text" style="font-size:90%;">Let’s analyze the composition of this optimal time:</span></p>
<table class="ltx_equation ltx_eqn_table" id="A4.Ex8">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="T^{*}(p)=\underbrace{MT_{\text{byte}}}_{\text{Bandwidth Term}}+\underbrace{(p-2)T_{\text{start}}}_{\text{Latency Term}}+\underbrace{2\sqrt{(p-2)MT_{\text{byte}}T_{\text{start}}}}_{\text{Pipeline Term}}" class="ltx_Math" display="block" id="A4.Ex8.m1" intent=":literal"><semantics><mrow><mrow><msup><mi mathsize="0.900em">T</mi><mo mathsize="0.900em">∗</mo></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="0.900em" minsize="0.900em">(</mo><mi mathsize="0.900em">p</mi><mo maxsize="0.900em" minsize="0.900em">)</mo></mrow></mrow><mo mathsize="0.900em">=</mo><mrow><munder><munder accentunder="true"><mrow><mi mathsize="0.900em">M</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi mathsize="0.900em">T</mi><mtext mathsize="0.900em">byte</mtext></msub></mrow><mo stretchy="true">⏟</mo></munder><mtext mathsize="0.900em">Bandwidth Term</mtext></munder><mo mathsize="0.900em">+</mo><munder><munder accentunder="true"><mrow><mrow><mo maxsize="0.900em" minsize="0.900em">(</mo><mrow><mi mathsize="0.900em">p</mi><mo mathsize="0.900em">−</mo><mn mathsize="0.900em">2</mn></mrow><mo maxsize="0.900em" minsize="0.900em">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msub><mi mathsize="0.900em">T</mi><mtext mathsize="0.900em">start</mtext></msub></mrow><mo stretchy="true">⏟</mo></munder><mtext mathsize="0.900em">Latency Term</mtext></munder><mo mathsize="0.900em">+</mo><munder><munder accentunder="true"><mrow><mn mathsize="0.900em">2</mn><mo lspace="0em" rspace="0em">​</mo><msqrt><mrow><mrow><mo maxsize="0.900em" minsize="0.900em">(</mo><mrow><mi mathsize="0.900em">p</mi><mo mathsize="0.900em">−</mo><mn mathsize="0.900em">2</mn></mrow><mo maxsize="0.900em" minsize="0.900em">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi mathsize="0.900em">M</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi mathsize="0.900em">T</mi><mtext mathsize="0.900em">byte</mtext></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi mathsize="0.900em">T</mi><mtext mathsize="0.900em">start</mtext></msub></mrow></msqrt></mrow><mo stretchy="true">⏟</mo></munder><mtext mathsize="0.900em">Pipeline Term</mtext></munder></mrow></mrow><annotation encoding="application/x-tex">T^{*}(p)=\underbrace{MT_{\text{byte}}}_{\text{Bandwidth Term}}+\underbrace{(p-2)T_{\text{start}}}_{\text{Latency Term}}+\underbrace{2\sqrt{(p-2)MT_{\text{byte}}T_{\text{start}}}}_{\text{Pipeline Term}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="A4.SS3.p2">
<p class="ltx_p"><span class="ltx_text" style="font-size:90%;">In the context of distributing large language models:</span></p>
</div>
<div class="ltx_para ltx_noindent" id="A4.SS3.p3">
<p class="ltx_p"><math alttext="\bullet" class="ltx_Math" display="inline" id="A4.SS3.p3.m1" intent=":literal"><semantics><mo mathsize="0.900em">∙</mo><annotation encoding="application/x-tex">\bullet</annotation></semantics></math><span class="ltx_text" style="font-size:90%;">
</span><span class="ltx_text ltx_font_bold" style="font-size:90%;">The message size <math alttext="M" class="ltx_Math" display="inline" id="A4.SS3.p3.m2" intent=":literal"><semantics><mi>M</mi><annotation encoding="application/x-tex">M</annotation></semantics></math> is very large</span><span class="ltx_text" style="font-size:90%;"> (e.g., 140 GB for a 70B model using BF16 precision).</span></p>
</div>
<div class="ltx_para ltx_noindent" id="A4.SS3.p4">
<p class="ltx_p"><math alttext="\bullet" class="ltx_Math" display="inline" id="A4.SS3.p4.m1" intent=":literal"><semantics><mo mathsize="0.900em">∙</mo><annotation encoding="application/x-tex">\bullet</annotation></semantics></math><span class="ltx_text" style="font-size:90%;">
</span><span class="ltx_text ltx_font_bold" style="font-size:90%;">The startup latency <math alttext="T_{\text{start}}" class="ltx_Math" display="inline" id="A4.SS3.p4.m2" intent=":literal"><semantics><msub><mi>T</mi><mtext class="ltx_mathvariant_bold">start</mtext></msub><annotation encoding="application/x-tex">T_{\text{start}}</annotation></semantics></math> is very small</span><span class="ltx_text" style="font-size:90%;"> for RDMA (on the order of microseconds).
Consequently:</span></p>
<ol class="ltx_enumerate" id="A4.I2">
<li class="ltx_item" id="A4.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(1)</span>
<div class="ltx_para" id="A4.I2.i1.p1">
<p class="ltx_p"><span class="ltx_text" style="font-size:90%;">The </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Bandwidth Term (<math alttext="MT_{\text{byte}}" class="ltx_Math" display="inline" id="A4.I2.i1.p1.m1" intent=":literal"><semantics><mrow><mi>M</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>T</mi><mtext class="ltx_mathvariant_bold">byte</mtext></msub></mrow><annotation encoding="application/x-tex">MT_{\text{byte}}</annotation></semantics></math>)</span><span class="ltx_text" style="font-size:90%;"> is the time to serialize the entire model onto a single network link. Given the large </span><math alttext="M" class="ltx_Math" display="inline" id="A4.I2.i1.p1.m2" intent=":literal"><semantics><mi mathsize="0.900em">M</mi><annotation encoding="application/x-tex">M</annotation></semantics></math><span class="ltx_text" style="font-size:90%;">, this term is the dominant component of the total latency and is completely independent of </span><math alttext="p" class="ltx_Math" display="inline" id="A4.I2.i1.p1.m3" intent=":literal"><semantics><mi mathsize="0.900em">p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math><span class="ltx_text" style="font-size:90%;">.</span></p>
</div>
</li>
<li class="ltx_item" id="A4.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(2)</span>
<div class="ltx_para" id="A4.I2.i2.p1">
<p class="ltx_p"><span class="ltx_text" style="font-size:90%;">The </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Latency Term (<math alttext="(p-2)T_{\text{start}}" class="ltx_Math" display="inline" id="A4.I2.i2.p1.m1" intent=":literal"><semantics><mrow><mrow><mo stretchy="false">(</mo><mrow><mi>p</mi><mo>−</mo><mn>2</mn></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msub><mi>T</mi><mtext class="ltx_mathvariant_bold">start</mtext></msub></mrow><annotation encoding="application/x-tex">(p-2)T_{\text{start}}</annotation></semantics></math>)</span><span class="ltx_text" style="font-size:90%;"> grows linearly with </span><math alttext="p" class="ltx_Math" display="inline" id="A4.I2.i2.p1.m2" intent=":literal"><semantics><mi mathsize="0.900em">p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math><span class="ltx_text" style="font-size:90%;">. However, because </span><math alttext="T_{\text{start}}" class="ltx_Math" display="inline" id="A4.I2.i2.p1.m3" intent=":literal"><semantics><msub><mi mathsize="0.900em">T</mi><mtext mathsize="0.900em">start</mtext></msub><annotation encoding="application/x-tex">T_{\text{start}}</annotation></semantics></math><span class="ltx_text" style="font-size:90%;"> is extremely small, this term’s contribution is negligible even for thousands of nodes (e.g., </span><math alttext="2000\times 5\mu s=10ms" class="ltx_Math" display="inline" id="A4.I2.i2.p1.m4" intent=":literal"><semantics><mrow><mrow><mrow><mn mathsize="0.900em">2000</mn><mo lspace="0.222em" mathsize="0.900em" rspace="0.222em">×</mo><mn mathsize="0.900em">5</mn></mrow><mo lspace="0em" rspace="0em">​</mo><mi mathsize="0.900em">μ</mi><mo lspace="0em" rspace="0em">​</mo><mi mathsize="0.900em">s</mi></mrow><mo mathsize="0.900em">=</mo><mrow><mn mathsize="0.900em">10</mn><mo lspace="0em" rspace="0em">​</mo><mi mathsize="0.900em">m</mi><mo lspace="0em" rspace="0em">​</mo><mi mathsize="0.900em">s</mi></mrow></mrow><annotation encoding="application/x-tex">2000\times 5\mu s=10ms</annotation></semantics></math><span class="ltx_text" style="font-size:90%;">).</span></p>
</div>
</li>
<li class="ltx_item" id="A4.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(3)</span>
<div class="ltx_para" id="A4.I2.i3.p1">
<p class="ltx_p"><span class="ltx_text" style="font-size:90%;">The </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Pipeline Term</span><span class="ltx_text" style="font-size:90%;"> grows sub-linearly with the number of nodes (</span><math alttext="O(\sqrt{p})" class="ltx_Math" display="inline" id="A4.I2.i3.p1.m1" intent=":literal"><semantics><mrow><mi mathsize="0.900em">O</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="0.900em" minsize="0.900em">(</mo><msqrt><mi mathsize="0.900em">p</mi></msqrt><mo maxsize="0.900em" minsize="0.900em">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(\sqrt{p})</annotation></semantics></math><span class="ltx_text" style="font-size:90%;">). While it grows faster than the latency term, its contribution remains significantly smaller than the bandwidth term for realistic system parameters.</span></p>
</div>
</li>
</ol>
</div>
<div class="ltx_para ltx_noindent" id="A4.SS3.p5">
<p class="ltx_p"><span class="ltx_text ltx_font_bold" style="font-size:90%;">Conclusion:</span><span class="ltx_text" style="font-size:90%;"> The total broadcast time is overwhelmingly dominated by the constant bandwidth term (</span><math alttext="MT_{\text{byte}}" class="ltx_Math" display="inline" id="A4.SS3.p5.m1" intent=":literal"><semantics><mrow><mi mathsize="0.900em">M</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi mathsize="0.900em">T</mi><mtext mathsize="0.900em">byte</mtext></msub></mrow><annotation encoding="application/x-tex">MT_{\text{byte}}</annotation></semantics></math><span class="ltx_text" style="font-size:90%;">). The terms dependent on the number of nodes </span><math alttext="p" class="ltx_Math" display="inline" id="A4.SS3.p5.m2" intent=":literal"><semantics><mi mathsize="0.900em">p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math><span class="ltx_text" style="font-size:90%;"> either have a very small coefficient (</span><math alttext="T_{\text{start}}" class="ltx_Math" display="inline" id="A4.SS3.p5.m3" intent=":literal"><semantics><msub><mi mathsize="0.900em">T</mi><mtext mathsize="0.900em">start</mtext></msub><annotation encoding="application/x-tex">T_{\text{start}}</annotation></semantics></math><span class="ltx_text" style="font-size:90%;">) or grow sub-linearly (</span><math alttext="O(\sqrt{p})" class="ltx_Math" display="inline" id="A4.SS3.p5.m4" intent=":literal"><semantics><mrow><mi mathsize="0.900em">O</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="0.900em" minsize="0.900em">(</mo><msqrt><mi mathsize="0.900em">p</mi></msqrt><mo maxsize="0.900em" minsize="0.900em">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(\sqrt{p})</annotation></semantics></math><span class="ltx_text" style="font-size:90%;">). This makes the total latency largely insensitive to the number of relays, proving that the chain-based pipelined broadcast is a highly scalable mechanism for distributing large model weights.</span></p>
</div>
</section>
</section>
</section>
</div>
</figure>
</section>
</section>
</section>
</section>
</section>
</section>
</section>
</div>
</figure>
</section>
</div>
</figure>
</figure>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Oct 14 15:20:53 2025 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
